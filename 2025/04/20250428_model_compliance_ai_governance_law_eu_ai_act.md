**[1. [2408.08318] First Analysis of the EU Artifical Intelligence Act: Towards a Global
  Standard for Trustworthy AI?](https://arxiv.org/pdf/2408.08318.pdf)** (2024-08-19)

*UA, CDEP  Marion Ho-Dac*

  The EU Artificial Intelligence Act (AI Act) came into force in the European
Union (EU) on 1 August 2024. It is a key piece of legislation both for the
citizens at the heart of AI technologies and for the industry active in the
internal market. The AI Act imposes progressive compliance on organisations -
both private and public - involved in the global value chain of AI systems and
models marketed and used in the EU. While the Act is unprecedented on an
international scale in terms of its horizontal and binding regulatory scope,
its global appeal in support of trustworthy AI is one of its major challenges.


---

**[2. [2307.10458] Complying with the EU AI Act](https://arxiv.org/pdf/2307.10458.pdf)** (2023-07-21)

*Jacintha Walters, Diptish Dey, Debarati Bhaumik, Sophie Horsman*

  The EU AI Act is the proposed EU legislation concerning AI systems. This
paper identifies several categories of the AI Act. Based on this
categorization, a questionnaire is developed that serves as a tool to offer
insights by creating quantitative data. Analysis of the data shows various
challenges for organizations in different compliance categories. The influence
of organization characteristics, such as size and sector, is examined to
determine the impact on compliance. The paper will also share qualitative data
on which questions were prevalent among respondents, both on the content of the
AI Act as the application. The paper concludes by stating that there is still
room for improvement in terms of compliance with the AIA and refers to a
related project that examines a solution to help these organizations.


---

**[3. [2410.14501] Using sensitive data to de-bias AI systems: Article 10(5) of the EU AI
  Act](https://arxiv.org/pdf/2410.14501.pdf)** (2025-02-28)

*Marvin van Bekkum*

  In June 2024, the EU AI Act came into force. The AI Act includes obligations
for the provider of an AI system. Article 10 of the AI Act includes a new
obligation for providers to evaluate whether their training, validation and
testing datasets meet certain quality criteria, including an appropriate
examination of biases in the datasets and correction measures. With the
obligation comes a new provision in Article 10(5) AI Act, allowing providers to
collect sensitive data to fulfil the obligation. The exception aims to prevent
discrimination. In this paper, I research the scope and implications of Article
10(5) AI Act. The paper primarily concerns European Union law, but may be
relevant in other parts of the world, as policymakers aim to regulate biases in
AI systems.


---

**[4. [2407.10369] A Robust Governance for the AI Act: AI Office, AI Board, Scientific
  Panel, and National Authorities](https://arxiv.org/pdf/2407.10369.pdf)** (2024-10-29)

*Claudio Novelli, Philipp Hacker, Jessica Morley, Jarle Trondal, Luciano Floridi*

  Regulation is nothing without enforcement. This particularly holds for the
dynamic field of emerging technologies. Hence, this article has two ambitions.
First, it explains how the EU's new Artificial Intelligence Act (AIA) will be
implemented and enforced by various institutional bodies, thus clarifying the
governance framework of the AIA. Second, it proposes a normative model of
governance, providing recommendations to ensure uniform and coordinated
execution of the AIA and the fulfilment of the legislation. Taken together, the
article explores how the AIA may be implemented by national and EU
institutional bodies, encompassing longstanding bodies, such as the European
Commission, and those newly established under the AIA, such as the AI Office.
It investigates their roles across supranational and national levels,
emphasizing how EU regulations influence institutional structures and
operations. These regulations may not only directly dictate the structural
design of institutions but also indirectly request administrative capacities
needed to enforce the AIA.


---

**[5. [2107.14099] The ghost of AI governance past, present and future: AI governance in
  the European Union](https://arxiv.org/pdf/2107.14099.pdf)** (2021-07-30)

*Charlotte Stix*

  The received wisdom is that artificial intelligence (AI) is a competition
between the US and China. In this chapter, the author will examine how the
European Union (EU) fits into that mix and what it can offer as a third way to
govern AI. The chapter presents this by exploring the past, present and future
of AI governance in the EU. Section 1 serves to explore and evidence the EUs
coherent and comprehensive approach to AI governance. In short, the EU ensures
and encourages ethical, trustworthy and reliable technological development.
This will cover a range of key documents and policy tools that lead to the most
crucial effort of the EU to date: to regulate AI. Section 2 maps the EUs drive
towards digital sovereignty through the lens of regulation and infrastructure.
This covers topics such as the trustworthiness of AI systems, cloud, compute
and foreign direct investment. In Section 3, the chapter concludes by offering
several considerations to achieve good AI governance in the EU.


---

**[6. [2501.10391] Developing an Ontology for AI Act Fundamental Rights Impact Assessments](https://arxiv.org/pdf/2501.10391.pdf)** (2025-01-22)

*Tytti Rintamaki, Harshvardhan J. Pandit*

  The recently published EU Artificial Intelligence Act (AI Act) is a landmark
regulation that regulates the use of AI technologies. One of its novel
requirements is the obligation to conduct a Fundamental Rights Impact
Assessment (FRIA), where organisations in the role of deployers must assess the
risks of their AI system regarding health, safety, and fundamental rights.
Another novelty in the AI Act is the requirement to create a questionnaire and
an automated tool to support organisations in their FRIA obligations. Such
automated tools will require a machine-readable form of information involved
within the FRIA process, and additionally also require machine-readable
documentation to enable further compliance tools to be created. In this
article, we present our novel representation of the FRIA as an ontology based
on semantic web standards. Our work builds upon the existing state of the art,
notably the Data Privacy Vocabulary (DPV), where similar works have been
established to create tools for GDPR's Data Protection Impact Assessments
(DPIA) and other obligations. Through our ontology, we enable the creation and
management of FRIA, and the use of automated tool in its various steps.


---

**[7. [2411.08535] The EU AI Act is a good start but falls short](https://arxiv.org/pdf/2411.08535.pdf)** (2025-04-18)

*Chalisa Veesommai Sillberg, Jose Siqueira De Cerqueira, Pekka Sillberg, Kai-Kristian Kemell, Pekka Abrahamsson*

  The EU AI Act was created to ensure ethical and safe Artificial Intelligence
(AI) development and deployment across the EU. This study aims to identify key
challenges and strategies for helping enterprises focus on resources
effectively. To achieve this aim, we conducted a Multivocal Literature Review
(MLR) to explore the sentiments of both the industry and the academia. From 130
articles, 56 met the criteria. Our key findings are three-fold. First,
liability. Second, discrimination. Third, tool adequacy. Additionally, some
negative sentiments were expressed by industry and academia regarding
regulatory interpretations, specific requirements, and transparency issues.
Next, our findings are three essential themes for enterprises. First,
risk-based regulatory compliance. Second, ethical frameworks and principles in
technology development. Third, policies and systems for regulatory risk
management. These results identify the key challenges and strategies and
provide less commonly discussed themes, enabling enterprises to align with the
requirements and minimize their distance from the EU market.


---

**[8. [2407.08105] Federated Learning and AI Regulation in the European Union: Who is
  Responsible? -- An Interdisciplinary Analysis](https://arxiv.org/pdf/2407.08105.pdf)** (2024-07-15)

*Herbert Woisetschläger, Simon Mertel, Christoph Krönke, Ruben Mayer, Hans-Arno Jacobsen*

  The European Union Artificial Intelligence Act mandates clear stakeholder
responsibilities in developing and deploying machine learning applications to
avoid substantial fines, prioritizing private and secure data processing with
data remaining at its origin. Federated Learning (FL) enables the training of
generative AI Models across data siloes, sharing only model parameters while
improving data security. Since FL is a cooperative learning paradigm, clients
and servers naturally share legal responsibility in the FL pipeline. Our work
contributes to clarifying the roles of both parties, explains strategies for
shifting responsibilities to the server operator, and points out open technical
challenges that we must solve to improve FL's practical applicability under the
EU AI Act.


---

**[9. [2410.22120] Vision Paper: Designing Graph Neural Networks in Compliance with the
  European Artificial Intelligence Act](https://arxiv.org/pdf/2410.22120.pdf)** (2024-10-30)

*Barbara Hoffmann, Jana Vatter, Ruben Mayer*

  The European Union's Artificial Intelligence Act (AI Act) introduces
comprehensive guidelines for the development and oversight of Artificial
Intelligence (AI) and Machine Learning (ML) systems, with significant
implications for Graph Neural Networks (GNNs). This paper addresses the unique
challenges posed by the AI Act for GNNs, which operate on complex
graph-structured data. The legislation's requirements for data management, data
governance, robustness, human oversight, and privacy necessitate tailored
strategies for GNNs. Our study explores the impact of these requirements on GNN
training and proposes methods to ensure compliance. We provide an in-depth
analysis of bias, robustness, explainability, and privacy in the context of
GNNs, highlighting the need for fair sampling strategies and effective
interpretability techniques. Our contributions fill the research gap by
offering specific guidance for GNNs under the new legislative framework and
identifying open questions and future research directions.


---

**[10. [2403.16808] Navigating the EU AI Act: A Methodological Approach to Compliance for
  Safety-critical Products](https://arxiv.org/pdf/2403.16808.pdf)** (2024-07-12)

*J. Kelly, S. Zafar, L. Heidemann, J. Zacchi, D. Espinoza, N. Mata*

  In December 2023, the European Parliament provisionally agreed on the EU AI
Act. This unprecedented regulatory framework for AI systems lays out guidelines
to ensure the safety, legality, and trustworthiness of AI products. This paper
presents a methodology for interpreting the EU AI Act requirements for
high-risk AI systems by leveraging product quality models. We first propose an
extended product quality model for AI systems, incorporating attributes
relevant to the Act not covered by current quality models. We map the Act
requirements to relevant quality attributes with the goal of refining them into
measurable characteristics. We then propose a contract-based approach to derive
technical requirements at the stakeholder level. This facilitates the
development and assessment of AI systems that not only adhere to established
quality standards, but also comply with the regulatory requirements outlined in
the Act for high-risk (including safety-critical) AI systems. We demonstrate
the applicability of this methodology on an exemplary automotive supply chain
use case, where several stakeholders interact to achieve EU AI Act compliance.


---

**[11. [2504.03300] How to Test for Compliance with Human Oversight Requirements in AI
  Regulation?](https://arxiv.org/pdf/2504.03300.pdf)** (2025-04-07)

*Markus Langer, Veronika Lazar, Kevin Baum*

  Human oversight requirements are a core component of the European AI Act and
in AI governance. In this paper, we highlight key challenges in testing for
compliance with these requirements. A key difficulty lies in balancing simple,
but potentially ineffective checklist-based approaches with resource-intensive
empirical testing in diverse contexts where humans oversee AI systems.
Additionally, the absence of easily operationalizable standards and the
context-dependent nature of human oversight further complicate compliance
testing. We argue that these challenges illustrate broader challenges in the
future of sociotechnical AI governance.


---

**[12. [2308.02033] AI and the EU Digital Markets Act: Addressing the Risks of Bigness in
  Generative AI](https://arxiv.org/pdf/2308.02033.pdf)** (2023-08-07)

*Ayse Gizem Yasar, Andrew Chong, Evan Dong, Thomas Krendl Gilbert, Sarah Hladikova, Roland Maio, Carlos Mougan, Xudong Shen, Shubham Singh, Ana-Andreea Stoica, Savannah Thais, Miri Zilka*

  As AI technology advances rapidly, concerns over the risks of bigness in
digital markets are also growing. The EU's Digital Markets Act (DMA) aims to
address these risks. Still, the current framework may not adequately cover
generative AI systems that could become gateways for AI-based services. This
paper argues for integrating certain AI software as core platform services and
classifying certain developers as gatekeepers under the DMA. We also propose an
assessment of gatekeeper obligations to ensure they cover generative AI
services. As the EU considers generative AI-specific rules and possible DMA
amendments, this paper provides insights towards diversity and openness in
generative AI services.


---

**[13. [2410.05306] Towards Assuring EU AI Act Compliance and Adversarial Robustness of LLMs](https://arxiv.org/pdf/2410.05306.pdf)** (2024-10-10)

*Tomas Bueno Momcilovic, Beat Buesser, Giulio Zizzo, Mark Purcell, Dian Balta*

  Large language models are prone to misuse and vulnerable to security threats,
raising significant safety and security concerns. The European Union's
Artificial Intelligence Act seeks to enforce AI robustness in certain contexts,
but faces implementation challenges due to the lack of standards, complexity of
LLMs and emerging security vulnerabilities. Our research introduces a framework
using ontologies, assurance cases, and factsheets to support engineers and
stakeholders in understanding and documenting AI system compliance and security
regarding adversarial robustness. This approach aims to ensure that LLMs adhere
to regulatory standards and are equipped to counter potential threats.


---

**[14. [2302.07872] Data-Centric Governance](https://arxiv.org/pdf/2302.07872.pdf)** (2023-02-17)

*Sean McGregor, Jesse Hostetler*

  Artificial intelligence (AI) governance is the body of standards and
practices used to ensure that AI systems are deployed responsibly. Current AI
governance approaches consist mainly of manual review and documentation
processes. While such reviews are necessary for many systems, they are not
sufficient to systematically address all potential harms, as they do not
operationalize governance requirements for system engineering, behavior, and
outcomes in a way that facilitates rigorous and reproducible evaluation. Modern
AI systems are data-centric: they act on data, produce data, and are built
through data engineering. The assurance of governance requirements must also be
carried out in terms of data. This work explores the systematization of
governance requirements via datasets and algorithmic evaluations. When applied
throughout the product lifecycle, data-centric governance decreases time to
deployment, increases solution quality, decreases deployment risks, and places
the system in a continuous state of assured compliance with governance
requirements.


---

**[15. [2502.16184] Robustness and Cybersecurity in the EU Artificial Intelligence Act](https://arxiv.org/pdf/2502.16184.pdf)** (2025-02-27)

*Henrik Nolte, Miriam Rateike, Michèle Finck*

  The EU Artificial Intelligence Act (AIA) establishes different legal
principles for different types of AI systems. While prior work has sought to
clarify some of these principles, little attention has been paid to robustness
and cybersecurity. This paper aims to fill this gap. We identify legal
challenges and shortcomings in provisions related to robustness and
cybersecurity for high-risk AI systems (Art. 15 AIA) and general-purpose AI
models (Art. 55 AIA). We show that robustness and cybersecurity demand
resilience against performance disruptions. Furthermore, we assess potential
challenges in implementing these provisions in light of recent advancements in
the machine learning (ML) literature. Our analysis informs efforts to develop
harmonized standards, guidelines by the European Commission, as well as
benchmarks and measurement methodologies under Art. 15(2) AIA. With this, we
seek to bridge the gap between legal terminology and ML research, fostering a
better alignment between research and implementation efforts.


---

**[16. [2206.07506] Legal Provocations for HCI in the Design and Development of Trustworthy
  Autonomous Systems](https://arxiv.org/pdf/2206.07506.pdf)** (2022-06-16)

*Lachlan D. Urquhart, Glenn McGarry, Andy Crabtree*

  We consider a series of legal provocations emerging from the proposed
European Union AI Act 2021 (AIA) and how they open up new possibilities for HCI
in the design and development of trustworthy autonomous systems. The AIA
continues the by design trend seen in recent EU regulation of emerging
technologies. The AIA targets AI developments that pose risks to society and
citizens fundamental rights, introducing mandatory design and development
requirements for high-risk AI systems (HRAIS). These requirements regulate
different stages of the AI development cycle including ensuring data quality
and governance strategies, mandating testing of systems, ensuring appropriate
risk management, designing for human oversight, and creating technical
documentation. These requirements open up new opportunities for HCI that reach
beyond established concerns with the ethics and explainability of AI and
situate AI development in human-centered processes and methods of design to
enable compliance with regulation and foster societal trust in AI.


---

**[17. [2501.04014] AICat: An AI Cataloguing Approach to Support the EU AI Act](https://arxiv.org/pdf/2501.04014.pdf)** (2025-01-09)

*Delaram Golpayegani, Harshvardhan J. Pandit, Dave Lewis*

  The European Union's Artificial Intelligence Act (AI Act) requires providers
and deployers of high-risk AI applications to register their systems into the
EU database, wherein the information should be represented and maintained in an
easily-navigable and machine-readable manner. Given the uptake of open data and
Semantic Web-based approaches for other EU repositories, in particular the use
of the Data Catalogue vocabulary Application Profile (DCAT-AP), a similar
solution for managing the EU database of high-risk AI systems is needed. This
paper introduces AICat - an extension of DCAT for representing catalogues of AI
systems that provides consistency, machine-readability, searchability, and
interoperability in managing open metadata regarding AI systems. This open
approach to cataloguing ensures transparency, traceability, and accountability
in AI application markets beyond the immediate needs of high-risk AI compliance
in the EU. AICat is available online at https://w3id.org/aicat under the
CC-BY-4.0 license.


---

**[18. [2410.09078] Knowledge-Augmented Reasoning for EUAIA Compliance and Adversarial
  Robustness of LLMs](https://arxiv.org/pdf/2410.09078.pdf)** (2024-10-15)

*Tomas Bueno Momcilovic, Dian Balta, Beat Buesser, Giulio Zizzo, Mark Purcell*

  The EU AI Act (EUAIA) introduces requirements for AI systems which intersect
with the processes required to establish adversarial robustness. However, given
the ambiguous language of regulation and the dynamicity of adversarial attacks,
developers of systems with highly complex models such as LLMs may find their
effort to be duplicated without the assurance of having achieved either
compliance or robustness. This paper presents a functional architecture that
focuses on bridging the two properties, by introducing components with clear
reference to their source. Taking the detection layer recommended by the
literature, and the reporting layer required by the law, we aim to support
developers and auditors with a reasoning layer based on knowledge augmentation
(rules, assurance cases, contextual mappings). Our findings demonstrate a novel
direction for ensuring LLMs deployed in the EU are both compliant and
adversarially robust, which underpin trustworthiness.


---

**[19. [2407.06234] The US Algorithmic Accountability Act of 2022 vs. The EU Artificial
  Intelligence Act: What can they learn from each other?](https://arxiv.org/pdf/2407.06234.pdf)** (2024-07-10)

*Jakob Mokander, Prathm Juneja, David Watson, Luciano Floridi*

  On the whole, the U.S. Algorithmic Accountability Act of 2022 (US AAA) is a
pragmatic approach to balancing the benefits and risks of automated decision
systems. Yet there is still room for improvement. This commentary highlights
how the US AAA can both inform and learn from the European Artificial
Intelligence Act (EU AIA).


---

**[20. [2409.07473] Ethical AI Governance: Methods for Evaluating Trustworthy AI](https://arxiv.org/pdf/2409.07473.pdf)** (2024-09-13)

*Louise McCormack, Malika Bendechache*

  Trustworthy Artificial Intelligence (TAI) integrates ethics that align with
human values, looking at their influence on AI behaviour and decision-making.
Primarily dependent on self-assessment, TAI evaluation aims to ensure ethical
standards and safety in AI development and usage. This paper reviews the
current TAI evaluation methods in the literature and offers a classification,
contributing to understanding self-assessment methods in this field.


---

**[21. [2011.10672] AI Governance for Businesses](https://arxiv.org/pdf/2011.10672.pdf)** (2022-06-28)

*Johannes Schneider, Rene Abraham, Christian Meske, Jan vom Brocke*

  Artificial Intelligence (AI) governance regulates the exercise of authority
and control over the management of AI. It aims at leveraging AI through
effective use of data and minimization of AI-related cost and risk. While
topics such as AI governance and AI ethics are thoroughly discussed on a
theoretical, philosophical, societal and regulatory level, there is limited
work on AI governance targeted to companies and corporations. This work views
AI products as systems, where key functionality is delivered by machine
learning (ML) models leveraging (training) data. We derive a conceptual
framework by synthesizing literature on AI and related fields such as ML. Our
framework decomposes AI governance into governance of data, (ML) models and
(AI) systems along four dimensions. It relates to existing IT and data
governance frameworks and practices. It can be adopted by practitioners and
academics alike. For practitioners the synthesis of mainly research papers, but
also practitioner publications and publications of regulatory bodies provides a
valuable starting point to implement AI governance, while for academics the
paper highlights a number of areas of AI governance that deserve more
attention.


---

**[22. [2402.14728] The European Commitment to Human-Centered Technology: The Integral Role
  of HCI in the EU AI Act's Success](https://arxiv.org/pdf/2402.14728.pdf)** (2024-06-14)

*André Calero Valdez, Moreen Heine, Thomas Franke, Nicole Jochems, Hans-Christian Jetter, Tim Schrills*

  The evolution of AI is set to profoundly reshape the future. The European
Union, recognizing this impending prominence, has enacted the AI Act,
regulating market access for AI-based systems. A salient feature of the Act is
to guard democratic and humanistic values by focusing regulation on
transparency, explainability, and the human ability to understand and control
AI systems. Hereby, the EU AI Act does not merely specify technological
requirements for AI systems. The EU issues a democratic call for human-centered
AI systems and, in turn, an interdisciplinary research agenda for
human-centered innovation in AI development. Without robust methods to assess
AI systems and their effect on individuals and society, the EU AI Act may lead
to repeating the mistakes of the General Data Protection Regulation of the EU
and to rushed, chaotic, ad-hoc, and ambiguous implementation, causing more
confusion than lending guidance. Moreover, determined research activities in
Human-AI interaction will be pivotal for both regulatory compliance and the
advancement of AI in a manner that is both ethical and effective. Such an
approach will ensure that AI development aligns with human values and needs,
fostering a technology landscape that is innovative, responsible, and an
integral part of our society.


---

**[23. [2502.03468] AI Governance in the Context of the EU AI Act: A Bibliometric and
  Literature Review Approach](https://arxiv.org/pdf/2502.03468.pdf)** (2025-02-07)

*Byeong-Je Kim, Seunghoo Jeong, Bong-Kyung Cho, Ji-Bum Chung*

  The rapid advancement of artificial intelligence (AI) has brought about
significant societal changes, necessitating robust AI governance frameworks.
This study analyzed the research trends in AI governance within the framework
of the EU AI Act. This study conducted a bibliometric analysis to examine the
publications indexed in the Web of Science database. Our findings reveal that
research on AI governance, particularly concerning AI systems regulated by the
EU AI Act, remains relatively limited compared to the broader AI research
landscape. Nonetheless, a growing interdisciplinary interest in AI governance
is evident, with notable contributions from multi-disciplinary journals and
open-access publications. Dominant research themes include ethical
considerations, privacy concerns, and the growing impact of generative AI, such
as ChatGPT. Notably, education, healthcare, and worker management are prominent
application domains. Keyword network analysis highlights education, ethics, and
ChatGPT as central keywords, underscoring the importance of these areas in
current AI governance research. Subsequently, a comprehensive literature review
was undertaken based on the bibliometric analysis findings to identify research
trends, challenges, and insights within the categories of the EU AI Act. The
findings provide valuable insights for researchers and policymakers, informing
future research directions and contributing to developing comprehensive AI
governance frameworks beyond the EU AI Act.


---

**[24. [2411.08054] GREI Data Repository AI Taxonomy](https://arxiv.org/pdf/2411.08054.pdf)** (2024-11-14)

*California Digital Library  John Chodacki, figshare  Mark Hanhel, Dataverse  Stefano Iacus, Dryad  Ryan Scherle, Center for Open
  Science  Eric Olson, Center for Open Science  Nici Pfeiffer, Zenodo  Kristi Holmes, Zenodo  Mohammad Hosseini*

  The Generalist Repository Ecosystem Initiative (GREI), funded by the NIH,
developed an AI taxonomy tailored to data repository roles to guide AI
integration across repository management. It categorizes the roles into stages,
including acquisition, validation, organization, enhancement, analysis,
sharing, and user support, providing a structured framework for implementing AI
in repository workflows.


---

**[25. [2208.12645] The Brussels Effect and Artificial Intelligence: How EU regulation will
  impact the global AI market](https://arxiv.org/pdf/2208.12645.pdf)** (2022-08-29)

*Charlotte Siegmann, Markus Anderljung*

  The European Union is likely to introduce among the first, most stringent,
and most comprehensive AI regulatory regimes of the world's major
jurisdictions. In this report, we ask whether the EU's upcoming regulation for
AI will diffuse globally, producing a so-called "Brussels Effect". Building on
and extending Anu Bradford's work, we outline the mechanisms by which such
regulatory diffusion may occur. We consider both the possibility that the EU's
AI regulation will incentivise changes in products offered in non-EU countries
(a de facto Brussels Effect) and the possibility it will influence regulation
adopted by other jurisdictions (a de jure Brussels Effect). Focusing on the
proposed EU AI Act, we tentatively conclude that both de facto and de jure
Brussels effects are likely for parts of the EU regulatory regime. A de facto
effect is particularly likely to arise in large US tech companies with AI
systems that the AI Act terms "high-risk". We argue that the upcoming
regulation might be particularly important in offering the first and most
influential operationalisation of what it means to develop and deploy
trustworthy or human-centred AI. If the EU regime is likely to see significant
diffusion, ensuring it is well-designed becomes a matter of global importance.


---

**[26. [2408.11249] The Dilemma of Uncertainty Estimation for General Purpose AI in the EU
  AI Act](https://arxiv.org/pdf/2408.11249.pdf)** (2024-08-22)

*Matias Valdenegro-Toro, Radina Stoykova*

  The AI act is the European Union-wide regulation of AI systems. It includes
specific provisions for general-purpose AI models which however need to be
further interpreted in terms of technical standards and state-of-art studies to
ensure practical compliance solutions. This paper examines the AI act
requirements for providers and deployers of general-purpose AI and further
proposes uncertainty estimation as a suitable measure for legal compliance and
quality assurance in training of such models. We argue that uncertainty
estimation should be a required component for deploying models in the real
world, and under the EU AI Act, it could fulfill several requirements for
transparency, accuracy, and trustworthiness. However, generally using
uncertainty estimation methods increases the amount of computation, producing a
dilemma, as computation might go over the threshold ($10^{25}$ FLOPS) to
classify the model as a systemic risk system which bears more regulatory
burden.


---

**[27. [2411.08363] On Algorithmic Fairness and the EU Regulations](https://arxiv.org/pdf/2411.08363.pdf)** (2024-12-24)

*Jukka Ruohonen*

  The short paper discusses algorithmic fairness by focusing on
non-discrimination and a few important laws in the European Union (EU). In
addition to the EU laws addressing discrimination explicitly, the discussion is
based on the EU's recently enacted regulation for artificial intelligence (AI)
and the older General Data Protection Regulation (GDPR). Through a theoretical
scenario analysis, on one hand, the paper demonstrates that correcting
discriminatory biases in AI systems can be legally done under the EU
regulations. On the other hand, the scenarios also illustrate some practical
scenarios from which legal non-compliance may follow. With these scenarios and
the accompanying discussion, the paper contributes to the algorithmic fairness
research with a few legal insights, enlarging and strengthening also the
growing research domain of compliance in AI engineering.


---

**[28. [2212.03109] Risk management in the Artificial Intelligence Act](https://arxiv.org/pdf/2212.03109.pdf)** (2024-11-20)

*Jonas Schuett*

  The proposed EU AI Act is the first comprehensive attempt to regulate AI in a
major jurisdiction. This article analyses Article 9, the key risk management
provision in the AI Act. It gives an overview of the regulatory concept behind
Article 9, determines its purpose and scope of application, offers a
comprehensive interpretation of the specific risk management requirements, and
outlines ways in which the requirements can be enforced. This article is
written with the aim of helping providers of high-risk systems comply with the
requirements set out in Article 9. In addition, it can inform revisions of the
current draft of the AI Act and efforts to develop harmonised standards on AI
risk management.


---

**[29. [2404.11476] Taxonomy to Regulation: A (Geo)Political Taxonomy for AI Risks and
  Regulatory Measures in the EU AI Act](https://arxiv.org/pdf/2404.11476.pdf)** (2024-04-18)

*Sinan Arda*

  Technological innovations have shown remarkable capabilities to benefit and
harm society alike. AI constitutes a democratized sophisticated technology
accessible to large parts of society, including malicious actors. This work
proposes a taxonomy focusing on on (geo)political risks associated with AI. It
identifies 12 risks in total divided into four categories: (1) Geopolitical
Pressures, (2) Malicious Usage, (3) Environmental, Social, and Ethical Risks,
and (4) Privacy and Trust Violations. Incorporating a regulatory side, this
paper conducts a policy assessment of the EU AI Act. Adopted in March 2023, the
landmark regulation has the potential to have a positive top-down impact
concerning AI risk reduction but needs regulatory adjustments to mitigate risks
more comprehensively. Regulatory exceptions for open-source models, excessively
high parameters for the classification of GPAI models as a systemic risk, and
the exclusion of systems designed exclusively for military purposes from the
regulation's obligations leave room for future action.


---

**[30. [2403.07904] Addressing the regulatory gap: moving towards an EU AI audit ecosystem
  beyond the AI Act by including civil society](https://arxiv.org/pdf/2403.07904.pdf)** (2025-02-20)

*David Hartmann, José Renato Laranjeira de Pereira, Chiara Streitbörger, Bettina Berendt*

  The European legislature has proposed the Digital Services Act (DSA) and
Artificial Intelligence Act (AIA) to regulate platforms and Artificial
Intelligence (AI) products. We review to what extent third-party audits are
part of both laws and how is access to information on models and the data
provided. By considering the value of third-party audits and third-party data
access in an audit ecosystem, we identify a regulatory gap in that the AIA does
not provide access to data for researchers and civil society. Our contributions
to the literature include: (1) Defining an AI audit ecosystem incorporating
compliance and oversight. (2) Highlighting a regulatory gap within the DSA and
AIA regulatory framework, preventing the establishment of an AI audit ecosystem
that has effective oversight by civil society and academia. (3) Emphasizing
that third-party audits by research and civil society must be part of that
ecosystem, we call for AIA amendments and delegated acts to include data and
model access for certain AI products. Furthermore, we call for the DSA to
provide NGOs and investigative journalists with data access to platforms by
delegated acts and for adaptions and amendments of the AIA to provide
third-party audits and data and model access, at least for high-risk systems.
Regulations modeled after EU AI regulations should enable data access and
third-party audits, fostering an AI audit ecosystem that promotes compliance
and oversight mechanisms.


---

**[31. [2410.19749] Using AI Alignment Theory to understand the potential pitfalls of
  regulatory frameworks](https://arxiv.org/pdf/2410.19749.pdf)** (2024-10-29)

*Alejandro Tlaie*

  This paper leverages insights from Alignment Theory (AT) research, which
primarily focuses on the potential pitfalls of technical alignment in
Artificial Intelligence, to critically examine the European Union's Artificial
Intelligence Act (EU AI Act). In the context of AT research, several key
failure modes - such as proxy gaming, goal drift, reward hacking or
specification gaming - have been identified. These can arise when AI systems
are not properly aligned with their intended objectives. The central logic of
this report is: what can we learn if we treat regulatory efforts in the same
way as we treat advanced AI systems? As we systematically apply these concepts
to the EU AI Act, we uncover potential vulnerabilities and areas for
improvement in the regulation.


---

**[32. [2407.11271] An Analysis of European Data and AI Regulations for Automotive
  Organizations](https://arxiv.org/pdf/2407.11271.pdf)** (2024-07-22)

*Charlotte A. Shahlaei, Nicholas Berente*

  This report summarizes the European Union's series of data and AI regulations
and analyzes them for managers in automotive vehicle manufacturing
organizations. In particular, we highlight the relevant ideas of the
regulations, including how they find their roots in earlier legislation, how
they contradict and complement each other, as well as the business
opportunities that these regulations offer. The structure of the report is as
follows. First, we address the GDPR as the cornerstone against which the
requirements of other regulations are weighed and legislated. Second, we
explain the EU Data Act since it directly addresses Internet of Things (IoT)
for businesses in the private sector and imposes strict requirements on large
data generators such as vehicle manufacturers. For manufacturers, compliance
with the EU Data Act is a prerequisite for the subsequent legislation, in
particular the EU AI Act. Third, we explain the Data Governance Act, Digital
Services Act, Digital Markets Act, and EU AI Act in chronological order.
Overall, we characterize European Union data regulations as a wave set, rooted
in historical precedent, with important implications for the automotive
industry.


---

**[33. [2410.07959] COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking
  Suite for the EU Artificial Intelligence Act](https://arxiv.org/pdf/2410.07959.pdf)** (2025-02-04)

*Philipp Guldimann, Alexander Spiridonov, Robin Staab, Nikola Jovanović, Mark Vero, Velko Vechev, Anna-Maria Gueorguieva, Mislav Balunović, Nikola Konstantinov, Pavol Bielik, Petar Tsankov, Martin Vechev*

  The EU's Artificial Intelligence Act (AI Act) is a significant step towards
responsible AI development, but lacks clear technical interpretation, making it
difficult to assess models' compliance. This work presents COMPL-AI, a
comprehensive framework consisting of (i) the first technical interpretation of
the EU AI Act, translating its broad regulatory requirements into measurable
technical requirements, with the focus on large language models (LLMs), and
(ii) an open-source Act-centered benchmarking suite, based on thorough
surveying and implementation of state-of-the-art LLM benchmarks. By evaluating
12 prominent LLMs in the context of COMPL-AI, we reveal shortcomings in
existing models and benchmarks, particularly in areas like robustness, safety,
diversity, and fairness. This work highlights the need for a shift in focus
towards these aspects, encouraging balanced development of LLMs and more
comprehensive regulation-aligned benchmarks. Simultaneously, COMPL-AI for the
first time demonstrates the possibilities and difficulties of bringing the
Act's obligations to a more concrete, technical level. As such, our work can
serve as a useful first step towards having actionable recommendations for
model providers, and contributes to ongoing efforts of the EU to enable
application of the Act, such as the drafting of the GPAI Code of Practice.


---

**[34. [2308.02435] Designing Fiduciary Artificial Intelligence](https://arxiv.org/pdf/2308.02435.pdf)** (2023-08-07)

*Sebastian Benthall, David Shekman*

  A fiduciary is a trusted agent that has the legal duty to act with loyalty
and care towards a principal that employs them. When fiduciary organizations
interact with users through a digital interface, or otherwise automate their
operations with artificial intelligence, they will need to design these AI
systems to be compliant with their duties. This article synthesizes recent work
in computer science and law to develop a procedure for designing and auditing
Fiduciary AI. The designer of a Fiduciary AI should understand the context of
the system, identify its principals, and assess the best interests of those
principals. Then the designer must be loyal with respect to those interests,
and careful in an contextually appropriate way. We connect the steps in this
procedure to dimensions of Trustworthy AI, such as privacy and alignment.
Fiduciary AI is a promising means to address the incompleteness of data
subject's consent when interacting with complex technical systems.


---

**[35. [2211.10859] A Blockchain Protocol for Human-in-the-Loop AI](https://arxiv.org/pdf/2211.10859.pdf)** (2022-11-22)

*Nassim Dehouche, Richard Blythman*

  Intelligent human inputs are required both in the training and operation of
AI systems, and within the governance of blockchain systems and decentralized
autonomous organizations (DAOs). This paper presents a formal definition of
Human Intelligence Primitives (HIPs), and describes the design and
implementation of an Ethereum protocol for their on-chain collection, modeling,
and integration in machine learning workflows.


---

**[36. [2502.10036] Automation Bias in the AI Act: On the Legal Implications of Attempting
  to De-Bias Human Oversight of AI](https://arxiv.org/pdf/2502.10036.pdf)** (2025-02-17)

*Johann Laux, Hannah Ruschemeier*

  This paper examines the legal implications of the explicit mentioning of
automation bias (AB) in the Artificial Intelligence Act (AIA). The AIA mandates
human oversight for high-risk AI systems and requires providers to enable
awareness of AB, i.e., the tendency to over-rely on AI outputs. The paper
analyses how this extra-juridical concept is embedded in the AIA, the division
of responsibility between AI providers and deployers, and the challenges of
legally enforcing this novel awareness requirement. The analysis shows that the
AIA's focus on providers does not adequately address design and context as
causes of AB, and questions whether the AIA should directly regulate the risk
of AB rather than just mandating awareness. As the AIA's approach requires a
balance between legal mandates and behavioural science, the paper proposes that
harmonised standards should reference the state of research on AB and human-AI
interaction. Ultimately, further empirical research will be essential for
effective safeguards.


---

**[37. [2501.01738] Mapping Compliance: A Taxonomy for Political Content Analysis under the
  EU's Digital Electoral Framework](https://arxiv.org/pdf/2501.01738.pdf)** (2025-01-06)

*Marie-Therese Sekwenz, Rita Gsenger*

  The rise of digital platforms has transformed political campaigning,
introducing complex regulatory challenges. This paper presents a comprehensive
taxonomy for analyzing political content in the EU's digital electoral
landscape, aligning with the requirements set forth in new regulations, such as
the Digital Services Act. Using a legal doctrinal methodology, we construct a
detailed codebook that enables systematic content analysis across
user-generated and political ad content to assess compliance with regulatory
mandates.


---

**[38. [2101.02039] A survey of the European Union's artificial intelligence ecosystem](https://arxiv.org/pdf/2101.02039.pdf)** (2021-01-07)

*Charlotte Stix*

  Compared to other global powers, the European Union (EU) is rarely considered
a leading player in the development of artificial intelligence (AI). Why is
this, and does this in fact accurately reflect the activities of the EU? What
would it take for the EU to take a more leading role in AI? This report surveys
core components of the current AI ecosystem of the EU, providing the crucial
background context for answering these questions.


---

**[39. [2503.05758] ADAPT Centre Contribution on Implementation of the EU AI Act and
  Fundamental Right Protection](https://arxiv.org/pdf/2503.05758.pdf)** (2025-03-11)

*Dave Lewis, Marta Lasek-Markey, Harshvardhan J. Pandit, Delaram Golpayegani, Darren McCabe, Louise McCormack, Joshua Hovsha, Deirdre Ahern, Arthit Suriyawongku*

  This document represents the ADAPT Centre's submission to the Irish
Department of Enterprise, Trade and Employment (DETE) regarding the public
consultation on implementation of the EU AI Act.


---

**[40. [2109.00838] An Automated Framework for Supporting Data-Governance Rule Compliance in
  Decentralized MIMO Contexts](https://arxiv.org/pdf/2109.00838.pdf)** (2021-09-03)

*Rui Zhao*

  We propose Dr.Aid, a logic-based AI framework for automated compliance
checking of data governance rules over data-flow graphs. The rules are modelled
using a formal language based on situation calculus and are suitable for
decentralized contexts with multi-input-multi-output (MIMO) processes. Dr.Aid
models data rules and flow rules and checks compliance by reasoning about the
propagation, combination, modification and application of data rules over the
data flow graphs. Our approach is driven and evaluated by real-world datasets
using provenance graphs from data-intensive research.


---

**[41. [2110.07033] Compliance checking in reified IO logic via SHACL](https://arxiv.org/pdf/2110.07033.pdf)** (2021-10-15)

*Livio Robaldo, Kolawole J. Adebayo*

  Reified Input/Output (I/O) logic[21] has been recently proposed to model
real-world norms in terms of the logic in [11]. This is massively grounded on
the notion of reification, and it has specifically designed to model meaning of
natural language sentences, such as the ones occurring in existing legislation.
This paper presents a methodology to carry out compliance checking on reified
I/O logic formulae. These are translated in SHACL (Shapes Constraint Language)
shapes, a recent W3C recommendation to validate and reason with RDF
triplestores. Compliance checking is then enforced by validating RDF graphs
describing states of affairs with respect to these SHACL shapes.


---

**[42. [2503.08725] The Algorithmic State Architecture (ASA): An Integrated Framework for
  AI-Enabled Government](https://arxiv.org/pdf/2503.08725.pdf)** (2025-03-14)

*Zeynep Engin, Jon Crowcroft, David Hand, Philip Treleaven*

  As artificial intelligence transforms public sector operations, governments
struggle to integrate technological innovations into coherent systems for
effective service delivery. This paper introduces the Algorithmic State
Architecture (ASA), a novel four-layer framework conceptualising how Digital
Public Infrastructure, Data-for-Policy, Algorithmic Government/Governance, and
GovTech interact as an integrated system in AI-enabled states. Unlike
approaches that treat these as parallel developments, ASA positions them as
interdependent layers with specific enabling relationships and feedback
mechanisms. Through comparative analysis of implementations in Estonia,
Singapore, India, and the UK, we demonstrate how foundational digital
infrastructure enables systematic data collection, which powers algorithmic
decision-making processes, ultimately manifesting in user-facing services. Our
analysis reveals that successful implementations require balanced development
across all layers, with particular attention to integration mechanisms between
them. The framework contributes to both theory and practice by bridging
previously disconnected domains of digital government research, identifying
critical dependencies that influence implementation success, and providing a
structured approach for analysing the maturity and development pathways of
AI-enabled government systems.


---

**[43. [2408.17222] How Could Generative AI Support Compliance with the EU AI Act? A Review
  for Safe Automated Driving Perception](https://arxiv.org/pdf/2408.17222.pdf)** (2025-04-07)

*Mert Keser, Youssef Shoeb, Alois Knoll*

  Deep Neural Networks (DNNs) have become central for the perception functions
of autonomous vehicles, substantially enhancing their ability to understand and
interpret the environment. However, these systems exhibit inherent limitations
such as brittleness, opacity, and unpredictable behavior in out-of-distribution
scenarios. The European Union (EU) Artificial Intelligence (AI) Act, as a
pioneering legislative framework, aims to address these challenges by
establishing stringent norms and standards for AI systems, including those used
in autonomous driving (AD), which are categorized as high-risk AI. In this
work, we explore how the newly available generative AI models can potentially
support addressing upcoming regulatory requirements in AD perception,
particularly with respect to safety. This short review paper summarizes the
requirements arising from the EU AI Act regarding DNN-based perception systems
and systematically categorizes existing generative AI applications in AD. While
generative AI models show promise in addressing some of the EU AI Acts
requirements, such as transparency and robustness, this review examines their
potential benefits and discusses how developers could leverage these methods to
enhance compliance with the Act. The paper also highlights areas where further
research is needed to ensure reliable and safe integration of these
technologies.


---

**[44. [2408.16074] Verification methods for international AI agreements](https://arxiv.org/pdf/2408.16074.pdf)** (2024-11-06)

*Akash R. Wasil, Tom Reed, Jack William Miller, Peter Barnett*

  What techniques can be used to verify compliance with international
agreements about advanced AI development? In this paper, we examine 10
verification methods that could detect two types of potential violations:
unauthorized AI training (e.g., training runs above a certain FLOP threshold)
and unauthorized data centers. We divide the verification methods into three
categories: (a) national technical means (methods requiring minimal or no
access from suspected non-compliant nations), (b) access-dependent methods
(methods that require approval from the nation suspected of unauthorized
activities), and (c) hardware-dependent methods (methods that require rules
around advanced hardware). For each verification method, we provide a
description, historical precedents, and possible evasion techniques. We
conclude by offering recommendations for future work related to the
verification and enforcement of international AI governance agreements.


---

**[45. [2107.03721] Demystifying the Draft EU Artificial Intelligence Act](https://arxiv.org/pdf/2107.03721.pdf)** (2022-06-14)

*Michael Veale, Frederik Zuiderveen Borgesius*

  In April 2021, the European Commission proposed a Regulation on Artificial
Intelligence, known as the AI Act. We present an overview of the Act and
analyse its implications, drawing on scholarship ranging from the study of
contemporary AI practices to the structure of EU product safety regimes over
the last four decades. Aspects of the AI Act, such as different rules for
different risk-levels of AI, make sense. But we also find that some provisions
of the Draft AI Act have surprising legal implications, whilst others may be
largely ineffective at achieving their stated goals. Several overarching
aspects, including the enforcement regime and the risks of maximum
harmonisation pre-empting legitimate national AI policy, engender significant
concern. These issues should be addressed as a priority in the legislative
process.


---

**[46. [1606.08514] Towards Verified Artificial Intelligence](https://arxiv.org/pdf/1606.08514.pdf)** (2020-07-24)

*Sanjit A. Seshia, Dorsa Sadigh, S. Shankar Sastry*

  Verified artificial intelligence (AI) is the goal of designing AI-based
systems that that have strong, ideally provable, assurances of correctness with
respect to mathematically-specified requirements. This paper considers Verified
AI from a formal methods perspective. We describe five challenges for achieving
Verified AI, and five corresponding principles for addressing these challenges.


---

**[47. [2310.04072] AI Regulation in Europe: From the AI Act to Future Regulatory Challenges](https://arxiv.org/pdf/2310.04072.pdf)** (2023-10-09)

*Philipp Hacker*

  This chapter provides a comprehensive discussion on AI regulation in the
European Union, contrasting it with the more sectoral and self-regulatory
approach in the UK. It argues for a hybrid regulatory strategy that combines
elements from both philosophies, emphasizing the need for agility and safe
harbors to ease compliance. The paper examines the AI Act as a pioneering
legislative effort to address the multifaceted challenges posed by AI,
asserting that, while the Act is a step in the right direction, it has
shortcomings that could hinder the advancement of AI technologies. The paper
also anticipates upcoming regulatory challenges, such as the management of
toxic content, environmental concerns, and hybrid threats. It advocates for
immediate action to create protocols for regulated access to high-performance,
potentially open-source AI systems. Although the AI Act is a significant
legislative milestone, it needs additional refinement and global collaboration
for the effective governance of rapidly evolving AI technologies.


---

**[48. [2502.14868] Unlocking the Black Box: Analysing the EU Artificial Intelligence Act's
  Framework for Explainability in AI](https://arxiv.org/pdf/2502.14868.pdf)** (2025-02-24)

*Georgios Pavlidis*

  The lack of explainability of Artificial Intelligence (AI) is one of the
first obstacles that the industry and regulators must overcome to mitigate the
risks associated with the technology. The need for eXplainable AI (XAI) is
evident in fields where accountability, ethics and fairness are critical, such
as healthcare, credit scoring, policing and the criminal justice system. At the
EU level, the notion of explainability is one of the fundamental principles
that underpin the AI Act, though the exact XAI techniques and requirements are
still to be determined and tested in practice. This paper explores various
approaches and techniques that promise to advance XAI, as well as the
challenges of implementing the principle of explainability in AI governance and
policies. Finally, the paper examines the integration of XAI into EU law,
emphasising the issues of standard setting, oversight, and enforcement.


---

**[49. [2405.04528] Implementing ISO/IEC TS 27560:2023 Consent Records and Receipts for GDPR
  and DGA](https://arxiv.org/pdf/2405.04528.pdf)** (2024-05-08)

*Harshvardhan J. Pandit, Jan Lindquist, Georg P. Krog*

  The ISO/IEC TS 27560:2023 Privacy technologies - Consent record information
structure provides guidance for the creation and maintenance of records
regarding consent as machine-readable information. It also provides guidance on
the use of this information to exchange such records between entities in the
form of 'receipts'. In this article, we compare requirements regarding consent
between ISO/IEC TS 27560:2023, ISO/IEC 29184:2020 Privacy Notices, and the EU's
General Data Protection Regulation (GDPR) to show how these standards can be
used to support GDPR compliance. We then use the Data Privacy Vocabulary (DPV)
to implement ISO/IEC TS 27560:2023 and create interoperable consent records and
receipts. We also discuss how this work benefits the the implementation of EU
Data Governance Act (DGA), specifically for machine-readable consent forms.


---

**[50. [2308.15514] International Governance of Civilian AI: A Jurisdictional Certification
  Approach](https://arxiv.org/pdf/2308.15514.pdf)** (2023-09-12)

*Robert Trager, Ben Harack, Anka Reuel, Allison Carnegie, Lennart Heim, Lewis Ho, Sarah Kreps, Ranjit Lall, Owen Larter, Seán Ó hÉigeartaigh, Simon Staffell, José Jaime Villalobos*

  This report describes trade-offs in the design of international governance
arrangements for civilian artificial intelligence (AI) and presents one
approach in detail. This approach represents the extension of a standards,
licensing, and liability regime to the global level. We propose that states
establish an International AI Organization (IAIO) to certify state
jurisdictions (not firms or AI projects) for compliance with international
oversight standards. States can give force to these international standards by
adopting regulations prohibiting the import of goods whose supply chains embody
AI from non-IAIO-certified jurisdictions. This borrows attributes from models
of existing international organizations, such as the International Civilian
Aviation Organization (ICAO), the International Maritime Organization (IMO),
and the Financial Action Task Force (FATF). States can also adopt multilateral
controls on the export of AI product inputs, such as specialized hardware, to
non-certified jurisdictions. Indeed, both the import and export standards could
be required for certification. As international actors reach consensus on risks
of and minimum standards for advanced AI, a jurisdictional certification regime
could mitigate a broad range of potential harms, including threats to public
safety.


---

**[51. [2408.01449] AI Act for the Working Programmer](https://arxiv.org/pdf/2408.01449.pdf)** (2024-08-06)

*Holger Hermanns, Anne Lauber-Rönsberg, Philip Meinel, Sarah Sterz, Hanwei Zhang*

  The European AI Act is a new, legally binding instrument that will enforce
certain requirements on the development and use of AI technology potentially
affecting people in Europe. It can be expected that the stipulations of the
Act, in turn, are going to affect the work of many software engineers, software
testers, data engineers, and other professionals across the IT sector in Europe
and beyond. The 113 articles, 180 recitals, and 13 annexes that make up the Act
cover 144 pages. This paper aims at providing an aid for navigating the Act
from the perspective of some professional in the software domain, termed "the
working programmer", who feels the need to know about the stipulations of the
Act.


---

**[52. [2503.05787] Mapping the Regulatory Learning Space for the EU AI Act](https://arxiv.org/pdf/2503.05787.pdf)** (2025-03-11)

*Dave Lewis, Marta Lasek-Markey, Delaram Golpayegani, Harshvardhan J. Pandit*

  The EU's AI Act represents the world first transnational AI regulation with
concrete enforcement measures. It builds upon existing EU mechanisms for
product health and safety regulation, but extends it to protect fundamental
rights and by addressing AI as a horizontal technology that is regulated across
multiple vertical application sectors. These extensions introduce uncertainties
in terms of how the technical state of the art will be applied to AI system
certification and enforcement actions, how horizontal technical measures will
map into vertical enforcement responsibilities and the degree to which
different fundamental rights can be protected across EU Member States. We argue
that these uncertainties, coupled with the fast changing nature of AI and the
relative immaturity of the state of the art in fundamental rights risk
management require the implementation of the AI Act to place a strong emphasis
on comprehensive and rapid regulatory learning. We define parameterised axes
for the regulatory learning space set out in the Act and describe a layered
system of different learning arenas where the population of oversight
authorities, value chain participants and affected stakeholders may interact to
apply and learn from technical, organisational and legal implementation
measures. We conclude by exploring how existing open data policies and
practices in the EU can be adapted to support regulatory learning in a
transparent manner that supports the development of trust in and predictability
of regulated AI. We discuss how the Act may result in a regulatory turn in the
research of AI fairness, accountability and transparency towards investigations
into implementations of and interactions between different fundamental rights
protections and reproducible and accountable models of metrology for AI risk
assessment and treatment.


---

**[53. [2412.12108] Responsible AI Governance: A Response to UN Interim Report on Governing
  AI for Humanity](https://arxiv.org/pdf/2412.12108.pdf)** (2025-01-03)

*Sarah Kiden, Bernd Stahl, Beverley Townsend, Carsten Maple, Charles Vincent, Fraser Sampson, Geoff Gilbert, Helen Smith, Jayati Deshmukh, Jen Ross, Jennifer Williams, Jesus Martinez del Rincon, Justyna Lisinska, Karen O'Shea, Márjory Da Costa Abreu, Nelly Bencomo, Oishi Deb, Peter Winter, Phoebe Li, Philip Torr, Pin Lean Lau, Raquel Iniesta, Gopal Ramchurn, Sebastian Stein, Vahid Yazdanpanah*

  This report presents a comprehensive response to the United Nation's Interim
Report on Governing Artificial Intelligence (AI) for Humanity. It emphasizes
the transformative potential of AI in achieving the Sustainable Development
Goals (SDGs) while acknowledging the need for robust governance to mitigate
associated risks. The response highlights opportunities for promoting
equitable, secure, and inclusive AI ecosystems, which should be supported by
investments in infrastructure and multi-stakeholder collaborations across
jurisdictions. It also underscores challenges, including societal inequalities
exacerbated by AI, ethical concerns, and environmental impacts. Recommendations
advocate for legally binding norms, transparency, and multi-layered data
governance models, alongside fostering AI literacy and capacity-building
initiatives. Internationally, the report calls for harmonising AI governance
frameworks with established laws, human rights standards, and regulatory
approaches. The report concludes with actionable principles for fostering
responsible AI governance through collaboration among governments, industry,
academia, and civil society, ensuring the development of AI aligns with
universal human values and the public good.


---

**[54. [2403.20089] Implications of the AI Act for Non-Discrimination Law and Algorithmic
  Fairness](https://arxiv.org/pdf/2403.20089.pdf)** (2024-06-27)

*Luca Deck, Jan-Laurin Müller, Conradin Braun, Domenique Zipperling, Niklas Kühl*

  The topic of fairness in AI, as debated in the FATE (Fairness,
Accountability, Transparency, and Ethics in AI) communities, has sparked
meaningful discussions in the past years. However, from a legal perspective,
particularly from the perspective of European Union law, many open questions
remain. Whereas algorithmic fairness aims to mitigate structural inequalities
at design-level, European non-discrimination law is tailored to individual
cases of discrimination after an AI model has been deployed. The AI Act might
present a tremendous step towards bridging these two approaches by shifting
non-discrimination responsibilities into the design stage of AI models. Based
on an integrative reading of the AI Act, we comment on legal as well as
technical enforcement problems and propose practical implications on bias
detection and bias correction in order to specify and comply with specific
technical requirements.


---

**[55. [2406.14724] An Exploratory Mixed-Methods Study on General Data Protection Regulation
  (GDPR) Compliance in Open-Source Software](https://arxiv.org/pdf/2406.14724.pdf)** (2024-06-24)

*Lucas Franke, Huayu Liang, Sahar Farzanehpour, Aaron Brantly, James C. Davis, Chris Brown*

  Background: Governments worldwide are considering data privacy regulations.
These laws, e.g. the European Union's General Data Protection Regulation
(GDPR), require software developers to meet privacy-related requirements when
interacting with users' data. Prior research describes the impact of such laws
on software development, but only for commercial software. Open-source software
is commonly integrated into regulated software, and thus must be engineered or
adapted for compliance. We do not know how such laws impact open-source
software development.
  Aims: To understand how data privacy laws affect open-source software
development. We studied the European Union's GDPR, the most prominent such law.
We investigated how GDPR compliance activities influence OSS developer activity
(RQ1), how OSS developers perceive fulfilling GDPR requirements (RQ2), the most
challenging GDPR requirements to implement (RQ3), and how OSS developers assess
GDPR compliance (RQ4).
  Method: We distributed an online survey to explore perceptions of GDPR
implementations from open-source developers (N=56). We further conducted a
repository mining study to analyze development metrics on pull requests
(N=31462) submitted to open-source GitHub repositories.
  Results: GDPR policies complicate open-source development processes and
introduce challenges for developers, primarily regarding the management of
users' data, implementation costs and time, and assessments of compliance.
Moreover, we observed negative perceptions of GDPR from open-source developers
and significant increases in development activity, in particular metrics
related to coding and reviewing activity, on GitHub pull requests related to
GDPR compliance.
  Conclusions: Our findings motivate policy-related resources and automated
tools to support data privacy regulation implementation and compliance efforts
in open-source software.


---

**[56. [2501.14756] Towards An Automated AI Act FRIA Tool That Can Reuse GDPR's DPIA](https://arxiv.org/pdf/2501.14756.pdf)** (2025-01-28)

*Tytti Rintamaki, Harshvardhan J. Pandit*

  The AI Act introduces the obligation to conduct a Fundamental Rights Impact
Assessment (FRIA), with the possibility to reuse a Data Protection Impact
Assessment (DPIA), and requires the EU Commission to create of an automated
tool to support the FRIA process. In this article, we provide our novel
exploration of the DPIA and FRIA as information processes to enable the
creation of automated tools. We first investigate the information involved in
DPIA and FRIA, and then use this to align the two to state where a DPIA can be
reused in a FRIA. We then present the FRIA as a 5-step process and discuss the
role of an automated tool for each step. Our work provides the necessary
foundation for creating and managing information for FRIA and supporting it
through an automated tool as required by the AI Act.


---

**[57. [2203.00469] Compliance Challenges in Forensic Image Analysis Under the Artificial
  Intelligence Act](https://arxiv.org/pdf/2203.00469.pdf)** (2022-03-02)

*Benedikt Lorch, Nicole Scheler, Christian Riess*

  In many applications of forensic image analysis, state-of-the-art results are
nowadays achieved with machine learning methods. However, concerns about their
reliability and opaqueness raise the question whether such methods can be used
in criminal investigations. So far, this question of legal compliance has
hardly been discussed, also because legal regulations for machine learning
methods were not defined explicitly. To this end, the European Commission
recently proposed the artificial intelligence (AI) act, a regulatory framework
for the trustworthy use of AI. Under the draft AI act, high-risk AI systems for
use in law enforcement are permitted but subject to compliance with mandatory
requirements. In this paper, we review why the use of machine learning in
forensic image analysis is classified as high-risk. We then summarize the
mandatory requirements for high-risk AI systems and discuss these requirements
in light of two forensic applications, license plate recognition and deep fake
detection. The goal of this paper is to raise awareness of the upcoming legal
requirements and to point out avenues for future research.


---

**[58. [2306.08959] Statutory Professions in AI governance and their consequences for
  explainable AI](https://arxiv.org/pdf/2306.08959.pdf)** (2023-06-16)

*Labhaoise NiFhaolain, Andrew Hines, Vivek Nallur*

  Intentional and accidental harms arising from the use of AI have impacted the
health, safety and rights of individuals. While regulatory frameworks are being
developed, there remains a lack of consensus on methods necessary to deliver
safe AI. The potential for explainable AI (XAI) to contribute to the
effectiveness of the regulation of AI is being increasingly examined.
Regulation must include methods to ensure compliance on an ongoing basis,
though there is an absence of practical proposals on how to achieve this. For
XAI to be successfully incorporated into a regulatory system, the individuals
who are engaged in interpreting/explaining the model to stakeholders should be
sufficiently qualified for the role. Statutory professionals are prevalent in
domains in which harm can be done to the health, safety and rights of
individuals. The most obvious examples are doctors, engineers and lawyers.
Those professionals are required to exercise skill and judgement and to defend
their decision making process in the event of harm occurring. We propose that a
statutory profession framework be introduced as a necessary part of the AI
regulatory framework for compliance and monitoring purposes. We will refer to
this new statutory professional as an AI Architect (AIA). This AIA would be
responsible to ensure the risk of harm is minimised and accountable in the
event that harms occur. The AIA would also be relied on to provide appropriate
interpretations/explanations of XAI models to stakeholders. Further, in order
to satisfy themselves that the models have been developed in a satisfactory
manner, the AIA would require models to have appropriate transparency.
Therefore it is likely that the introduction of an AIA system would lead to an
increase in the use of XAI to enable AIA to discharge their professional
obligations.


---

**[59. [2203.14122] A Runtime Environment for Contract Automata](https://arxiv.org/pdf/2203.14122.pdf)** (2023-03-16)

*Davide Basile, Maurice H. ter Beek*

  Contract automata have been introduced for specifying applications through
behavioural contracts and for synthesising their orchestrations as finite state
automata. This paper addresses the realisation of applications from contract
automata specifications. We present CARE, a new runtime environment to
coordinate services implementing contracts that guarantees the adherence of the
implementation to its contract. We discuss how CARE can be adopted to realise
contract-based applications, its formal guarantees, and we identify the
responsibilities of the involved business actors. Experiments show the benefits
of adopting CARE with respect to manual implementations.


---

**[60. [2410.17281] A Comprehensive Survey and Classification of Evaluation Criteria for
  Trustworthy Artificial Intelligence](https://arxiv.org/pdf/2410.17281.pdf)** (2024-10-24)

*Louise McCormack, Malika Bendechache*

  This paper presents a systematic review of the literature on evaluation
criteria for Trustworthy Artificial Intelligence (TAI), with a focus on the
seven EU principles of TAI. This systematic literature review identifies and
analyses current evaluation criteria, maps them to the EU TAI principles and
proposes a new classification system for each principle. The findings reveal
both a need for and significant barriers to standardising criteria for TAI
evaluation. The proposed classification contributes to the development,
selection and standardization of evaluation criteria for TAI governance.


---

**[61. [2402.18326] When Should Algorithms Resign? A Proposal for AI Governance](https://arxiv.org/pdf/2402.18326.pdf)** (2024-07-18)

*Umang Bhatt, Holli Sargeant*

  Algorithmic resignation is a strategic approach for managing the use of
artificial intelligence (AI) by embedding governance directly into AI systems.
It involves deliberate and informed disengagement from AI, such as restricting
access AI outputs or displaying performance disclaimers, in specific scenarios
to aid the appropriate and effective use of AI. By integrating algorithmic
resignation as a governance mechanism, organizations can better control when
and how AI is used, balancing the benefits of automation with the need for
human oversight.


---

**[62. [2308.16364] Strengthening the EU AI Act: Defining Key Terms on AI Manipulation](https://arxiv.org/pdf/2308.16364.pdf)** (2023-09-01)

*Matija Franklin, Philip Moreira Tomei, Rebecca Gorman*

  The European Union's Artificial Intelligence Act aims to regulate
manipulative and harmful uses of AI, but lacks precise definitions for key
concepts. This paper provides technical recommendations to improve the Act's
conceptual clarity and enforceability. We review psychological models to define
"personality traits," arguing the Act should protect full "psychometric
profiles." We urge expanding "behavior" to include "preferences" since
preferences causally influence and are influenced by behavior. Clear
definitions are provided for "subliminal," "manipulative," and "deceptive"
techniques, considering incentives, intent, and covertness. We distinguish
"exploiting individuals" from "exploiting groups," emphasising different policy
needs. An "informed decision" is defined by four facets: comprehension,
accurate information, no manipulation, and understanding AI's influence. We
caution the Act's therapeutic use exemption given the lack of regulation of
digital therapeutics by the EMA. Overall, the recommendations strengthen
definitions of vague concepts in the EU AI Act, enhancing precise applicability
to regulate harmful AI manipulation.


---

**[63. [2105.15133] An Assessment of the AI Regulation Proposed by the European Commission](https://arxiv.org/pdf/2105.15133.pdf)** (2021-06-01)

*Patrick Glauner*

  In April 2021, the European Commission published a proposed regulation on AI.
It intends to create a uniform legal framework for AI within the European Union
(EU). In this chapter, we analyze and assess the proposal. We show that the
proposed regulation is actually not needed due to existing regulations. We also
argue that the proposal clearly poses the risk of overregulation. As a
consequence, this would make the use or development of AI applications in
safety-critical application areas, such as in healthcare, almost impossible in
the EU. This would also likely further strengthen Chinese and US corporations
in their technology leadership. Our assessment is based on the oral evidence we
gave in May 2021 to the joint session of the European Union affairs committees
of the German federal parliament and the French National Assembly.


---

**[64. [2407.01557] AI Governance and Accountability: An Analysis of Anthropic's Claude](https://arxiv.org/pdf/2407.01557.pdf)** (2024-07-03)

*Aman Priyanshu, Yash Maurya, Zuofei Hong*

  As AI systems become increasingly prevalent and impactful, the need for
effective AI governance and accountability measures is paramount. This paper
examines the AI governance landscape, focusing on Anthropic's Claude, a
foundational AI model. We analyze Claude through the lens of the NIST AI Risk
Management Framework and the EU AI Act, identifying potential threats and
proposing mitigation strategies. The paper highlights the importance of
transparency, rigorous benchmarking, and comprehensive data handling processes
in ensuring the responsible development and deployment of AI systems. We
conclude by discussing the social impact of AI governance and the ethical
considerations surrounding AI accountability.


---

**[65. [2007.05479] Impact of Legal Requirements on Explainability in Machine Learning](https://arxiv.org/pdf/2007.05479.pdf)** (2020-07-13)

*Adrien Bibal, Michael Lognoul, Alexandre de Streel, Benoît Frénay*

  The requirements on explainability imposed by European laws and their
implications for machine learning (ML) models are not always clear. In that
perspective, our research analyzes explanation obligations imposed for private
and public decision-making, and how they can be implemented by machine learning
techniques.


---

**[66. [2304.02924] The Governance of Physical Artificial Intelligence](https://arxiv.org/pdf/2304.02924.pdf)** (2023-04-07)

*Yingbo Li, Anamaria-Beatrice Spulber, Yucong Duan*

  Physical artificial intelligence can prove to be one of the most important
challenges of the artificial intelligence. The governance of physical
artificial intelligence would define its responsible intelligent application in
the society.


---

**[67. [2409.15828] Mitigating Digital Discrimination in Dating Apps -- The Dutch Breeze
  case](https://arxiv.org/pdf/2409.15828.pdf)** (2024-09-25)

*Tim de Jonge, Frederik Zuiderveen Borgesius*

  In September 2023, the Netherlands Institute for Human Rights, the Dutch
non-discrimination authority, decided that Breeze, a Dutch dating app, was
justified in suspecting that their algorithm discriminated against non-white.
Consequently, the Institute decided that Breeze must prevent this
discrimination based on ethnicity. This paper explores two questions. (i) Is
the discrimination based on ethnicity in Breeze's matching algorithm illegal?
(ii) How can dating apps mitigate or stop discrimination in their matching
algorithms? We illustrate the legal and technical difficulties dating apps face
in tackling discrimination and illustrate promising solutions. We analyse the
Breeze decision in-depth, combining insights from computer science and law. We
discuss the implications of this judgment for scholarship and practice in the
field of fair and non-discriminatory machine learning.


---

**[68. [2408.04689] Design of a Quality Management System based on the EU Artificial
  Intelligence Act](https://arxiv.org/pdf/2408.04689.pdf)** (2024-11-13)

*Henryk Mustroph, Stefanie Rinderle-Ma*

  The EU AI Act mandates that providers and deployers of high-risk AI systems
establish a quality management system (QMS). Among other criteria, a QMS shall
help verify and document the AI system design and quality and monitor the
proper implementation of all high-risk AI system requirements. Current research
rarely explores practical solutions for implementing the EU AI Act. Instead, it
tends to focus on theoretical concepts. As a result, more attention must be
paid to tools that help humans actively check and document AI systems and
orchestrate the implementation of all high-risk AI system requirements.
Therefore, this paper introduces a new design concept and prototype for a QMS
as a microservice Software as a Service web application. It connects directly
to the AI system for verification and documentation and enables the
orchestration and integration of various sub-services, which can be
individually designed, each tailored to specific high-risk AI system
requirements. The first version of the prototype connects to the
Phi-3-mini-128k-instruct LLM as an example of an AI system and integrates a
risk management system and a data management system. The prototype is evaluated
through a qualitative assessment of the implemented requirements, a GPU memory
and performance analysis, and an evaluation with IT, AI, and legal experts.


---

**[69. [2401.07348] Generative AI in EU Law: Liability, Privacy, Intellectual Property, and
  Cybersecurity](https://arxiv.org/pdf/2401.07348.pdf)** (2024-03-18)

*Claudio Novelli, Federico Casolari, Philipp Hacker, Giorgio Spedicato, Luciano Floridi*

  The advent of Generative AI, particularly through Large Language Models
(LLMs) like ChatGPT and its successors, marks a paradigm shift in the AI
landscape. Advanced LLMs exhibit multimodality, handling diverse data formats,
thereby broadening their application scope. However, the complexity and
emergent autonomy of these models introduce challenges in predictability and
legal compliance. This paper delves into the legal and regulatory implications
of Generative AI and LLMs in the European Union context, analyzing aspects of
liability, privacy, intellectual property, and cybersecurity. It critically
examines the adequacy of the existing and proposed EU legislation, including
the Artificial Intelligence Act (AIA) draft, in addressing the unique
challenges posed by Generative AI in general and LLMs in particular. The paper
identifies potential gaps and shortcomings in the legislative framework and
proposes recommendations to ensure the safe and compliant deployment of
generative models, ensuring they align with the EU's evolving digital landscape
and legal standards.


---

**[70. [2311.10748] An international treaty to implement a global compute cap for advanced
  artificial intelligence](https://arxiv.org/pdf/2311.10748.pdf)** (2023-11-21)

*Andrea Miotti, Akash Wasil*

  This paper presents an international treaty to reduce risks from the
development of advanced artificial intelligence (AI). The main provision of the
treaty is a global compute cap: a ban on the development of AI systems above an
agreed-upon computational resource threshold. The treaty also proposes the
development and testing of emergency response plans, negotiations to establish
an international agency to enforce the treaty, the establishment of new
communication channels and whistleblower protections, and a commitment to avoid
an AI arms race. We hope this treaty serves as a useful template for global
leaders as they implement governance regimes to protect civilization from the
dangers of advanced artificial intelligence.


---

**[71. [2404.00600] AI Act and Large Language Models (LLMs): When critical issues and
  privacy impact require human and ethical oversight](https://arxiv.org/pdf/2404.00600.pdf)** (2024-04-03)

*Nicola Fabiano*

  The imposing evolution of artificial intelligence systems and, specifically,
of Large Language Models (LLM) makes it necessary to carry out assessments of
their level of risk and the impact they may have in the area of privacy,
personal data protection and at an ethical level, especially on the weakest and
most vulnerable. This contribution addresses human oversight, ethical
oversight, and privacy impact assessment.


---

**[72. [2205.06666] The Case for a Legal Compliance API for the Enforcement of the EU's
  Digital Services Act on Social Media Platforms](https://arxiv.org/pdf/2205.06666.pdf)** (2022-05-16)

*Catalina Goanta, Thales Bertaglia, Adriana Iamnitchi*

  In the course of under a year, the European Commission has launched some of
the most important regulatory proposals to date on platform governance. The
Commission's goals behind cross-sectoral regulation of this sort include the
protection of markets and democracies alike. While all these acts propose
sophisticated rules for setting up new enforcement institutions and procedures,
one aspect remains highly unclear: how digital enforcement will actually take
place in practice. Focusing on the Digital Services Act (DSA), this discussion
paper critically addresses issues around social media data access for the
purpose of digital enforcement and proposes the use of a legal compliance
application programming interface (API) as a means to facilitate compliance
with the DSA and complementary European and national regulation. To
contextualize this discussion, the paper pursues two scenarios that exemplify
the harms arising out of content monetization affecting a particularly
vulnerable category of social media users: children. The two scenarios are used
to further reflect upon essential issues surrounding data access and legal
compliance with the DSA and further applicable legal standards in the field of
labour and consumer law.


---

**[73. [2501.10371] What we learned while automating bias detection in AI hiring systems for
  compliance with NYC Local Law 144](https://arxiv.org/pdf/2501.10371.pdf)** (2025-01-22)

*Gemma Galdon Clavell, Rubén González-Sendino*

  Since July 5, 2023, New York City's Local Law 144 requires employers to
conduct independent bias audits for any automated employment decision tools
(AEDTs) used in hiring processes. The law outlines a minimum set of bias tests
that AI developers and implementers must perform to ensure compliance. Over the
past few months, we have collected and analyzed audits conducted under this
law, identified best practices, and developed a software tool to streamline
employer compliance. Our tool, ITACA_144, tailors our broader bias auditing
framework to meet the specific requirements of Local Law 144. While automating
these legal mandates, we identified several critical challenges that merit
attention to ensure AI bias regulations and audit methodologies are both
effective and practical. This document presents the insights gained from
automating compliance with NYC Local Law 144. It aims to support other cities
and states in crafting similar legislation while addressing the limitations of
the NYC framework. The discussion focuses on key areas including data
requirements, demographic inclusiveness, impact ratios, effective bias,
metrics, and data reliability.


---

**[74. [2503.05571] Compliance of AI Systems](https://arxiv.org/pdf/2503.05571.pdf)** (2025-03-10)

*Julius Schöning, Niklas Kruse*

  The increasing integration of artificial intelligence (AI) systems in various
fields requires solid concepts to ensure compliance with upcoming legislation.
This paper systematically examines the compliance of AI systems with relevant
legislation, focusing on the EU's AI Act and the compliance of data sets. The
analysis highlighted many challenges associated with edge devices, which are
increasingly being used to deploy AI applications closer and closer to the data
sources. Such devices often face unique issues due to their decentralized
nature and limited computing resources for implementing sophisticated
compliance mechanisms. By analyzing AI implementations, the paper identifies
challenges and proposes the first best practices for legal compliance when
developing, deploying, and running AI. The importance of data set compliance is
highlighted as a cornerstone for ensuring the trustworthiness, transparency,
and explainability of AI systems, which must be aligned with ethical standards
set forth in regulatory frameworks such as the AI Act. The insights gained
should contribute to the ongoing discourse on the responsible development and
deployment of embedded AI systems.


---

**[75. [2501.12962] It's complicated. The relationship of algorithmic fairness and
  non-discrimination regulations in the EU AI Act](https://arxiv.org/pdf/2501.12962.pdf)** (2025-03-17)

*Kristof Meding*

  What constitutes a fair decision? This question is not only difficult for
humans but becomes more challenging when Artificial Intelligence (AI) models
are used. In light of discriminatory algorithmic behaviors, the EU has recently
passed the AI Act, which mandates specific rules for AI models, incorporating
both traditional legal non-discrimination regulations and machine learning
based algorithmic fairness concepts. This paper aims to bridge these two
different concepts in the AI Act through: First a high-level introduction of
both concepts targeting legal and computer science-oriented scholars, and
second an in-depth analysis of the AI Act's relationship between legal
non-discrimination regulations and algorithmic fairness. Our analysis reveals
three key findings: (1.), most non-discrimination regulations target only
high-risk AI systems. (2.), the regulation of high-risk systems encompasses
both data input requirements and output monitoring, though these regulations
are often inconsistent and raise questions of computational feasibility. (3.)
Regulations for General Purpose AI Models, such as Large Language Models that
are not simultaneously classified as high-risk systems, currently lack
specificity compared to other regulations. Based on these findings, we
recommend developing more specific auditing and testing methodologies for AI
systems. This paper aims to serve as a foundation for future interdisciplinary
collaboration between legal scholars and computer science-oriented machine
learning researchers studying discrimination in AI systems.


---

**[76. [2503.15682] Transfeminist AI Governance](https://arxiv.org/pdf/2503.15682.pdf)** (2025-03-21)

*Blair Attard-Frost*

  This article re-imagines the governance of artificial intelligence (AI)
through a transfeminist lens, focusing on challenges of power, participation,
and injustice, and on opportunities for advancing equity, community-based
resistance, and transformative change. AI governance is a field of research and
practice seeking to maximize benefits and minimize harms caused by AI systems.
Unfortunately, AI governance practices are frequently ineffective at preventing
AI systems from harming people and the environment, with historically
marginalized groups such as trans people being particularly vulnerable to harm.
Building upon trans and feminist theories of ethics, I introduce an approach to
transfeminist AI governance. Applying a transfeminist lens in combination with
a critical self-reflexivity methodology, I retroactively reinterpret findings
from three empirical studies of AI governance practices in Canada and globally.
In three reflections on my findings, I show that large-scale AI governance
systems structurally prioritize the needs of industry over marginalized
communities. As a result, AI governance is limited by power imbalances and
exclusionary norms. This research shows that re-grounding AI governance in
transfeminist ethical principles can support AI governance researchers,
practitioners, and organizers in addressing those limitations.


---

**[77. [2401.10896] Responsible AI Governance: A Systematic Literature Review](https://arxiv.org/pdf/2401.10896.pdf)** (2024-01-23)

*Amna Batool, Didar Zowghi, Muneera Bano*

  As artificial intelligence transforms a wide range of sectors and drives
innovation, it also introduces complex challenges concerning ethics,
transparency, bias, and fairness. The imperative for integrating Responsible AI
(RAI) principles within governance frameworks is paramount to mitigate these
emerging risks. While there are many solutions for AI governance, significant
questions remain about their effectiveness in practice. Addressing this
knowledge gap, this paper aims to examine the existing literature on AI
Governance. The focus of this study is to analyse the literature to answer key
questions: WHO is accountable for AI systems' governance, WHAT elements are
being governed, WHEN governance occurs within the AI development life cycle,
and HOW it is executed through various mechanisms like frameworks, tools,
standards, policies, or models. Employing a systematic literature review
methodology, a rigorous search and selection process has been employed. This
effort resulted in the identification of 61 relevant articles on the subject of
AI Governance. Out of the 61 studies analysed, only 5 provided complete
responses to all questions. The findings from this review aid research in
formulating more holistic and comprehensive Responsible AI (RAI) governance
frameworks. This study highlights important role of AI governance on various
levels specially organisational in establishing effective and responsible AI
practices. The findings of this study provides a foundational basis for future
research and development of comprehensive governance models that align with RAI
principles.


---

**[78. [2411.13808] GPAI Evaluations Standards Taskforce: Towards Effective AI Governance](https://arxiv.org/pdf/2411.13808.pdf)** (2024-11-22)

*Patricia Paskov, Lukas Berglund, Everett Smith, Lisa Soder*

  General-purpose AI evaluations have been proposed as a promising way of
identifying and mitigating systemic risks posed by AI development and
deployment. While GPAI evaluations play an increasingly central role in
institutional decision- and policy-making -- including by way of the European
Union AI Act's mandate to conduct evaluations on GPAI models presenting
systemic risk -- no standards exist to date to promote their quality or
legitimacy. To strengthen GPAI evaluations in the EU, which currently
constitutes the first and only jurisdiction that mandates GPAI evaluations, we
outline four desiderata for GPAI evaluations: internal validity, external
validity, reproducibility, and portability. To uphold these desiderata in a
dynamic environment of continuously evolving risks, we propose a dedicated EU
GPAI Evaluation Standards Taskforce, to be housed within the bodies established
by the EU AI Act. We outline the responsibilities of the Taskforce, specify the
GPAI provider commitments that would facilitate Taskforce success, discuss the
potential impact of the Taskforce on global AI governance, and address
potential sources of failure that policymakers should heed.


---

**[79. [2503.17730] Aportes para el cumplimiento del Reglamento (UE) 2024/1689 en rob\'otica
  y sistemas aut\'onomos](https://arxiv.org/pdf/2503.17730.pdf)** (2025-03-25)

*Francisco J. Rodríguez Lera, Yoana Pita Lorenzo, David Sobrín Hidalgo, Laura Fernández Becerra, Irene González Fernández, Jose Miguel Guerrero Hernández*

  Cybersecurity in robotics stands out as a key aspect within Regulation (EU)
2024/1689, also known as the Artificial Intelligence Act, which establishes
specific guidelines for intelligent and automated systems. A fundamental
distinction in this regulatory framework is the difference between robots with
Artificial Intelligence (AI) and those that operate through automation systems
without AI, since the former are subject to stricter security requirements due
to their learning and autonomy capabilities. This work analyzes cybersecurity
tools applicable to advanced robotic systems, with special emphasis on the
protection of knowledge bases in cognitive architectures. Furthermore, a list
of basic tools is proposed to guarantee the security, integrity, and resilience
of these systems, and a practical case is presented, focused on the analysis of
robot knowledge management, where ten evaluation criteria are defined to ensure
compliance with the regulation and reduce risks in human-robot interaction
(HRI) environments.


---

**[80. [2408.11925] An Open Knowledge Graph-Based Approach for Mapping Concepts and
  Requirements between the EU AI Act and International Standards](https://arxiv.org/pdf/2408.11925.pdf)** (2024-08-23)

*Julio Hernandez, Delaram Golpayegani, Dave Lewis*

  The many initiatives on trustworthy AI result in a confusing and multipolar
landscape that organizations operating within the fluid and complex
international value chains must navigate in pursuing trustworthy AI. The EU's
AI Act will now shift the focus of such organizations toward conformance with
the technical requirements for regulatory compliance, for which the Act relies
on Harmonized Standards. Though a high-level mapping to the Act's requirements
will be part of such harmonization, determining the degree to which standards
conformity delivers regulatory compliance with the AI Act remains a complex
challenge. Variance and gaps in the definitions of concepts and how they are
used in requirements between the Act and harmonized standards may impact the
consistency of compliance claims across organizations, sectors, and
applications. This may present regulatory uncertainty, especially for SMEs and
public sector bodies relying on standards conformance rather than proprietary
equivalents for developing and deploying compliant high-risk AI systems. To
address this challenge, this paper offers a simple and repeatable mechanism for
mapping the terms and requirements relevant to normative statements in
regulations and standards, e.g., AI Act and ISO management system standards,
texts into open knowledge graphs. This representation is used to assess the
adequacy of standards conformance to regulatory compliance and thereby provide
a basis for identifying areas where further technical consensus development in
trustworthy AI value chains is required to achieve regulatory compliance.


---

**[81. [2311.11776] Responsible AI Research Needs Impact Statements Too](https://arxiv.org/pdf/2311.11776.pdf)** (2023-11-21)

*Alexandra Olteanu, Michael Ekstrand, Carlos Castillo, Jina Suh*

  All types of research, development, and policy work can have unintended,
adverse consequences - work in responsible artificial intelligence (RAI),
ethical AI, or ethics in AI is no exception.


---

**[82. [2205.12259] Policy Compliance Detection via Expression Tree Inference](https://arxiv.org/pdf/2205.12259.pdf)** (2022-05-25)

*Neema Kotonya, Andreas Vlachos, Majid Yazdani, Lambert Mathias, Marzieh Saeidi*

  Policy Compliance Detection (PCD) is a task we encounter when reasoning over
texts, e.g. legal frameworks. Previous work to address PCD relies heavily on
modeling the task as a special case of Recognizing Textual Entailment.
Entailment is applicable to the problem of PCD, however viewing the policy as a
single proposition, as opposed to multiple interlinked propositions, yields
poor performance and lacks explainability. To address this challenge, more
recent proposals for PCD have argued for decomposing policies into expression
trees consisting of questions connected with logic operators. Question
answering is used to obtain answers to these questions with respect to a
scenario. Finally, the expression tree is evaluated in order to arrive at an
overall solution. However, this work assumes expression trees are provided by
experts, thus limiting its applicability to new policies. In this work, we
learn how to infer expression trees automatically from policy texts. We ensure
the validity of the inferred trees by introducing constrained decoding using a
finite state automaton to ensure the generation of valid trees. We determine
through automatic evaluation that 63% of the expression trees generated by our
constrained generation model are logically equivalent to gold trees. Human
evaluation shows that 88% of trees generated by our model are correct.


---

**[83. [2111.05071] Conformity Assessments and Post-market Monitoring: A Guide to the Role
  of Auditing in the Proposed European AI Regulation](https://arxiv.org/pdf/2111.05071.pdf)** (2021-11-10)

*Jakob Mokander, Maria Axente, Federico Casolari, Luciano Floridi*

  The proposed European Artificial Intelligence Act (AIA) is the first attempt
to elaborate a general legal framework for AI carried out by any major global
economy. As such, the AIA is likely to become a point of reference in the
larger discourse on how AI systems can (and should) be regulated. In this
article, we describe and discuss the two primary enforcement mechanisms
proposed in the AIA: the conformity assessments that providers of high-risk AI
systems are expected to conduct, and the post-market monitoring plans that
providers must establish to document the performance of high-risk AI systems
throughout their lifetimes. We argue that AIA can be interpreted as a proposal
to establish a Europe-wide ecosystem for conducting AI auditing, albeit in
other words. Our analysis offers two main contributions. First, by describing
the enforcement mechanisms included in the AIA in terminology borrowed from
existing literature on AI auditing, we help providers of AI systems understand
how they can prove adherence to the requirements set out in the AIA in
practice. Second, by examining the AIA from an auditing perspective, we seek to
provide transferable lessons from previous research about how to refine further
the regulatory approach outlined in the AIA. We conclude by highlighting seven
aspects of the AIA where amendments (or simply clarifications) would be
helpful. These include, above all, the need to translate vague concepts into
verifiable criteria and to strengthen the institutional safeguards concerning
conformity assessments based on internal checks.


---

**[84. [2406.10218] Semantic Membership Inference Attack against Large Language Models](https://arxiv.org/pdf/2406.10218.pdf)** (2024-06-17)

*Hamid Mozaffari, Virendra J. Marathe*

  Membership Inference Attacks (MIAs) determine whether a specific data point
was included in the training set of a target model. In this paper, we introduce
the Semantic Membership Inference Attack (SMIA), a novel approach that enhances
MIA performance by leveraging the semantic content of inputs and their
perturbations. SMIA trains a neural network to analyze the target model's
behavior on perturbed inputs, effectively capturing variations in output
probability distributions between members and non-members. We conduct
comprehensive evaluations on the Pythia and GPT-Neo model families using the
Wikipedia dataset. Our results show that SMIA significantly outperforms
existing MIAs; for instance, SMIA achieves an AUC-ROC of 67.39% on Pythia-12B,
compared to 58.90% by the second-best attack.


---

**[85. [2501.17755] AI Governance through Markets](https://arxiv.org/pdf/2501.17755.pdf)** (2025-03-06)

*Philip Moreira Tomei, Rupal Jain, Matija Franklin*

  This paper argues that market governance mechanisms should be considered a
key approach in the governance of artificial intelligence (AI), alongside
traditional regulatory frameworks. While current governance approaches have
predominantly focused on regulation, we contend that market-based mechanisms
offer effective incentives for responsible AI development. We examine four
emerging vectors of market governance: insurance, auditing, procurement, and
due diligence, demonstrating how these mechanisms can affirm the relationship
between AI risk and financial risk while addressing capital allocation
inefficiencies. While we do not claim that market forces alone can adequately
protect societal interests, we maintain that standardised AI disclosures and
market mechanisms can create powerful incentives for safe and responsible AI
development. This paper urges regulators, economists, and machine learning
researchers to investigate and implement market-based approaches to AI
governance.


---

**[86. [2306.01788] Responsible Design Patterns for Machine Learning Pipelines](https://arxiv.org/pdf/2306.01788.pdf)** (2023-06-09)

*Saud Hakem Al Harbi, Lionel Nganyewou Tidjon, Foutse Khomh*

  Integrating ethical practices into the AI development process for artificial
intelligence (AI) is essential to ensure safe, fair, and responsible operation.
AI ethics involves applying ethical principles to the entire life cycle of AI
systems. This is essential to mitigate potential risks and harms associated
with AI, such as algorithm biases. To achieve this goal, responsible design
patterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee
ethical and fair outcomes. In this paper, we propose a comprehensive framework
incorporating RDPs into ML pipelines to mitigate risks and ensure the ethical
development of AI systems. Our framework comprises new responsible AI design
patterns for ML pipelines identified through a survey of AI ethics and data
management experts and validated through real-world scenarios with expert
feedback. The framework guides AI developers, data scientists, and
policy-makers to implement ethical practices in AI development and deploy
responsible AI systems in production.


---

**[87. [2311.13871] Legal Requirements Analysis](https://arxiv.org/pdf/2311.13871.pdf)** (2024-02-20)

*Sallam Abualhaija, Marcello Ceci, Lionel Briand*

  Modern software has been an integral part of everyday activities in many
disciplines and application contexts. Introducing intelligent automation by
leveraging artificial intelligence (AI) led to break-throughs in many fields.
The effectiveness of AI can be attributed to several factors, among which is
the increasing availability of data. Regulations such as the general data
protection regulation (GDPR) in the European Union (EU) are introduced to
ensure the protection of personal data. Software systems that collect, process,
or share personal data are subject to compliance with such regulations.
Developing compliant software depends heavily on addressing legal requirements
stipulated in applicable regulations, a central activity in the requirements
engineering (RE) phase of the software development process. RE is concerned
with specifying and maintaining requirements of a system-to-be, including legal
requirements. Legal agreements which describe the policies organizations
implement for processing personal data can provide an additional source to
regulations for eliciting legal requirements. In this chapter, we explore a
variety of methods for analyzing legal requirements and exemplify them on GDPR.
Specifically, we describe possible alternatives for creating machine-analyzable
representations from regulations, survey the existing automated means for
enabling compliance verification against regulations, and further reflect on
the current challenges of legal requirements analysis.


---

**[88. [2404.12762] How should AI decisions be explained? Requirements for Explanations from
  the Perspective of European Law](https://arxiv.org/pdf/2404.12762.pdf)** (2024-11-27)

*Benjamin Fresz, Elena Dubovitskaya, Danilo Brajovic, Marco Huber, Christian Horz*

  This paper investigates the relationship between law and eXplainable
Artificial Intelligence (XAI). While there is much discussion about the AI Act,
for which the trilogue of the European Parliament, Council and Commission
recently concluded, other areas of law seem underexplored. This paper focuses
on European (and in part German) law, although with international concepts and
regulations such as fiduciary plausibility checks, the General Data Protection
Regulation (GDPR), and product safety and liability. Based on XAI-taxonomies,
requirements for XAI-methods are derived from each of the legal bases,
resulting in the conclusion that each legal basis requires different XAI
properties and that the current state of the art does not fulfill these to full
satisfaction, especially regarding the correctness (sometimes called fidelity)
and confidence estimates of XAI-methods.
  Published in the Proceedings of the AAAI/ACM Conference on AI, Ethics, and
Society https://doi.org/10.1609/aies.v7i1.31648 .


---

**[89. [2504.08508] An Early Experience with Confidential Computing Architecture for
  On-Device Model Protection](https://arxiv.org/pdf/2504.08508.pdf)** (2025-04-14)

*Sina Abdollahi, Mohammad Maheri, Sandra Siby, Marios Kogias, Hamed Haddadi*

  Deploying machine learning (ML) models on user devices can improve privacy
(by keeping data local) and reduce inference latency. Trusted Execution
Environments (TEEs) are a practical solution for protecting proprietary models,
yet existing TEE solutions have architectural constraints that hinder on-device
model deployment. Arm Confidential Computing Architecture (CCA), a new Arm
extension, addresses several of these limitations and shows promise as a secure
platform for on-device ML. In this paper, we evaluate the performance-privacy
trade-offs of deploying models within CCA, highlighting its potential to enable
confidential and efficient ML applications. Our evaluations show that CCA can
achieve an overhead of, at most, 22% in running models of different sizes and
applications, including image classification, voice recognition, and chat
assistants. This performance overhead comes with privacy benefits; for example,
our framework can successfully protect the model against membership inference
attack by an 8.3% reduction in the adversary's success rate. To support further
research and early adoption, we make our code and methodology publicly
available.


---

**[90. [2008.00877] Towards a Semantic Model of the GDPR Register of Processing Activities](https://arxiv.org/pdf/2008.00877.pdf)** (2020-08-04)

*Paul Ryan, Harshvardhan J. Pandit, Rob Brennan*

  A core requirement for GDPR compliance is the maintenance of a register of
processing activities (ROPA). Our analysis of six ROPA templates from EU data
protection regulators shows the scope and granularity of a ROPA is subject to
widely varying guidance in different jurisdictions. We present a consolidated
data model based on common concepts and relationships across analysed
templates. We then analyse the extent of using the Data Privacy Vocabulary - a
vocabulary specification for GDPR. We show that the DPV currently does not
provide sufficient concepts to represent the ROPA data model and propose an
extension to fill this gap. This will enable creation of a pan-EU information
management framework for interoperability between organisations and regulators
for GDPR compliance.


---

**[91. [2412.18670] Interplay of ISMS and AIMS in context of the EU AI Act](https://arxiv.org/pdf/2412.18670.pdf)** (2024-12-30)

*Jordan Pötsch*

  The EU AI Act (AIA) mandates the implementation of a risk management system
(RMS) and a quality management system (QMS) for high-risk AI systems. The
ISO/IEC 42001 standard provides a foundation for fulfilling these requirements
but does not cover all EU-specific regulatory stipulations. To enhance the
implementation of the AIA in Germany, the Federal Office for Information
Security (BSI) could introduce the national standard BSI 200-5, which specifies
AIA requirements and integrates existing ISMS standards, such as ISO/IEC 27001.
This paper examines the interfaces between an information security management
system (ISMS) and an AI management system (AIMS), demonstrating that
incorporating existing ISMS controls with specific AI extensions presents an
effective strategy for complying with Article 15 of the AIA. Four new AI
modules are introduced, proposed for inclusion in the BSI IT Grundschutz
framework to comprehensively ensure the security of AI systems. Additionally,
an approach for adapting BSI's qualification and certification systems is
outlined to ensure that expertise in secure AI handling is continuously
developed. Finally, the paper discusses how the BSI could bridge international
standards and the specific requirements of the AIA through the nationalization
of ISO/IEC 42001, creating synergies and bolstering the competitiveness of the
German AI landscape.


---

**[92. [2409.02017] AI Governance in Higher Education: Case Studies of Guidance at Big Ten
  Universities](https://arxiv.org/pdf/2409.02017.pdf)** (2024-09-04)

*Chuhao Wu, He Zhang, John M. Carroll*

  Generative AI has drawn significant attention from stakeholders in higher
education. As it introduces new opportunities for personalized learning and
tutoring support, it simultaneously poses challenges to academic integrity and
leads to ethical issues. Consequently, governing responsible AI usage within
higher education institutions (HEIs) becomes increasingly important. Leading
universities have already published guidelines on Generative AI, with most
attempting to embrace this technology responsibly. This study provides a new
perspective by focusing on strategies for responsible AI governance as
demonstrated in these guidelines. Through a case study of 14 prestigious
universities in the United States, we identified the multi-unit governance of
AI, the role-specific governance of AI, and the academic characteristics of AI
governance from their AI guidelines. The strengths and potential limitations of
these strategies and characteristics are discussed. The findings offer
practical implications for guiding responsible AI usage in HEIs and beyond.


---

**[93. [2403.02231] CODE-ACCORD: A Corpus of building regulatory data for rule generation
  towards automatic compliance checking](https://arxiv.org/pdf/2403.02231.pdf)** (2025-02-19)

*Hansi Hettiarachchi, Amna Dridi, Mohamed Medhat Gaber, Pouyan Parsafard, Nicoleta Bocaneala, Katja Breitenfelder, Gonçal Costa, Maria Hedblom, Mihaela Juganaru-Mathieu, Thamer Mecharnia, Sumee Park, He Tan, Abdel-Rahman H. Tawil, Edlira Vakaj*

  Automatic Compliance Checking (ACC) within the Architecture, Engineering, and
Construction (AEC) sector necessitates automating the interpretation of
building regulations to achieve its full potential. Converting textual rules
into machine-readable formats is challenging due to the complexities of natural
language and the scarcity of resources for advanced Machine Learning (ML).
Addressing these challenges, we introduce CODE-ACCORD, a dataset of 862
sentences from the building regulations of England and Finland. Only the
self-contained sentences, which express complete rules without needing
additional context, were considered as they are essential for ACC. Each
sentence was manually annotated with entities and relations by a team of 12
annotators to facilitate machine-readable rule generation, followed by careful
curation to ensure accuracy. The final dataset comprises 4,297 entities and
4,329 relations across various categories, serving as a robust ground truth.
CODE-ACCORD supports a range of ML and Natural Language Processing (NLP) tasks,
including text classification, entity recognition, and relation extraction. It
enables applying recent trends, such as deep neural networks and large language
models, to ACC.


---

**[94. [2408.11820] Responsible AI Question Bank: A Comprehensive Tool for AI Risk
  Assessment](https://arxiv.org/pdf/2408.11820.pdf)** (2025-01-23)

*Sung Une Lee, Harsha Perera, Yue Liu, Boming Xia, Qinghua Lu, Liming Zhu, Olivier Salvado, Jon Whittle*

  The rapid growth of Artificial Intelligence (AI) has underscored the urgent
need for responsible AI practices. Despite increasing interest, a comprehensive
AI risk assessment toolkit remains lacking. This study introduces our
Responsible AI (RAI) Question Bank, a comprehensive framework and tool designed
to support diverse AI initiatives. By integrating AI ethics principles such as
fairness, transparency, and accountability into a structured question format,
the RAI Question Bank aids in identifying potential risks, aligning with
emerging regulations like the EU AI Act, and enhancing overall AI governance. A
key benefit of the RAI Question Bank is its systematic approach to linking
lower-level risk questions to higher-level ones and related themes, preventing
siloed assessments and ensuring a cohesive evaluation process. Case studies
illustrate the practical application of the RAI Question Bank in assessing AI
projects, from evaluating risk factors to informing decision-making processes.
The study also demonstrates how the RAI Question Bank can be used to ensure
compliance with standards, mitigate risks, and promote the development of
trustworthy AI systems. This work advances RAI by providing organizations with
a valuable tool to navigate the complexities of ethical AI development and
deployment while ensuring comprehensive risk management.


---

**[95. [2412.07782] Trustworthy artificial intelligence in the energy sector: Landscape
  analysis and evaluation framework](https://arxiv.org/pdf/2412.07782.pdf)** (2024-12-12)

*Sotiris Pelekis, Evangelos Karakolis, George Lampropoulos, Spiros Mouzakitis, Ourania Markaki, Christos Ntanos, Dimitris Askounis*

  The present study aims to evaluate the current fuzzy landscape of Trustworthy
AI (TAI) within the European Union (EU), with a specific focus on the energy
sector. The analysis encompasses legal frameworks, directives, initiatives, and
standards like the AI Ethics Guidelines for Trustworthy AI (EGTAI), the
Assessment List for Trustworthy AI (ALTAI), the AI act, and relevant
CEN-CENELEC standardization efforts, as well as EU-funded projects such as
AI4EU and SHERPA. Subsequently, we introduce a new TAI application framework,
called E-TAI, tailored for energy applications, including smart grid and smart
building systems. This framework draws inspiration from EGTAI but is customized
for AI systems in the energy domain. It is designed for stakeholders in
electrical power and energy systems (EPES), including researchers, developers,
and energy experts linked to transmission system operators, distribution system
operators, utilities, and aggregators. These stakeholders can utilize E-TAI to
develop and evaluate AI services for the energy sector with a focus on ensuring
trustworthiness throughout their development and iterative assessment
processes.


---

**[96. [2403.03883] SaulLM-7B: A pioneering Large Language Model for Law](https://arxiv.org/pdf/2403.03883.pdf)** (2024-03-08)

*Pierre Colombo, Telmo Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui Melo, Caio Corro, Andre F. T. Martins, Fabrizio Esposito, Vera Lúcia Raposo, Sofia Morgado, Michael Desa*

  In this paper, we introduce SaulLM-7B, a large language model (LLM) tailored
for the legal domain. With 7 billion parameters, SaulLM-7B is the first LLM
designed explicitly for legal text comprehension and generation. Leveraging the
Mistral 7B architecture as its foundation, SaulLM-7B is trained on an English
legal corpus of over 30 billion tokens. SaulLM-7B exhibits state-of-the-art
proficiency in understanding and processing legal documents. Additionally, we
present a novel instructional fine-tuning method that leverages legal datasets
to further enhance SaulLM-7B's performance in legal tasks. SaulLM-7B is
released under the MIT License.


---

**[97. [2408.02386] Responsibility and Regulation: Exploring Social Measures of Trust in
  Medical AI](https://arxiv.org/pdf/2408.02386.pdf)** (2024-08-06)

*Glenn McGarry, Andy Crabtree, Lachlan Urquhart, Alan Chamberlain*

  This paper explores expert accounts of autonomous systems (AS) development in
the medical device domain (MD) involving applications of artificial
intelligence (AI), machine learning (ML), and other algorithmic and
mathematical modelling techniques. We frame our observations with respect to
notions of responsible innovation (RI) and the emerging problem of how to do RI
in practice. In contribution to the ongoing discourse surrounding trustworthy
autonomous system (TAS) [29], we illuminate practical challenges inherent in
deploying novel AS within existing governance structures, including domain
specific regulations and policies, and rigorous testing and development
processes, and discuss the implications of these for the distribution of
responsibility in novel AI deployment.


---

**[98. [2207.00804] An AIoT-enabled Autonomous Dementia Monitoring System](https://arxiv.org/pdf/2207.00804.pdf)** (2022-07-05)

*Xingyu Wu, Jinyang Li*

  An autonomous Artificial Internet of Things (AIoT) system for elderly
dementia patients monitoring in a smart home is presented. The system mainly
implements two functions based on the activity inference of the sensor data,
which are real time abnormal activity monitoring and trend prediction of
disease related activities. Specifically, CASAS dataset is employed to train a
Random Forest (RF) model for activity inference. Then, another RF model trained
by the output data of activity inference is used for abnormal activity
monitoring. Particularly, RF is chosen for these tasks because of its balanced
trade offs between accuracy, time efficiency, flexibility, and
interpretability. Moreover, Long Short Term Memory (LSTM) is utilised to
forecast the disease related activity trend of a patient. Consequently, the
accuracy of two RF classifiers designed for activity inference and abnormal
activity detection is greater than 99 percent and 94 percent, respectively.
Furthermore, using the duration of sleep as an example, the LSTM model achieves
accurate and evident future trends prediction.


---

**[99. [2309.14876] APPRAISE: a governance framework for innovation with AI systems](https://arxiv.org/pdf/2309.14876.pdf)** (2023-12-14)

*Diptish Dey, Debarati Bhaumik*

  As artificial intelligence (AI) systems increasingly impact society, the EU
Artificial Intelligence Act (AIA) is the first serious legislative attempt to
contain the harmful effects of AI systems. This paper proposes a governance
framework for AI innovation. The framework bridges the gap between strategic
variables and responsible value creation, recommending audit as an enforcement
mechanism. Strategic variables include, among others, organization size,
exploration versus exploitation -, and build versus buy dilemmas. The proposed
framework is based on primary and secondary research; the latter describes four
pressures that organizations innovating with AI experience. Primary research
includes an experimental setup, using which 34 organizations in the Netherlands
are surveyed, followed up by 2 validation interviews. The survey measures the
extent to which organizations coordinate technical elements of AI systems to
ultimately comply with the AIA. The validation interviews generated additional
in-depth insights and provided root causes. The moderating effect of the
strategic variables is tested and found to be statistically significant for
variables such as organization size. Relevant insights from primary and
secondary research are eventually combined to propose the APPRAISE framework.


---

**[100. [2411.15149] The Fundamental Rights Impact Assessment (FRIA) in the AI Act: Roots,
  legal obligations and key elements for a model template](https://arxiv.org/pdf/2411.15149.pdf)** (2024-11-26)

*Alessandro Mantelero*

  What is the context which gave rise to the obligation to carry out a
Fundamental Rights Impact Assessment (FRIA) in the AI Act? How has assessment
of the impact on fundamental rights been framed by the EU legislator in the AI
Act? What methodological criteria should be followed in developing the FRIA?
These are the three main research questions that this article aims to address,
through both legal analysis of the relevant provisions of the AI Act and
discussion of various possible models for assessment of the impact of AI on
fundamental rights. The overall objective of this article is to fill existing
gaps in the theoretical and methodological elaboration of the FRIA, as outlined
in the AI Act. In order to facilitate the future work of EU and national bodies
and AI operators in placing this key tool for human-centric and trustworthy AI
at the heart of the EU approach to AI design and development, this article
outlines the main building blocks of a model template for the FRIA. While this
proposal is consistent with the rationale and scope of the AI Act, it is also
applicable beyond the cases listed in Article 27 and can serve as a blueprint
for other national and international regulatory initiatives to ensure that AI
is fully consistent with human rights.


---

**[101. [2406.10121] Data Ethics in the Era of Healthcare Artificial Intelligence in Africa:
  An Ubuntu Philosophy Perspective](https://arxiv.org/pdf/2406.10121.pdf)** (2024-06-17)

*Abdoul Jalil Djiberou Mahamadou, Aloysius Ochasi, Russ B. Altman*

  Data are essential in developing healthcare artificial intelligence (AI)
systems. However, patient data collection, access, and use raise ethical
concerns, including informed consent, data bias, data protection and privacy,
data ownership, and benefit sharing. Various ethical frameworks have been
proposed to ensure the ethical use of healthcare data and AI, however, these
frameworks often align with Western cultural values, social norms, and
institutional contexts emphasizing individual autonomy and well-being. Ethical
guidelines must reflect political and cultural settings to account for cultural
diversity, inclusivity, and historical factors such as colonialism. Thus, this
paper discusses healthcare data ethics in the AI era in Africa from the Ubuntu
philosophy perspective. It focuses on the contrast between individualistic and
communitarian approaches to data ethics. The proposed framework could inform
stakeholders, including AI developers, healthcare providers, the public, and
policy-makers about healthcare data ethical usage in AI in Africa.


---

**[102. [2402.01717] From RAG to QA-RAG: Integrating Generative AI for Pharmaceutical
  Regulatory Compliance Process](https://arxiv.org/pdf/2402.01717.pdf)** (2024-02-07)

*Sungkyunkwan University  Jaewoong Kim, Sungkyunkwan
  University  Moohong Min*

  Regulatory compliance in the pharmaceutical industry entails navigating
through complex and voluminous guidelines, often requiring significant human
resources. To address these challenges, our study introduces a chatbot model
that utilizes generative AI and the Retrieval Augmented Generation (RAG)
method. This chatbot is designed to search for guideline documents relevant to
the user inquiries and provide answers based on the retrieved guidelines.
Recognizing the inherent need for high reliability in this domain, we propose
the Question and Answer Retrieval Augmented Generation (QA-RAG) model. In
comparative experiments, the QA-RAG model demonstrated a significant
improvement in accuracy, outperforming all other baselines including
conventional RAG methods. This paper details QA-RAG's structure and performance
evaluation, emphasizing its potential for the regulatory compliance domain in
the pharmaceutical industry and beyond. We have made our work publicly
available for further research and development.


---

**[103. [2409.09046] HyPA-RAG: A Hybrid Parameter Adaptive Retrieval-Augmented Generation
  System for AI Legal and Policy Applications](https://arxiv.org/pdf/2409.09046.pdf)** (2025-02-26)

*Rishi Kalra, Zekun Wu, Ayesha Gulley, Airlie Hilliard, Xin Guan, Adriano Koshiyama, Philip Treleaven*

  Large Language Models (LLMs) face limitations in AI legal and policy
applications due to outdated knowledge, hallucinations, and poor reasoning in
complex contexts. Retrieval-Augmented Generation (RAG) systems address these
issues by incorporating external knowledge, but suffer from retrieval errors,
ineffective context integration, and high operational costs. This paper
presents the Hybrid Parameter-Adaptive RAG (HyPA-RAG) system, designed for the
AI legal domain, with NYC Local Law 144 (LL144) as the test case. HyPA-RAG
integrates a query complexity classifier for adaptive parameter tuning, a
hybrid retrieval approach combining dense, sparse, and knowledge graph methods,
and a comprehensive evaluation framework with tailored question types and
metrics. Testing on LL144 demonstrates that HyPA-RAG enhances retrieval
accuracy, response fidelity, and contextual precision, offering a robust and
adaptable solution for high-stakes legal and policy applications.


---

**[104. [2307.03198] A multilevel framework for AI governance](https://arxiv.org/pdf/2307.03198.pdf)** (2023-07-14)

*Hyesun Choung, Prabu David, John S. Seberger*

  To realize the potential benefits and mitigate potential risks of AI, it is
necessary to develop a framework of governance that conforms to ethics and
fundamental human values. Although several organizations have issued guidelines
and ethical frameworks for trustworthy AI, without a mediating governance
structure, these ethical principles will not translate into practice. In this
paper, we propose a multilevel governance approach that involves three groups
of interdependent stakeholders: governments, corporations, and citizens. We
examine their interrelationships through dimensions of trust, such as
competence, integrity, and benevolence. The levels of governance combined with
the dimensions of trust in AI provide practical insights that can be used to
further enhance user experiences and inform public policy related to AI.


---

**[105. [2501.19112] Logical Modalities within the European AI Act: An Analysis](https://arxiv.org/pdf/2501.19112.pdf)** (2025-02-03)

*Lara Lawniczak, Christoph Benzmüller*

  The paper presents a comprehensive analysis of the European AI Act in terms
of its logical modalities, with the aim of preparing its formal representation,
for example, within the logic-pluralistic Knowledge Engineering Framework and
Methodology (LogiKEy). LogiKEy develops computational tools for normative
reasoning based on formal methods, employing Higher-Order Logic (HOL) as a
unifying meta-logic to integrate diverse logics through shallow semantic
embeddings. This integration is facilitated by Isabelle/HOL, a proof assistant
tool equipped with several automated theorem provers. The modalities within the
AI Act and the logics suitable for their representation are discussed. For a
selection of these logics, embeddings in HOL are created, which are then used
to encode sample paragraphs. Initial experiments evaluate the suitability of
these embeddings for automated reasoning, and highlight key challenges on the
way to more robust reasoning capabilities.


---

**[106. [2310.00828] A Model for Calculating Cost of Applying Electronic Governance and
  Robotic Process Automation to a Distributed Management System](https://arxiv.org/pdf/2310.00828.pdf)** (2023-10-03)

*Bonny Banerjee, Saurabh Pahune*

  Electronic Governance (eGov) and Robotic Process Automation (RPA) are two
technological advancements that have the potential to revolutionize the way
organizations manage their operations. When applied to Distributed Management
(DM), these technologies can further enhance organizational efficiency and
effectiveness. In this brief article, we present a mathematical model for
calculating the cost of accomplishing a task by applying eGov and RPA in a DM
system. This model is one of the first of its kind, and is expected to spark
further research on cost analysis for organizational efficiency given the
unprecedented advancements in electronic and automation technologies.


---

**[107. [2301.08741] Enactive Artificial Intelligence: Subverting Gender Norms in Robot-Human
  Interaction](https://arxiv.org/pdf/2301.08741.pdf)** (2023-05-09)

*Ines Hipolito, Katie Winkle, Merete Lie*

  This paper introduces Enactive Artificial Intelligence (eAI) as an
intersectional gender-inclusive stance towards AI. AI design is an enacted
human sociocultural practice that reflects human culture and values.
Unrepresentative AI design could lead to social marginalisation. Section 1,
drawing from radical enactivism, outlines embodied cultural practices. In
Section 2, explores how intersectional gender intertwines with technoscience as
a sociocultural practice. Section 3 focuses on subverting gender norms in the
specific case of Robot-Human Interaction in AI. Finally, Section 4 identifies
four vectors of ethics: explainability, fairness, transparency, and
auditability for adopting an intersectionality-inclusive stance in developing
gender-inclusive AI and subverting existing gender norms in robot design.


---

**[108. [2504.00652] Towards Adaptive AI Governance: Comparative Insights from the U.S., EU,
  and Asia](https://arxiv.org/pdf/2504.00652.pdf)** (2025-04-02)

*Vikram Kulothungan, Deepti Gupta*

  Artificial intelligence (AI) trends vary significantly across global regions,
shaping the trajectory of innovation, regulation, and societal impact. This
variation influences how different regions approach AI development, balancing
technological progress with ethical and regulatory considerations. This study
conducts a comparative analysis of AI trends in the United States (US), the
European Union (EU), and Asia, focusing on three key dimensions: generative AI,
ethical oversight, and industrial applications. The US prioritizes
market-driven innovation with minimal regulatory constraints, the EU enforces a
precautionary risk-based framework emphasizing ethical safeguards, and Asia
employs state-guided AI strategies that balance rapid deployment with
regulatory oversight. Although these approaches reflect different economic
models and policy priorities, their divergence poses challenges to
international collaboration, regulatory harmonization, and the development of
global AI standards. To address these challenges, this paper synthesizes
regional strengths to propose an adaptive AI governance framework that
integrates risk-tiered oversight, innovation accelerators, and strategic
alignment mechanisms. By bridging governance gaps, this study offers actionable
insights for fostering responsible AI development while ensuring a balance
between technological progress, ethical imperatives, and regulatory coherence.


---

**[109. [2504.05951] Representing Normative Regulations in OWL DL for Automated Compliance
  Checking Supported by Text Annotation](https://arxiv.org/pdf/2504.05951.pdf)** (2025-04-09)

*Ildar Baimuratov, Denis Turygin*

  Compliance checking is the process of determining whether a regulated entity
adheres to these regulations. Currently, compliance checking is predominantly
manual, requiring significant time and highly skilled experts, while still
being prone to errors caused by the human factor. Various approaches have been
explored to automate compliance checking, however, representing regulations in
OWL DL language which enables compliance checking through OWL reasoning has not
been adopted. In this work, we propose an annotation schema and an algorithm
that transforms text annotations into machine-interpretable OWL DL code. The
proposed approach is validated through a proof-of-concept implementation
applied to examples from the building construction domain.


---

**[110. [2104.04147] Artificial intelligence, human rights, democracy, and the rule of law: a
  primer](https://arxiv.org/pdf/2104.04147.pdf)** (2021-04-12)

*David Leslie, Christopher Burr, Mhairi Aitken, Josh Cowls, Michael Katell, Morgan Briggs*

  In September 2019, the Council of Europe's Committee of Ministers adopted the
terms of reference for the Ad Hoc Committee on Artificial Intelligence (CAHAI).
The CAHAI is charged with examining the feasibility and potential elements of a
legal framework for the design, development, and deployment of AI systems that
accord with Council of Europe standards across the interrelated areas of human
rights, democracy, and the rule of law. As a first and necessary step in
carrying out this responsibility, the CAHAI's Feasibility Study, adopted by its
plenary in December 2020, has explored options for an international legal
response that fills existing gaps in legislation and tailors the use of binding
and non-binding legal instruments to the specific risks and opportunities
presented by AI systems. The Study examines how the fundamental rights and
freedoms that are already codified in international human rights law can be
used as the basis for such a legal framework. The purpose of this primer is to
introduce the main concepts and principles presented in the CAHAI's Feasibility
Study for a general, non-technical audience. It also aims to provide some
background information on the areas of AI innovation, human rights law,
technology policy, and compliance mechanisms covered therein. In keeping with
the Council of Europe's commitment to broad multi-stakeholder consultations,
outreach, and engagement, this primer has been designed to help facilitate the
meaningful and informed participation of an inclusive group of stakeholders as
the CAHAI seeks feedback and guidance regarding the essential issues raised by
the Feasibility Study.


---

**[111. [2205.15279] The openESEA Modelling Language for Ethical, Social and Environmental
  Accounting: Technical Report](https://arxiv.org/pdf/2205.15279.pdf)** (2024-10-21)

*Sergio España, Vijanti Ramautar*

  Over the years ethical, social and environmental accounting (ESEA) has become
a common practice among responsible organisations. ESEA entails assessing and
reporting organisations" performance on environmental, social and governance
topics. In this report, we present a textual grammar for specifying ESEA
methods. With the grammar ESEA models can be created. Such models can be
interpreted by our open-source, model-driven tool, called openESEA. The report
presents the metamodel of the grammar, the grammar itself, and explanations of
each grammar primitive.


---

**[112. [2109.03283] Have a break from making decisions, have a MARS: The Multi-valued Action
  Reasoning System](https://arxiv.org/pdf/2109.03283.pdf)** (2023-02-08)

*Cosmin Badea*

  The Multi-valued Action Reasoning System (MARS) is an automated value-based
ethical decision-making model for artificial agents (AI). Given a set of
available actions and an underlying moral paradigm, by employing MARS one can
identify the ethically preferred action. It can be used to implement and model
different ethical theories, different moral paradigms, as well as combinations
of such, in the context of automated practical reasoning and normative decision
analysis. It can also be used to model moral dilemmas and discover the moral
paradigms that result in the desired outcomes therein. In this paper, we give a
condensed description of MARS, explain its uses, and comparatively place it in
the existing literature.


---

**[113. [2405.01560] Copyright related risks in the creation and use of ML/AI systems](https://arxiv.org/pdf/2405.01560.pdf)** (2024-05-06)

*Daniel M. German*

  This paper summarizes the current copyright related risks that Machine
Learning (ML) and Artificial Intelligence (AI) systems (including Large
Language Models --LLMs) incur. These risks affect different stakeholders:
owners of the copyright of the training data, the users of ML/AI systems, the
creators of trained models, and the operators of AI systems. This paper also
provides an overview of ongoing legal cases in the United States related to
these risks.


---

**[114. [2407.21060] Using Large Language Models for the Interpretation of Building
  Regulations](https://arxiv.org/pdf/2407.21060.pdf)** (2024-08-01)

*Stefan Fuchs, Michael Witbrock, Johannes Dimyadi, Robert Amor*

  Compliance checking is an essential part of a construction project. The
recent rapid uptake of building information models (BIM) in the construction
industry has created more opportunities for automated compliance checking
(ACC). BIM enables sharing of digital building design data that can be used for
compliance checking with legal requirements, which are conventionally conveyed
in natural language and not intended for machine processing. Creating a
computable representation of legal requirements suitable for ACC is complex,
costly, and time-consuming. Large language models (LLMs) such as the generative
pre-trained transformers (GPT), GPT-3.5 and GPT-4, powering OpenAI's ChatGPT,
can generate logically coherent text and source code responding to user
prompts. This capability could be used to automate the conversion of building
regulations into a semantic and computable representation. This paper evaluates
the performance of LLMs in translating building regulations into LegalRuleML in
a few-shot learning setup. By providing GPT-3.5 with only a few example
translations, it can learn the basic structure of the format. Using a system
prompt, we further specify the LegalRuleML representation and explore the
existence of expert domain knowledge in the model. Such domain knowledge might
be ingrained in GPT-3.5 through the broad pre-training but needs to be brought
forth by careful contextualisation. Finally, we investigate whether strategies
such as chain-of-thought reasoning and self-consistency could apply to this use
case. As LLMs become more sophisticated, the increased common sense, logical
coherence, and means to domain adaptation can significantly support ACC,
leading to more efficient and effective checking processes.


---

**[115. [2210.08984] AI Governance and Ethics Framework for Sustainable AI and Sustainability](https://arxiv.org/pdf/2210.08984.pdf)** (2022-10-18)

*Mahendra Samarawickrama*

  AI is transforming the existing technology landscape at a rapid phase
enabling data-informed decision making and autonomous decision making. Unlike
any other technology, because of the decision-making ability of AI, ethics and
governance became a key concern. There are many emerging AI risks for humanity,
such as autonomous weapons, automation-spurred job loss, socio-economic
inequality, bias caused by data and algorithms, privacy violations and
deepfakes. Social diversity, equity and inclusion are considered key success
factors of AI to mitigate risks, create values and drive social justice.
Sustainability became a broad and complex topic entangled with AI. Many
organizations (government, corporate, not-for-profits, charities and NGOs) have
diversified strategies driving AI for business optimization and
social-and-environmental justice. Partnerships and collaborations become
important more than ever for equity and inclusion of diversified and
distributed people, data and capabilities. Therefore, in our journey towards an
AI-enabled sustainable future, we need to address AI ethics and governance as a
priority. These AI ethics and governance should be underpinned by human ethics.


---

**[116. [2503.14527] Threefold model for AI Readiness: A Case Study with Finnish Healthcare
  SMEs](https://arxiv.org/pdf/2503.14527.pdf)** (2025-03-20)

*Mohammed Alnajjar, Khalid Alnajjar, Mika Hämäläinen*

  This study examines AI adoption among Finnish healthcare SMEs through
semi-structured interviews with six health-tech companies. We identify three AI
engagement categories: AI-curious (exploring AI), AI-embracing (integrating
AI), and AI-catering (providing AI solutions). Our proposed threefold model
highlights key adoption barriers, including regulatory complexities, technical
expertise gaps, and financial constraints. While SMEs recognize AI's potential,
most remain in early adoption stages. We provide actionable recommendations to
accelerate AI integration, focusing on regulatory reforms, talent development,
and inter-company collaboration, offering valuable insights for healthcare
organizations, policymakers, and researchers.


---

**[117. [2212.03601] Artificial Intelligence in Governance, Risk and Compliance: Results of a
  study on potentials for the application of artificial intelligence (AI) in
  governance, risk and compliance (GRC)](https://arxiv.org/pdf/2212.03601.pdf)** (2024-05-18)

*Eva Ponick, Gabriele Wieczorek*

  The digital transformation leads to fundamental change in organizational
structures. To be able to apply new technologies not only selectively,
processes in companies must be revised and functional units must be viewed
holistically, especially with regard to interfaces. Target-oriented management
decisions are made, among other things, on the basis of risk management and
compliance in combination with the internal control system as governance
functions. The effectiveness and efficiency of these functions is decisive to
follow guidelines and regulatory requirements as well as for the evaluation of
alternative options for acting with regard to activities of companies. GRC
(Governance, Risk and Compliance) means an integrated governance-approach, in
which the mentioned governance functions are interlinked and not separated from
each other. Methods of artificial intelligence represents an important
technology of digital transformation. This technology, which offers a broad
range of methods such as machine learning, artificial neural networks, natural
language processing or deep learning, offers a lot of possible applications in
many business areas from purchasing to production or customer service.
Artificial intelligence is also being used in GRC, for example for processing
and analysis of unstructured data sets. This study contains the results of a
survey conducted in 2021 to identify and analyze the potential applications of
artificial intelligence in GRC.


---

**[118. [2202.02776] Human rights, democracy, and the rule of law assurance framework for AI
  systems: A proposal](https://arxiv.org/pdf/2202.02776.pdf)** (2022-02-08)

*David Leslie, Christopher Burr, Mhairi Aitken, Michael Katell, Morgan Briggs, Cami Rincon*

  Following on from the publication of its Feasibility Study in December 2020,
the Council of Europe's Ad Hoc Committee on Artificial Intelligence (CAHAI) and
its subgroups initiated efforts to formulate and draft its Possible Elements of
a Legal Framework on Artificial Intelligence, based on the Council of Europe's
standards on human rights, democracy, and the rule of law. This document was
ultimately adopted by the CAHAI plenary in December 2021. To support this
effort, The Alan Turing Institute undertook a programme of research that
explored the governance processes and practical tools needed to operationalise
the integration of human right due diligence with the assurance of trustworthy
AI innovation practices.
  The resulting framework was completed and submitted to the Council of Europe
in September 2021. It presents an end-to-end approach to the assurance of AI
project lifecycles that integrates context-based risk analysis and appropriate
stakeholder engagement with comprehensive impact assessment, and transparent
risk management, impact mitigation, and innovation assurance practices. Taken
together, these interlocking processes constitute a Human Rights, Democracy and
the Rule of Law Assurance Framework (HUDERAF). The HUDERAF combines the
procedural requirements for principles-based human rights due diligence with
the governance mechanisms needed to set up technical and socio-technical
guardrails for responsible and trustworthy AI innovation practices. Its purpose
is to provide an accessible and user-friendly set of mechanisms for
facilitating compliance with a binding legal framework on artificial
intelligence, based on the Council of Europe's standards on human rights,
democracy, and the rule of law, and to ensure that AI innovation projects are
carried out with appropriate levels of public accountability, transparency, and
democratic governance.


---

**[119. [2503.04766] Global AI Governance: Where the Challenge is the Solution- An
  Interdisciplinary, Multilateral, and Vertically Coordinated Approach](https://arxiv.org/pdf/2503.04766.pdf)** (2025-03-10)

*Huixin Zhong, Thao Do, Ynagliu Jie, Rostam J. Neuwirth, Hong Shen*

  Current global AI governance frameworks struggle with fragmented disciplinary
collaboration, ineffective multilateral coordination, and disconnects between
policy design and grassroots implementation. This study, guided by Integration
and Implementation Science (IIS) initiated a structured interdisciplinary
dialogue at the UN Science Summit, convening legal, NGO, and HCI experts to
tackle those challenges. Drawing on the common ground of the experts: dynamism,
experimentation, inclusivity, and paradoxical governance, this study, through
thematic analysis and interdisciplinary comparison analysis, identifies four
core principles of global AI governance. Furthermore, we translate these
abstract principles into concrete action plans leveraging the distinct yet
complementary perspectives of each discipline. These principles and action
plans are then integrated into a five-phase, time-sequential framework
including foundation building, experimental verification, collaborative
optimization, global adaptation, and continuous evolution phases. This
multilevel framework offers a novel and concrete pathway toward establishing
interdisciplinary, multilateral, and vertically coordinated AI governance,
transforming global AI governance challenges into opportunities for political
actions.


---

**[120. [2302.10766] Bridging the Transparency Gap: What Can Explainable AI Learn From the AI
  Act?](https://arxiv.org/pdf/2302.10766.pdf)** (2023-12-22)

*Balint Gyevnar, Nick Ferguson, Burkhard Schafer*

  The European Union has proposed the Artificial Intelligence Act which
introduces detailed requirements of transparency for AI systems. Many of these
requirements can be addressed by the field of explainable AI (XAI), however,
there is a fundamental difference between XAI and the Act regarding what
transparency is. The Act views transparency as a means that supports wider
values, such as accountability, human rights, and sustainable innovation. In
contrast, XAI views transparency narrowly as an end in itself, focusing on
explaining complex algorithmic properties without considering the
socio-technical context. We call this difference the ``transparency gap''.
Failing to address the transparency gap, XAI risks leaving a range of
transparency issues unaddressed. To begin to bridge this gap, we overview and
clarify the terminology of how XAI and European regulation -- the Act and the
related General Data Protection Regulation (GDPR) -- view basic definitions of
transparency. By comparing the disparate views of XAI and regulation, we arrive
at four axes where practical work could bridge the transparency gap: defining
the scope of transparency, clarifying the legal status of XAI, addressing
issues with conformity assessment, and building explainability for datasets.


---

**[121. [2404.02894] Automated Transparency: A Legal and Empirical Analysis of the Digital
  Services Act Transparency Database](https://arxiv.org/pdf/2404.02894.pdf)** (2024-05-06)

*Rishabh Kaushal, Jacob van de Kerkhof, Catalina Goanta, Gerasimos Spanakis, Adriana Iamnitchi*

  The Digital Services Act (DSA) is a much awaited platforms liability reform
in the European Union that was adopted on 1 November 2022 with the ambition to
set a global example in terms of accountability and transparency. Among other
obligations, the DSA emphasizes the need for online platforms to report on
their content moderation decisions (`statements of reasons' - SoRs), which is a
novel transparency mechanism we refer to as automated transparency in this
study. SoRs are currently made available in the DSA Transparency Database,
launched by the European Commission in September 2023. The DSA Transparency
Database marks a historical achievement in platform governance, and allows
investigations about the actual transparency gains, both at structure level as
well as at the level of platform compliance. This study aims to understand
whether the Transparency Database helps the DSA to live up to its transparency
promises. We use legal and empirical arguments to show that while there are
some transparency gains, compliance remains problematic, as the current
database structure allows for a lot of discretion from platforms in terms of
transparency practices. In our empirical study, we analyze a representative
sample of the Transparency Database (131m SoRs) submitted in November 2023, to
characterise and evaluate platform content moderation practices.


---

**[122. [2405.19970] Strategies to Counter Artificial Intelligence in Law Enforcement:
  Cross-Country Comparison of Citizens in Greece, Italy and Spain](https://arxiv.org/pdf/2405.19970.pdf)** (2024-05-31)

*Petra Saskia Bayerl, Babak Akhgar, Ernesto La Mattina, Barbara Pirillo, Ioana Cotoi, Davide Ariu, Matteo Mauri, Jorge Garcia, Dimitris Kavallieros, Antonia Kardara, Konstantina Karagiorgou*

  This paper investigates citizens' counter-strategies to the use of Artificial
Intelligence (AI) by law enforcement agencies (LEAs). Based on information from
three countries (Greece, Italy and Spain) we demonstrate disparities in the
likelihood of ten specific counter-strategies. We further identified factors
that increase the propensity for counter-strategies. Our study provides an
important new perspective to societal impacts of security-focused AI
applications by illustrating the conscious, strategic choices by citizens when
confronted with AI capabilities for LEAs.


---

**[123. [2008.09507] Authorized and Unauthorized Practices of Law: The Role of Autonomous
  Levels of AI Legal Reasoning](https://arxiv.org/pdf/2008.09507.pdf)** (2020-08-24)

*Lance Eliot*

  Advances in Artificial Intelligence (AI) and Machine Learning (ML) that are
being applied to legal efforts have raised controversial questions about the
existent restrictions imposed on the practice-of-law. Generally, the legal
field has sought to define Authorized Practices of Law (APL) versus
Unauthorized Practices of Law (UPL), though the boundaries are at times
amorphous and some contend capricious and self-serving, rather than being
devised holistically for the benefit of society all told. A missing ingredient
in these arguments is the realization that impending legal profession
disruptions due to AI can be more robustly discerned by examining the matter
through the lens of a framework utilizing the autonomous levels of AI Legal
Reasoning (AILR). This paper explores a newly derived instrumental grid
depicting the key characteristics underlying APL and UPL as they apply to the
AILR autonomous levels and offers key insights for the furtherance of these
crucial practice-of-law debates.


---

**[124. [2308.09979] Artificial Intelligence across Europe: A Study on Awareness, Attitude
  and Trust](https://arxiv.org/pdf/2308.09979.pdf)** (2023-08-22)

*Teresa Scantamburlo, Atia Cortés, Francesca Foffano, Cristian Barrué, Veronica Distefano, Long Pham, Alessandro Fabris*

  This paper presents the results of an extensive study investigating the
opinions on Artificial Intelligence (AI) of a sample of 4,006 European citizens
from eight distinct countries (France, Germany, Italy, Netherlands, Poland,
Romania, Spain, and Sweden). The aim of the study is to gain a better
understanding of people's views and perceptions within the European context,
which is already marked by important policy actions and regulatory processes.
To survey the perceptions of the citizens of Europe we design and validate a
new questionnaire (PAICE) structured around three dimensions: people's
awareness, attitude, and trust. We observe that while awareness is
characterized by a low level of self-assessed competency, the attitude toward
AI is very positive for more than half of the population. Reflecting upon the
collected results, we highlight implicit contradictions and identify trends
that may interfere with the creation of an ecosystem of trust and the
development of inclusive AI policies. The introduction of rules that ensure
legal and ethical standards, along with the activity of high-level educational
entities, and the promotion of AI literacy are identified as key factors in
supporting a trustworthy AI ecosystem. We make some recommendations for AI
governance focused on the European context and conclude with suggestions for
future work.


---

**[125. [2209.04963] Responsible AI Pattern Catalogue: A Collection of Best Practices for AI
  Governance and Engineering](https://arxiv.org/pdf/2209.04963.pdf)** (2023-09-29)

*Qinghua Lu, Liming Zhu, Xiwei Xu, Jon Whittle, Didar Zowghi, Aurelie Jacquet*

  Responsible AI is widely considered as one of the greatest scientific
challenges of our time and is key to increase the adoption of AI. Recently, a
number of AI ethics principles frameworks have been published. However, without
further guidance on best practices, practitioners are left with nothing much
beyond truisms. Also, significant efforts have been placed at algorithm-level
rather than system-level, mainly focusing on a subset of mathematics-amenable
ethical principles, such as fairness. Nevertheless, ethical issues can arise at
any step of the development lifecycle, cutting across many AI and non-AI
components of systems beyond AI algorithms and models. To operationalize
responsible AI from a system perspective, in this paper, we present a
Responsible AI Pattern Catalogue based on the results of a Multivocal
Literature Review (MLR). Rather than staying at the principle or algorithm
level, we focus on patterns that AI system stakeholders can undertake in
practice to ensure that the developed AI systems are responsible throughout the
entire governance and engineering lifecycle. The Responsible AI Pattern
Catalogue classifies the patterns into three groups: multi-level governance
patterns, trustworthy process patterns, and responsible-AI-by-design product
patterns. These patterns provide systematic and actionable guidance for
stakeholders to implement responsible AI.


---

**[126. [2309.12336] The E.U.'s Artificial Intelligence Act: An Ordoliberal Assessment](https://arxiv.org/pdf/2309.12336.pdf)** (2023-09-25)

*Manuel Woersdoerfer*

  In light of the rise of generative AI and recent debates about the
socio-political implications of large-language models and chatbots, this
article investigates the E.U.'s Artificial Intelligence Act (AIA), the world's
first major attempt by a government body to address and mitigate the
potentially negative impacts of AI technologies. The article critically
analyzes the AIA from a distinct economic ethics perspective, i.e.,
ordoliberalism 2.0 - a perspective currently lacking in the academic
literature. It evaluates, in particular, the AIA's ordoliberal strengths and
weaknesses and proposes reform measures that could be taken to strengthen the
AIA.


---

**[127. [2410.17278] Automated decision-making and artificial intelligence at European
  borders and their risks for human rights](https://arxiv.org/pdf/2410.17278.pdf)** (2024-10-24)

*Yiran Yang, Frederik Zuiderveen Borgesius, Pascal Beckers, Evelien Brouwer*

  Many countries use automated decision-making (ADM) systems, often based on
artificial intelligence (AI), to manage migration at their borders. This
interdisciplinary paper explores two questions. What are the main ways that
automated decision-making is used at EU borders? Does such automated
decision-making bring risks related to human rights, and if so: which risks?
The paper introduces a taxonomy of four types of ADM systems at EU borders.
Three types are used at borders: systems for (1) identification and
verification by checking biometrics, (2) risk assessment, and (3) border
monitoring. In addition, (4) polygraphs and emotion detectors are being tested
at EU borders. We discuss three categories of risks of such automated
decision-making, namely risks related to the human rights to (1) privacy and
data protection, (2) nondiscrimination, and (3) a fair trial and effective
remedies. The paper is largely based on a literature review that we conducted
about the use of automated decision-making at borders. The paper combines
insights from several disciplines, including social sciences, law, computer
science, and migration studies.


---

**[128. [2503.18994] HH4AI: A methodological Framework for AI Human Rights impact assessment
  under the EUAI ACT](https://arxiv.org/pdf/2503.18994.pdf)** (2025-03-26)

*Paolo Ceravolo, Ernesto Damiani, Maria Elisa D'Amico, Bianca de Teffe Erb, Simone Favaro, Nannerel Fiano, Paolo Gambatesa, Simone La Porta, Samira Maghool, Lara Mauri, Niccolo Panigada, Lorenzo Maria Ratto Vaquer, Marta A. Tamborini*

  This paper introduces the HH4AI Methodology, a structured approach to
assessing the impact of AI systems on human rights, focusing on compliance with
the EU AI Act and addressing technical, ethical, and regulatory challenges. The
paper highlights AIs transformative nature, driven by autonomy, data, and
goal-oriented design, and how the EU AI Act promotes transparency,
accountability, and safety. A key challenge is defining and assessing
"high-risk" AI systems across industries, complicated by the lack of
universally accepted standards and AIs rapid evolution.
  To address these challenges, the paper explores the relevance of ISO/IEC and
IEEE standards, focusing on risk management, data quality, bias mitigation, and
governance. It proposes a Fundamental Rights Impact Assessment (FRIA)
methodology, a gate-based framework designed to isolate and assess risks
through phases including an AI system overview, a human rights checklist, an
impact assessment, and a final output phase. A filtering mechanism tailors the
assessment to the system's characteristics, targeting areas like
accountability, AI literacy, data governance, and transparency.
  The paper illustrates the FRIA methodology through a fictional case study of
an automated healthcare triage service. The structured approach enables
systematic filtering, comprehensive risk assessment, and mitigation planning,
effectively prioritizing critical risks and providing clear remediation
strategies. This promotes better alignment with human rights principles and
enhances regulatory compliance.


---

**[129. [2403.08501] Governing Through the Cloud: The Intermediary Role of Compute Providers
  in AI Regulation](https://arxiv.org/pdf/2403.08501.pdf)** (2024-03-27)

*Lennart Heim, Tim Fist, Janet Egan, Sihao Huang, Stephen Zekany, Robert Trager, Michael A Osborne, Noa Zilberman*

  As jurisdictions around the world take their first steps toward regulating
the most powerful AI systems, such as the EU AI Act and the US Executive Order
14110, there is a growing need for effective enforcement mechanisms that can
verify compliance and respond to violations. We argue that compute providers
should have legal obligations and ethical responsibilities associated with AI
development and deployment, both to provide secure infrastructure and to serve
as intermediaries for AI regulation. Compute providers can play an essential
role in a regulatory ecosystem via four key capacities: as securers,
safeguarding AI systems and critical infrastructure; as record keepers,
enhancing visibility for policymakers; as verifiers of customer activities,
ensuring oversight; and as enforcers, taking actions against rule violations.
We analyze the technical feasibility of performing these functions in a
targeted and privacy-conscious manner and present a range of technical
instruments. In particular, we describe how non-confidential information, to
which compute providers largely already have access, can provide two key
governance-relevant properties of a computational workload: its type-e.g.,
large-scale training or inference-and the amount of compute it has consumed.
Using AI Executive Order 14110 as a case study, we outline how the US is
beginning to implement record keeping requirements for compute providers. We
also explore how verification and enforcement roles could be added to establish
a comprehensive AI compute oversight scheme. We argue that internationalization
will be key to effective implementation, and highlight the critical challenge
of balancing confidentiality and privacy with risk mitigation as the role of
compute providers in AI regulation expands.


---

**[130. [2408.15121] Aligning XAI with EU Regulations for Smart Biomedical Devices: A
  Methodology for Compliance Analysis](https://arxiv.org/pdf/2408.15121.pdf)** (2024-10-21)

*Francesco Sovrano, Michael Lognoul, Giulia Vilone*

  Significant investment and development have gone into integrating Artificial
Intelligence (AI) in medical and healthcare applications, leading to advanced
control systems in medical technology. However, the opacity of AI systems
raises concerns about essential characteristics needed in such sensitive
applications, like transparency and trustworthiness. Our study addresses these
concerns by investigating a process for selecting the most adequate Explainable
AI (XAI) methods to comply with the explanation requirements of key EU
regulations in the context of smart bioelectronics for medical devices. The
adopted methodology starts with categorising smart devices by their control
mechanisms (open-loop, closed-loop, and semi-closed-loop systems) and delving
into their technology. Then, we analyse these regulations to define their
explainability requirements for the various devices and related goals.
Simultaneously, we classify XAI methods by their explanatory objectives. This
allows for matching legal explainability requirements with XAI explanatory
goals and determining the suitable XAI algorithms for achieving them. Our
findings provide a nuanced understanding of which XAI algorithms align better
with EU regulations for different types of medical devices. We demonstrate this
through practical case studies on different neural implants, from chronic
disease management to advanced prosthetics. This study fills a crucial gap in
aligning XAI applications in bioelectronics with stringent provisions of EU
regulations. It provides a practical framework for developers and researchers,
ensuring their AI innovations advance healthcare technology and adhere to legal
and ethical standards.


---

**[131. [2501.08046] Building Symbiotic AI: Reviewing the AI Act for a Human-Centred,
  Principle-Based Framework](https://arxiv.org/pdf/2501.08046.pdf)** (2025-02-14)

*Miriana Calvano, Antonio Curci, Giuseppe Desolda, Andrea Esposito, Rosa Lanzilotti, Antonio Piccinno*

  Artificial Intelligence (AI) spreads quickly as new technologies and services
take over modern society. The need to regulate AI design, development, and use
is strictly necessary to avoid unethical and potentially dangerous consequences
to humans. The European Union (EU) has released a new legal framework, the AI
Act, to regulate AI by undertaking a risk-based approach to safeguard humans
during interaction. At the same time, researchers offer a new perspective on AI
systems, commonly known as Human-Centred AI (HCAI), highlighting the need for a
human-centred approach to their design. In this context, Symbiotic AI (a
subtype of HCAI) promises to enhance human capabilities through a deeper and
continuous collaboration between human intelligence and AI. This article
presents the results of a Systematic Literature Review (SLR) that aims to
identify principles that characterise the design and development of Symbiotic
AI systems while considering humans as the core of the process. Through content
analysis, four principles emerged from the review that must be applied to
create Human-Centred AI systems that can establish a symbiotic relationship
with humans. In addition, current trends and challenges were defined to
indicate open questions that may guide future research for the development of
SAI systems that comply with the AI Act.


---

**[132. [2409.14603] Brain Surgery: Ensuring GDPR Compliance in Large Language Models via
  Concept Erasure](https://arxiv.org/pdf/2409.14603.pdf)** (2024-09-24)

*Michele Laurelli*

  As large-scale AI systems proliferate, ensuring compliance with data privacy
laws such as the General Data Protection Regulation (GDPR) has become critical.
This paper introduces Brain Surgery, a transformative methodology for making
every local AI model GDPR-ready by enabling real-time privacy management and
targeted unlearning. Building on advanced techniques such as
Embedding-Corrupted Prompts (ECO Prompts), blockchain-based privacy management,
and privacy-aware continual learning, Brain Surgery provides a modular solution
that can be deployed across various AI architectures. This tool not only
ensures compliance with privacy regulations but also empowers users to define
their own privacy limits, creating a new paradigm in AI ethics and governance.


---

**[133. [2503.04259] Qualitative In-Depth Analysis of GDPR Data Subject Access Requests and
  Responses from Major Online Services](https://arxiv.org/pdf/2503.04259.pdf)** (2025-03-07)

*Daniela Pöhn, Nils Gruschka*

  The European General Data Protection Regulation (GDPR) grants European users
the right to access their data processed and stored by organizations. Although
the GDPR contains requirements for data processing organizations (e.g.,
understandable data provided within a month), it leaves much flexibility.
In-depth research on how online services handle data subject access request is
sparse. Specifically, it is unclear whether online services comply with the
individual GDPR requirements, if the privacy policies and the data subject
access responses are coherent, and how the responses change over time. To
answer these questions, we perform a qualitative structured review of the
processes and data exports of significant online services to (1) analyze the
data received in 2023 in detail, (2) compare the data exports with the privacy
policies, and (3) compare the data exports from November 2018 and November
2023. The study concludes that the quality of data subject access responses
varies among the analyzed services, and none fulfills all requirements
completely.


---

**[134. [2407.14981] Open Problems in Technical AI Governance](https://arxiv.org/pdf/2407.14981.pdf)** (2025-04-17)

*Anka Reuel, Ben Bucknall, Stephen Casper, Tim Fist, Lisa Soder, Onni Aarne, Lewis Hammond, Lujain Ibrahim, Alan Chan, Peter Wills, Markus Anderljung, Ben Garfinkel, Lennart Heim, Andrew Trask, Gabriel Mukobi, Rylan Schaeffer, Mauricio Baker, Sara Hooker, Irene Solaiman, Alexandra Sasha Luccioni, Nitarshan Rajkumar, Nicolas Moës, Jeffrey Ladish, David Bau, Paul Bricman, Neel Guha, Jessica Newman, Yoshua Bengio, Tobin South, Alex Pentland, Sanmi Koyejo, Mykel J. Kochenderfer, Robert Trager*

  AI progress is creating a growing range of risks and opportunities, but it is
often unclear how they should be navigated. In many cases, the barriers and
uncertainties faced are at least partly technical. Technical AI governance,
referring to technical analysis and tools for supporting the effective
governance of AI, seeks to address such challenges. It can help to (a) identify
areas where intervention is needed, (b) identify and assess the efficacy of
potential governance actions, and (c) enhance governance options by designing
mechanisms for enforcement, incentivization, or compliance. In this paper, we
explain what technical AI governance is, why it is important, and present a
taxonomy and incomplete catalog of its open problems. This paper is intended as
a resource for technical researchers or research funders looking to contribute
to AI governance.


---

**[135. [2105.06083] Cross-Domain Contract Element Extraction with a Bi-directional Feedback
  Clause-Element Relation Network](https://arxiv.org/pdf/2105.06083.pdf)** (2021-05-14)

*Zihan Wang, Hongye Song, Zhaochun Ren, Pengjie Ren, Zhumin Chen, Xiaozhong Liu, Hongsong Li, Maarten de Rijke*

  Contract element extraction (CEE) is the novel task of automatically
identifying and extracting legally relevant elements such as contract dates,
payments, and legislation references from contracts. Automatic methods for this
task view it as a sequence labeling problem and dramatically reduce human
labor. However, as contract genres and element types may vary widely, a
significant challenge for this sequence labeling task is how to transfer
knowledge from one domain to another, i.e., cross-domain CEE. Cross-domain CEE
differs from cross-domain named entity recognition (NER) in two important ways.
First, contract elements are far more fine-grained than named entities, which
hinders the transfer of extractors. Second, the extraction zones for
cross-domain CEE are much larger than for cross-domain NER. As a result, the
contexts of elements from different domains can be more diverse. We propose a
framework, the Bi-directional Feedback cLause-Element relaTion network
(Bi-FLEET), for the cross-domain CEE task that addresses the above challenges.
Bi-FLEET has three main components: (1) a context encoder, (2) a clause-element
relation encoder, and (3) an inference layer. To incorporate invariant
knowledge about element and clause types, a clause-element graph is constructed
across domains and a hierarchical graph neural network is adopted in the
clause-element relation encoder. To reduce the influence of context variations,
a multi-task framework with a bi-directional feedback scheme is designed in the
inference layer, conducting both clause classification and element extraction.
The experimental results over both cross-domain NER and CEE tasks show that
Bi-FLEET significantly outperforms state-of-the-art baselines.


---

**[136. [2403.00148] Implications of Regulations on the Use of AI and Generative AI for
  Human-Centered Responsible Artificial Intelligence](https://arxiv.org/pdf/2403.00148.pdf)** (2024-03-04)

*Marios Constantinides, Mohammad Tahaei, Daniele Quercia, Simone Stumpf, Michael Madaio, Sean Kennedy, Lauren Wilcox, Jessica Vitak, Henriette Cramer, Edyta Bogucka, Ricardo Baeza-Yates, Ewa Luger, Jess Holbrook, Michael Muller, Ilana Golbin Blumenfeld, Giada Pistilli*

  With the upcoming AI regulations (e.g., EU AI Act) and rapid advancements in
generative AI, new challenges emerge in the area of Human-Centered Responsible
Artificial Intelligence (HCR-AI). As AI becomes more ubiquitous, questions
around decision-making authority, human oversight, accountability,
sustainability, and the ethical and legal responsibilities of AI and their
creators become paramount. Addressing these questions requires a collaborative
approach. By involving stakeholders from various disciplines in the
2\textsuperscript{nd} edition of the HCR-AI Special Interest Group (SIG) at CHI
2024, we aim to discuss the implications of regulations in HCI research,
develop new theories, evaluation frameworks, and methods to navigate the
complex nature of AI ethics, steering AI development in a direction that is
beneficial and sustainable for all of humanity.


---

**[137. [2412.17114] Decentralized Governance of Autonomous AI Agents](https://arxiv.org/pdf/2412.17114.pdf)** (2025-01-14)

*Tomer Jordi Chaffer, II Charles von Goins, Bayo Okusanya, Dontrail Cotlage, Justin Goldston*

  Autonomous AI agents present transformative opportunities and significant
governance challenges. Existing frameworks, such as the EU AI Act and the NIST
AI Risk Management Framework, fall short of addressing the complexities of
these agents, which are capable of independent decision-making, learning, and
adaptation. To bridge these gaps, we propose the ETHOS (Ethical Technology and
Holistic Oversight System) framework, a decentralized governance (DeGov) model
leveraging Web3 technologies, including blockchain, smart contracts, and
decentralized autonomous organizations (DAOs). ETHOS establishes a global
registry for AI agents, enabling dynamic risk classification, proportional
oversight, and automated compliance monitoring through tools like soulbound
tokens and zero-knowledge proofs. Furthermore, the framework incorporates
decentralized justice systems for transparent dispute resolution and introduces
AI specific legal entities to manage limited liability, supported by mandatory
insurance to ensure financial accountability and incentivize ethical design. By
integrating philosophical principles of rationality, ethical grounding, and
goal alignment, ETHOS aims to create a robust research agenda for promoting
trust, transparency, and participatory governance. This innovative framework
offers a scalable and inclusive strategy for regulating AI agents, balancing
innovation with ethical responsibility to meet the demands of an AI-driven
future.


---

**[138. [2410.09004] DA-Ada: Learning Domain-Aware Adapter for Domain Adaptive Object
  Detection](https://arxiv.org/pdf/2410.09004.pdf)** (2024-10-14)

*Haochen Li, Rui Zhang, Hantao Yao, Xin Zhang, Yifan Hao, Xinkai Song, Xiaqing Li, Yongwei Zhao, Ling Li, Yunji Chen*

  Domain adaptive object detection (DAOD) aims to generalize detectors trained
on an annotated source domain to an unlabelled target domain. As the
visual-language models (VLMs) can provide essential general knowledge on unseen
images, freezing the visual encoder and inserting a domain-agnostic adapter can
learn domain-invariant knowledge for DAOD. However, the domain-agnostic adapter
is inevitably biased to the source domain. It discards some beneficial
knowledge discriminative on the unlabelled domain, i.e., domain-specific
knowledge of the target domain. To solve the issue, we propose a novel
Domain-Aware Adapter (DA-Ada) tailored for the DAOD task. The key point is
exploiting domain-specific knowledge between the essential general knowledge
and domain-invariant knowledge. DA-Ada consists of the Domain-Invariant Adapter
(DIA) for learning domain-invariant knowledge and the Domain-Specific Adapter
(DSA) for injecting the domain-specific knowledge from the information
discarded by the visual encoder. Comprehensive experiments over multiple DAOD
tasks show that DA-Ada can efficiently infer a domain-aware visual encoder for
boosting domain adaptive object detection. Our code is available at
https://github.com/Therock90421/DA-Ada.


---

**[139. [2109.11960] Towards a Governance Framework for Brain Data](https://arxiv.org/pdf/2109.11960.pdf)** (2021-09-29)

*Marcello Ienca, Joseph J. Fins, Ralf J. Jox, Fabrice Jotterand, Silja Voeneky, Roberto Andorno, Tonio Ball, Claude Castelluccia, Ricardo Chavarriaga, Hervé Chneiweiss, Agata Ferretti, Orsolya Friedrich, Samia Hurst, Grischa Merkel, Fruzsina Molnar-Gabor, Jean-Marc Rickli, James Scheibner, Effy Vayena, Rafael Yuste, Philipp Kellmeyer*

  The increasing availability of brain data within and outside the biomedical
field, combined with the application of artificial intelligence (AI) to brain
data analysis, poses a challenge for ethics and governance. We identify
distinctive ethical implications of brain data acquisition and processing, and
outline a multi-level governance framework. This framework is aimed at
maximizing the benefits of facilitated brain data collection and further
processing for science and medicine whilst minimizing risks and preventing
harmful use. The framework consists of four primary areas of regulatory
intervention: binding regulation, ethics and soft law, responsible innovation,
and human rights.


---

**[140. [2103.09051] Exploring the Assessment List for Trustworthy AI in the Context of
  Advanced Driver-Assistance Systems](https://arxiv.org/pdf/2103.09051.pdf)** (2021-03-17)

*Markus Borg, Joshua Bronson, Linus Christensson, Fredrik Olsson, Olof Lennartsson, Elias Sonnsjö, Hamid Ebabi, Martin Karsberg*

  Artificial Intelligence (AI) is increasingly used in critical applications.
Thus, the need for dependable AI systems is rapidly growing. In 2018, the
European Commission appointed experts to a High-Level Expert Group on AI
(AI-HLEG). AI-HLEG defined Trustworthy AI as 1) lawful, 2) ethical, and 3)
robust and specified seven corresponding key requirements. To help development
organizations, AI-HLEG recently published the Assessment List for Trustworthy
AI (ALTAI). We present an illustrative case study from applying ALTAI to an
ongoing development project of an Advanced Driver-Assistance System (ADAS) that
relies on Machine Learning (ML). Our experience shows that ALTAI is largely
applicable to ADAS development, but specific parts related to human agency and
transparency can be disregarded. Moreover, bigger questions related to societal
and environmental impact cannot be tackled by an ADAS supplier in isolation. We
present how we plan to develop the ADAS to ensure ALTAI-compliance. Finally, we
provide three recommendations for the next revision of ALTAI, i.e., life-cycle
variants, domain-specific adaptations, and removed redundancy.


---

**[141. [2503.14539] Ethical Implications of AI in Data Collection: Balancing Innovation with
  Privacy](https://arxiv.org/pdf/2503.14539.pdf)** (2025-03-20)

*Shahmar Mirishli*

  This article examines the ethical and legal implications of artificial
intelligence (AI) driven data collection, focusing on developments from 2023 to
2024. It analyzes recent advancements in AI technologies and their impact on
data collection practices across various sectors. The study compares regulatory
approaches in the European Union, the United States, and China, highlighting
the challenges in creating a globally harmonized framework for AI governance.
Key ethical issues, including informed consent, algorithmic bias, and privacy
protection, are critically assessed in the context of increasingly
sophisticated AI systems. The research explores case studies in healthcare,
finance, and smart cities to illustrate the practical challenges of AI
implementation. It evaluates the effectiveness of current legal frameworks and
proposes solutions encompassing legal and policy recommendations, technical
safeguards, and ethical frameworks. The article emphasizes the need for
adaptive governance and international cooperation to address the global nature
of AI development while balancing innovation with the protection of individual
rights and societal values.


---

**[142. [2312.00051] MIA-BAD: An Approach for Enhancing Membership Inference Attack and its
  Mitigation with Federated Learning](https://arxiv.org/pdf/2312.00051.pdf)** (2023-12-04)

*Soumya Banerjee, Sandip Roy, Sayyed Farid Ahamed, Devin Quinn, Marc Vucovich, Dhruv Nandakumar, Kevin Choi, Abdul Rahman, Edward Bowen, Sachin Shetty*

  The membership inference attack (MIA) is a popular paradigm for compromising
the privacy of a machine learning (ML) model. MIA exploits the natural
inclination of ML models to overfit upon the training data. MIAs are trained to
distinguish between training and testing prediction confidence to infer
membership information. Federated Learning (FL) is a privacy-preserving ML
paradigm that enables multiple clients to train a unified model without
disclosing their private data. In this paper, we propose an enhanced Membership
Inference Attack with the Batch-wise generated Attack Dataset (MIA-BAD), a
modification to the MIA approach. We investigate that the MIA is more accurate
when the attack dataset is generated batch-wise. This quantitatively decreases
the attack dataset while qualitatively improving it. We show how training an ML
model through FL, has some distinct advantages and investigate how the threat
introduced with the proposed MIA-BAD approach can be mitigated with FL
approaches. Finally, we demonstrate the qualitative effects of the proposed
MIA-BAD methodology by conducting extensive experiments with various target
datasets, variable numbers of federated clients, and training batch sizes.


---

**[143. [2503.05737] Local Differences, Global Lessons: Insights from Organisation Policies
  for International Legislation](https://arxiv.org/pdf/2503.05737.pdf)** (2025-03-11)

*Lucie-Aimée Kaffee, Pepa Atanasova, Anna Rogers*

  The rapid adoption of AI across diverse domains has led to the development of
organisational guidelines that vary significantly, even within the same sector.
This paper examines AI policies in two domains, news organisations and
universities, to understand how bottom-up governance approaches shape AI usage
and oversight. By analysing these policies, we identify key areas of
convergence and divergence in how organisations address risks such as bias,
privacy, misinformation, and accountability. We then explore the implications
of these findings for international AI legislation, particularly the EU AI Act,
highlighting gaps where practical policy insights could inform regulatory
refinements. Our analysis reveals that organisational policies often address
issues such as AI literacy, disclosure practices, and environmental impact,
areas that are underdeveloped in existing international frameworks. We argue
that lessons from domain-specific AI policies can contribute to more adaptive
and effective AI governance at the global level. This study provides actionable
recommendations for policymakers seeking to bridge the gap between local AI
practices and international regulations.


---

**[144. [2412.19494] Retrieval-augmented Generation for GenAI-enabled Semantic Communications](https://arxiv.org/pdf/2412.19494.pdf)** (2024-12-30)

*Shunpu Tang, Ruichen Zhang, Yuxuan Yan, Qianqian Yang, Dusit Niyato, Xianbin Wang, Shiwen Mao*

  Semantic communication (SemCom) is an emerging paradigm aiming at
transmitting only task-relevant semantic information to the receiver, which can
significantly improve communication efficiency. Recent advancements in
generative artificial intelligence (GenAI) have empowered GenAI-enabled SemCom
(GenSemCom) to further expand its potential in various applications. However,
current GenSemCom systems still face challenges such as semantic inconsistency,
limited adaptability to diverse tasks and dynamic environments, and the
inability to leverage insights from past transmission. Motivated by the success
of retrieval-augmented generation (RAG) in the domain of GenAI, this paper
explores the integration of RAG in GenSemCom systems. Specifically, we first
provide a comprehensive review of existing GenSemCom systems and the
fundamentals of RAG techniques. We then discuss how RAG can be integrated into
GenSemCom. Following this, we conduct a case study on semantic image
transmission using an RAG-enabled diffusion-based SemCom system, demonstrating
the effectiveness of the proposed integration. Finally, we outline future
directions for advancing RAG-enabled GenSemCom systems.


---

**[145. [2404.17522] Enhancing Legal Compliance and Regulation Analysis with Large Language
  Models](https://arxiv.org/pdf/2404.17522.pdf)** (2024-04-29)

*Shabnam Hassani*

  This research explores the application of Large Language Models (LLMs) for
automating the extraction of requirement-related legal content in the food
safety domain and checking legal compliance of regulatory artifacts. With
Industry 4.0 revolutionizing the food industry and with the General Data
Protection Regulation (GDPR) reshaping privacy policies and data processing
agreements, there is a growing gap between regulatory analysis and recent
technological advancements. This study aims to bridge this gap by leveraging
LLMs, namely BERT and GPT models, to accurately classify legal provisions and
automate compliance checks. Our findings demonstrate promising results,
indicating LLMs' significant potential to enhance legal compliance and
regulatory analysis efficiency, notably by reducing manual workload and
improving accuracy within reasonable time and financial constraints.


---

**[146. [2410.00608] Measurement challenges in AI catastrophic risk governance and safety
  frameworks](https://arxiv.org/pdf/2410.00608.pdf)** (2024-10-02)

*Atoosa Kasirzadeh*

  Safety frameworks represent a significant development in AI governance: they
are the first type of publicly shared catastrophic risk management framework
developed by major AI companies and focus specifically on AI scaling decisions.
I identify six critical measurement challenges in their implementation and
propose three policy recommendations to improve their validity and reliability.


---

**[147. [2102.00980] A Common Semantic Model of the GDPR Register of Processing Activities](https://arxiv.org/pdf/2102.00980.pdf)** (2021-02-02)

*Paul Ryan, Harshvardhan J. Pandit, Rob Brennan*

  The creation and maintenance of a Register of Processing Activities (ROPA) is
an essential process for the demonstration of GDPR compliance. We analyse ROPA
templates from six EU Data Protection Regulators and show that template scope
and granularity vary widely between jurisdictions. We then propose a flexible,
consolidated data model for consistent processing of ROPAs (CSM-ROPA). We
analyse the extent that the Data Privacy Vocabulary (DPV) can be used to
express CSM-ROPA. We find that it does not directly address modelling ROPAs,
and so needs additional concept definitions. We provide a mapping of our
CSM-ROPA to an extension of the Data Privacy Vocabulary.


---

**[148. [2206.00113] BRExIt: On Opponent Modelling in Expert Iteration](https://arxiv.org/pdf/2206.00113.pdf)** (2023-04-26)

*Daniel Hernandez, Hendrik Baier, Michael Kaisers*

  Finding a best response policy is a central objective in game theory and
multi-agent learning, with modern population-based training approaches
employing reinforcement learning algorithms as best-response oracles to improve
play against candidate opponents (typically previously learnt policies). We
propose Best Response Expert Iteration (BRExIt), which accelerates learning in
games by incorporating opponent models into the state-of-the-art learning
algorithm Expert Iteration (ExIt). BRExIt aims to (1) improve feature shaping
in the apprentice, with a policy head predicting opponent policies as an
auxiliary task, and (2) bias opponent moves in planning towards the given or
learnt opponent model, to generate apprentice targets that better approximate a
best response. In an empirical ablation on BRExIt's algorithmic variants
against a set of fixed test agents, we provide statistical evidence that BRExIt
learns better performing policies than ExIt.


---

**[149. [2409.07476] Responsible AI for Test Equity and Quality: The Duolingo English Test as
  a Case Study](https://arxiv.org/pdf/2409.07476.pdf)** (2024-09-13)

*Jill Burstein, Geoffrey T. LaFlair, Kevin Yancey, Alina A. von Davier, Ravit Dotan*

  Artificial intelligence (AI) creates opportunities for assessments, such as
efficiencies for item generation and scoring of spoken and written responses.
At the same time, it poses risks (such as bias in AI-generated item content).
Responsible AI (RAI) practices aim to mitigate risks associated with AI. This
chapter addresses the critical role of RAI practices in achieving test quality
(appropriateness of test score inferences), and test equity (fairness to all
test takers). To illustrate, the chapter presents a case study using the
Duolingo English Test (DET), an AI-powered, high-stakes English language
assessment. The chapter discusses the DET RAI standards, their development and
their relationship to domain-agnostic RAI principles. Further, it provides
examples of specific RAI practices, showing how these practices meaningfully
address the ethical principles of validity and reliability, fairness, privacy
and security, and transparency and accountability standards to ensure test
equity and quality.


---

**[150. [2502.15720] Training AI to be Loyal](https://arxiv.org/pdf/2502.15720.pdf)** (2025-02-25)

*Sewoong Oh, Himanshu Tyagi, Pramod Viswanath*

  Loyal AI is loyal to the community that builds it. An AI is loyal to a
community if the community has ownership, alignment, and control. Community
owned models can only be used with the approval of the community and share the
economic rewards communally. Community aligned models have values that are
aligned with the consensus of the community. Community controlled models
perform functions designed by the community. Since we would like permissionless
access to the loyal AI's community, we need the AI to be open source. The key
scientific question then is: how can we build models that are openly accessible
(open source) and yet are owned and governed by the community. This seeming
impossibility is the focus of this paper where we outline a concrete pathway to
Open, Monetizable and Loyal models (OML), building on our earlier work on OML,
arXiv:2411.03887(1) , and a representation via a cryptographic-ML library
http://github.com/sentient-agi/oml-1.0-fingerprinting .


---

**[151. [2307.04699] International Institutions for Advanced AI](https://arxiv.org/pdf/2307.04699.pdf)** (2023-07-12)

*Lewis Ho, Joslyn Barnhart, Robert Trager, Yoshua Bengio, Miles Brundage, Allison Carnegie, Rumman Chowdhury, Allan Dafoe, Gillian Hadfield, Margaret Levi, Duncan Snidal*

  International institutions may have an important role to play in ensuring
advanced AI systems benefit humanity. International collaborations can unlock
AI's ability to further sustainable development, and coordination of regulatory
efforts can reduce obstacles to innovation and the spread of benefits.
Conversely, the potential dangerous capabilities of powerful and
general-purpose AI systems create global externalities in their development and
deployment, and international efforts to further responsible AI practices could
help manage the risks they pose. This paper identifies a set of governance
functions that could be performed at an international level to address these
challenges, ranging from supporting access to frontier AI systems to setting
international safety standards. It groups these functions into four
institutional models that exhibit internal synergies and have precedents in
existing organizations: 1) a Commission on Frontier AI that facilitates expert
consensus on opportunities and risks from advanced AI, 2) an Advanced AI
Governance Organization that sets international standards to manage global
threats from advanced models, supports their implementation, and possibly
monitors compliance with a future governance regime, 3) a Frontier AI
Collaborative that promotes access to cutting-edge AI, and 4) an AI Safety
Project that brings together leading researchers and engineers to further AI
safety research. We explore the utility of these models and identify open
questions about their viability.


---

**[152. [2503.07384] Is My Text in Your AI Model? Gradient-based Membership Inference Test
  applied to LLMs](https://arxiv.org/pdf/2503.07384.pdf)** (2025-03-14)

*Gonzalo Mancera, Daniel DeAlcala, Julian Fierrez, Ruben Tolosana, Aythami Morales*

  This work adapts and studies the gradient-based Membership Inference Test
(gMINT) to the classification of text based on LLMs. MINT is a general approach
intended to determine if given data was used for training machine learning
models, and this work focuses on its application to the domain of Natural
Language Processing. Using gradient-based analysis, the MINT model identifies
whether particular data samples were included during the language model
training phase, addressing growing concerns about data privacy in machine
learning. The method was evaluated in seven Transformer-based models and six
datasets comprising over 2.5 million sentences, focusing on text classification
tasks. Experimental results demonstrate MINTs robustness, achieving AUC scores
between 85% and 99%, depending on data size and model architecture. These
findings highlight MINTs potential as a scalable and reliable tool for auditing
machine learning models, ensuring transparency, safeguarding sensitive data,
and fostering ethical compliance in the deployment of AI/NLP technologies.


---

**[153. [2406.14243] AuditMAI: Towards An Infrastructure for Continuous AI Auditing](https://arxiv.org/pdf/2406.14243.pdf)** (2024-06-21)

*Laura Waltersdorfer, Fajar J. Ekaputra, Tomasz Miksa, Marta Sabou*

  Artificial Intelligence (AI) Auditability is a core requirement for achieving
responsible AI system design. However, it is not yet a prominent design feature
in current applications. Existing AI auditing tools typically lack integration
features and remain as isolated approaches. This results in manual,
high-effort, and mostly one-off AI audits, necessitating alternative methods.
Inspired by other domains such as finance, continuous AI auditing is a
promising direction to conduct regular assessments of AI systems. The issue
remains, however, since the methods for continuous AI auditing are not mature
yet at the moment. To address this gap, we propose the Auditability Method for
AI (AuditMAI), which is intended as a blueprint for an infrastructure towards
continuous AI auditing. For this purpose, we first clarified the definition of
AI auditability based on literature. Secondly, we derived requirements from two
industrial use cases for continuous AI auditing tool support. Finally, we
developed AuditMAI and discussed its elements as a blueprint for a continuous
AI auditability infrastructure.


---

**[154. [2012.12718] Compliance Generation for Privacy Documents under GDPR: A Roadmap for
  Implementing Automation and Machine Learning](https://arxiv.org/pdf/2012.12718.pdf)** (2020-12-24)

*David Restrepo Amariles, Aurore Clément Troussel, Rajaa El Hamdani*

  Most prominent research today addresses compliance with data protection laws
through consumer-centric and public-regulatory approaches. We shift this
perspective with the Privatech project to focus on corporations and law firms
as agents of compliance. To comply with data protection laws, data processors
must implement accountability measures to assess and document compliance in
relation to both privacy documents and privacy practices. In this paper, we
survey, on the one hand, current research on GDPR automation, and on the other
hand, the operational challenges corporations face to comply with GDPR, and
that may benefit from new forms of automation. We attempt to bridge the gap. We
provide a roadmap for compliance assessment and generation by identifying
compliance issues, breaking them down into tasks that can be addressed through
machine learning and automation, and providing notes about related developments
in the Privatech project.


---

**[155. [2412.17848] Overview of the 2024 ALTA Shared Task: Detect Automatic AI-Generated
  Sentences for Human-AI Hybrid Articles](https://arxiv.org/pdf/2412.17848.pdf)** (2024-12-25)

*Diego Mollá, Qiongkai Xu, Zijie Zeng, Zhuang Li*

  The ALTA shared tasks have been running annually since 2010. In 2024, the
purpose of the task is to detect machine-generated text in a hybrid setting
where the text may contain portions of human text and portions
machine-generated. In this paper, we present the task, the evaluation criteria,
and the results of the systems participating in the shared task.


---

**[156. [2412.14684] Bel Esprit: Multi-Agent Framework for Building AI Model Pipelines](https://arxiv.org/pdf/2412.14684.pdf)** (2024-12-20)

*Yunsu Kim, AhmedElmogtaba Abdelaziz, Thiago Castro Ferreira, Mohamed Al-Badrashiny, Hassan Sawaf*

  As the demand for artificial intelligence (AI) grows to address complex
real-world tasks, single models are often insufficient, requiring the
integration of multiple models into pipelines. This paper introduces Bel
Esprit, a conversational agent designed to construct AI model pipelines based
on user-defined requirements. Bel Esprit employs a multi-agent framework where
subagents collaborate to clarify requirements, build, validate, and populate
pipelines with appropriate models. We demonstrate the effectiveness of this
framework in generating pipelines from ambiguous user queries, using both
human-curated and synthetic data. A detailed error analysis highlights ongoing
challenges in pipeline construction. Bel Esprit is available for a free trial
at https://belesprit.aixplain.com.


---

**[157. [2402.15770] From COBIT to ISO 42001: Evaluating Cybersecurity Frameworks for
  Opportunities, Risks, and Regulatory Compliance in Commercializing Large
  Language Models](https://arxiv.org/pdf/2402.15770.pdf)** (2024-06-25)

*Timothy R. McIntosh, Teo Susnjak, Tong Liu, Paul Watters, Raza Nowrozy, Malka N. Halgamuge*

  This study investigated the integration readiness of four predominant
cybersecurity Governance, Risk and Compliance (GRC) frameworks - NIST CSF 2.0,
COBIT 2019, ISO 27001:2022, and the latest ISO 42001:2023 - for the
opportunities, risks, and regulatory compliance when adopting Large Language
Models (LLMs), using qualitative content analysis and expert validation. Our
analysis, with both LLMs and human experts in the loop, uncovered potential for
LLM integration together with inadequacies in LLM risk oversight of those
frameworks. Comparative gap analysis has highlighted that the new ISO
42001:2023, specifically designed for Artificial Intelligence (AI) management
systems, provided most comprehensive facilitation for LLM opportunities,
whereas COBIT 2019 aligned most closely with the impending European Union AI
Act. Nonetheless, our findings suggested that all evaluated frameworks would
benefit from enhancements to more effectively and more comprehensively address
the multifaceted risks associated with LLMs, indicating a critical and
time-sensitive need for their continuous evolution. We propose integrating
human-expert-in-the-loop validation processes as crucial for enhancing
cybersecurity frameworks to support secure and compliant LLM integration, and
discuss implications for the continuous evolution of cybersecurity GRC
frameworks to support the secure integration of LLMs.


---

**[158. [2503.05747] Balancing Innovation and Integrity: AI Integration in Liberal Arts
  College Administration](https://arxiv.org/pdf/2503.05747.pdf)** (2025-03-11)

*Ian Olivo Read*

  This paper explores the intersection of artificial intelligence and higher
education administration, focusing on liberal arts colleges (LACs). It examines
AI's opportunities and challenges in academic and student affairs, legal
compliance, and accreditation processes, while also addressing the ethical
considerations of AI deployment in mission-driven institutions. Considering
AI's value pluralism and potential allocative or representational harms caused
by algorithmic bias, LACs must ensure AI aligns with its mission and
principles. The study highlights other strategies for responsible AI
integration, balancing innovation with institutional values.


---

**[159. [2112.11232] Human Activity Recognition (HAR) in Smart Homes](https://arxiv.org/pdf/2112.11232.pdf)** (2021-12-22)

*IMT Atlantique - INFO, Lab-STICC  Damien Bouchabou, IMT Atlantique - INFO, Lab-STICC  Christophe Lohr, Ioannis Kanellos, Sao Mai Nguyen*

  Generally, Human Activity Recognition (HAR) consists of monitoring and
analyzing the behavior of one or more persons in order to deduce their
activity. In a smart home context, the HAR consists of monitoring daily
activities of the residents. Thanks to this monitoring, a smart home can offer
home assistance services to improve quality of life, autonomy and health of
their residents, especially for elderly and dependent people.


---

**[160. [2309.17158] Compromise in Multilateral Negotiations and the Global Regulation of
  Artificial Intelligence](https://arxiv.org/pdf/2309.17158.pdf)** (2023-10-02)

*Michal Natorski*

  As artificial intelligence (AI) technologies spread worldwide, international
discussions have increasingly focused on their consequences for democracy,
human rights, fundamental freedoms, security, and economic and social
development. In this context, UNESCO's Recommendation on the Ethics of
Artificial Intelligence, adopted in November 2021, has emerged as the first
global normative framework for AI development and deployment. The intense
negotiations of every detail of the document brought forth numerous
controversies among UNESCO member states. Drawing on a unique set of primary
sources, including written positions and recorded deliberations, this paper
explains the achievement of global compromise on AI regulation despite the
multiplicity of UNESCO member-state positions representing a variety of liberal
and sovereignist preferences. Building upon Boltanski's pragmatic sociology, it
conceptualises the practice of multilateral negotiations and attributes the
multilateral compromise to two embedded therein mechanisms: Structural
normative hybridity and situated normative ambiguity allowed to accomplish a
compromise by linking macro-normative structures with situated debates of
multilateral negotiations.


---

**[161. [2009.13250] Advancing the Research and Development of Assured Artificial
  Intelligence and Machine Learning Capabilities](https://arxiv.org/pdf/2009.13250.pdf)** (2020-09-29)

*Tyler J. Shipp, Daniel J. Clouse, Michael J. De Lucia, Metin B. Ahiskali, Kai Steverson, Jonathan M. Mullin, Nathaniel D. Bastian*

  Artificial intelligence (AI) and machine learning (ML) have become
increasingly vital in the development of novel defense and intelligence
capabilities across all domains of warfare. An adversarial AI (A2I) and
adversarial ML (AML) attack seeks to deceive and manipulate AI/ML models. It is
imperative that AI/ML models can defend against these attacks. A2I/AML defenses
will help provide the necessary assurance of these advanced capabilities that
use AI/ML models. The A2I Working Group (A2IWG) seeks to advance the research
and development of assured AI/ML capabilities via new A2I/AML defenses by
fostering a collaborative environment across the U.S. Department of Defense and
U.S. Intelligence Community. The A2IWG aims to identify specific challenges
that it can help solve or address more directly, with initial focus on three
topics: AI Trusted Robustness, AI System Security, and AI/ML Architecture
Vulnerabilities.


---

**[162. [2409.00264] The Artificial Intelligence Act: critical overview](https://arxiv.org/pdf/2409.00264.pdf)** (2024-09-04)

*Nuno Sousa e Silva*

  This article provides a critical overview of the recently approved Artificial
Intelligence Act. It starts by presenting the main structure, objectives, and
approach of Regulation (EU) 2024/1689. A definition of key concepts follows,
and then the material and territorial scope, as well as the timing of
application, are analyzed. Although the Regulation does not explicitly set out
principles, the main ideas of fairness, accountability, transparency, and
equity in AI underly a set of rules of the regulation. This is discussed before
looking at the ill-defined set of forbidden AI practices (manipulation and e
exploitation of vulnerabilities, social scoring, biometric identification and
classification, and predictive policing). It is highlighted that those rules
deal with behaviors rather than AI systems. The qualification and regulation of
high-risk AI systems are tackled, alongside the obligation of transparency for
certain systems, the regulation of general-purpose models, and the rules on
certification, supervision, and sanctions. The text concludes that even if the
overall framework can be deemed adequate and balanced, the approach is so
complex that it risks defeating its own purpose of promoting responsible
innovation within the European Union and beyond its borders.


---

**[163. [2205.10929] rgpdOS: GDPR Enforcement By The Operating System](https://arxiv.org/pdf/2205.10929.pdf)** (2022-05-31)

*Alain Tchana, Raphael Colin, Adrien Le Berre, Vincent Berger, Benoit Combemale, Natacha Crooks, Ludovic Pailler*

  The General Data Protection Regulation (GDPR) forces IT companies to comply
with a number of principles when dealing with European citizens' personal data.
Non-compliant companies are exposed to penalties which may represent up to 4%
of their turnover. Currently, it is very hard for companies driven by personal
data to make their applications GDPR-compliant, especially if those
applications were developed before the GDPR was established. We present rgpdOS,
a GDPR-aware operating system that aims to bring GDPR-compliance to every
application, while requiring minimal changes to application code.


---

**[164. [2402.05968] Federated Learning Priorities Under the European Union Artificial
  Intelligence Act](https://arxiv.org/pdf/2402.05968.pdf)** (2024-02-14)

*Herbert Woisetschläger, Alexander Erben, Bill Marino, Shiqiang Wang, Nicholas D. Lane, Ruben Mayer, Hans-Arno Jacobsen*

  The age of AI regulation is upon us, with the European Union Artificial
Intelligence Act (AI Act) leading the way. Our key inquiry is how this will
affect Federated Learning (FL), whose starting point of prioritizing data
privacy while performing ML fundamentally differs from that of centralized
learning. We believe the AI Act and future regulations could be the missing
catalyst that pushes FL toward mainstream adoption. However, this can only
occur if the FL community reprioritizes its research focus. In our position
paper, we perform a first-of-its-kind interdisciplinary analysis (legal and ML)
of the impact the AI Act may have on FL and make a series of observations
supporting our primary position through quantitative and qualitative analysis.
We explore data governance issues and the concern for privacy. We establish new
challenges regarding performance and energy efficiency within lifecycle
monitoring. Taken together, our analysis suggests there is a sizable
opportunity for FL to become a crucial component of AI Act-compliant ML systems
and for the new regulation to drive the adoption of FL techniques in general.
Most noteworthy are the opportunities to defend against data bias and enhance
private and secure computation


---

**[165. [2306.16127] MLSMM: Machine Learning Security Maturity Model](https://arxiv.org/pdf/2306.16127.pdf)** (2023-06-29)

*Felix Jedrzejewski, Davide Fucci, Oleksandr Adamov*

  Assessing the maturity of security practices during the development of
Machine Learning (ML) based software components has not gotten as much
attention as traditional software development. In this Blue Sky idea paper, we
propose an initial Machine Learning Security Maturity Model (MLSMM) which
organizes security practices along the ML-development lifecycle and, for each,
establishes three levels of maturity. We envision MLSMM as a step towards
closer collaboration between industry and academia.


---

**[166. [2311.10733] Proceedings of the 3rd International Workshop on Mining and Learning in
  the Legal Domain (MLLD-23)](https://arxiv.org/pdf/2311.10733.pdf)** (2023-11-21)

*Masoud Makrehchi, Dell Zhang, Alina Petrova, John Armour*

  This is the Proceedings of the 3rd International Workshop on Mining and
Learning in the Legal Domain (MLLD-23) which took place in conjunction with the
32nd ACM International Conference on Information and Knowledge Management
(CIKM-2023) at the University of Birmingham, Birmingham, UK on Sunday 22nd
October 2023.


---

**[167. [2208.06327] Developing moral AI to support antimicrobial decision making](https://arxiv.org/pdf/2208.06327.pdf)** (2022-12-06)

*William J Bolton, Cosmin Badea, Pantelis Georgiou, Alison Holmes, Timothy M Rawson*

  Artificial intelligence (AI) assisting with antimicrobial prescribing raises
significant moral questions. Utilising ethical frameworks alongside AI-driven
systems, while considering infection specific complexities, can support moral
decision making to tackle antimicrobial resistance.


---

**[168. [2404.12576] Requirements Satisfiability with In-Context Learning](https://arxiv.org/pdf/2404.12576.pdf)** (2024-04-22)

*Sarah Santos, Travis Breaux, Thomas Norton, Sara Haghighi, Sepideh Ghanavati*

  Language models that can learn a task at inference time, called in-context
learning (ICL), show increasing promise in natural language inference tasks. In
ICL, a model user constructs a prompt to describe a task with a natural
language instruction and zero or more examples, called demonstrations. The
prompt is then input to the language model to generate a completion. In this
paper, we apply ICL to the design and evaluation of satisfaction arguments,
which describe how a requirement is satisfied by a system specification and
associated domain knowledge. The approach builds on three prompt design
patterns, including augmented generation, prompt tuning, and chain-of-thought
prompting, and is evaluated on a privacy problem to check whether a mobile app
scenario and associated design description satisfies eight consent requirements
from the EU General Data Protection Regulation (GDPR). The overall results show
that GPT-4 can be used to verify requirements satisfaction with 96.7% accuracy
and dissatisfaction with 93.2% accuracy. Inverting the requirement improves
verification of dissatisfaction to 97.2%. Chain-of-thought prompting improves
overall GPT-3.5 performance by 9.0% accuracy. We discuss the trade-offs among
templates, models and prompt strategies and provide a detailed analysis of the
generated specifications to inform how the approach can be applied in practice.


---

**[169. [2101.08095] Automatic Differentiation via Effects and Handlers: An Implementation in
  Frank](https://arxiv.org/pdf/2101.08095.pdf)** (2021-01-21)

*Jesse Sigal*

  Automatic differentiation (AD) is an important family of algorithms which
enables derivative based optimization. We show that AD can be simply
implemented with effects and handlers by doing so in the Frank language. By
considering how our implementation behaves in Frank's operational semantics, we
show how our code performs the dynamic creation of programs during evaluation.


---

**[170. [2204.03028] Software Testing, AI and Robotics (STAIR) Learning Lab](https://arxiv.org/pdf/2204.03028.pdf)** (2022-04-08)

*Simon Haller-Seeber, Thomas Gatterer, Patrick Hofmann, Christopher Kelter, Thomas Auer, Michael Felderer*

  In this paper we presented the Software Testing, AI and Robotics (STAIR)
Learning Lab. STAIR is an initiative started at the University of Innsbruck to
bring robotics, Artificial Intelligence (AI) and software testing into schools.
In the lab physical and virtual learning units are developed in parallel and in
sync with each other. Its core learning approach is based the develop of both a
physical and simulated robotics environment. In both environments AI scenarios
(like traffic sign recognition) are deployed and tested. We present and focus
on our newly designed MiniBot that are both built on hardware which was
designed for educational and research purposes as well as the simulation
environment. Additionally, we describe first learning design concepts and a
showcase scenario (i.e., AI-based traffic sign recognition) with different
exercises which can easily be extended.


---

**[171. [2305.15922] Towards a Capability Assessment Model for the Comprehension and Adoption
  of AI in Organisations](https://arxiv.org/pdf/2305.15922.pdf)** (2023-05-26)

*Butler, Tom, Espinoza-Limón, Angelina, Seppälä, Selja*

  The comprehension and adoption of Artificial Intelligence (AI) are beset with
practical and ethical problems. This article presents a 5-level AI Capability
Assessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to
assist practitioners in AI comprehension and adoption. These practical tools
were developed with business executives, technologists, and other
organisational stakeholders in mind. They are founded on a comprehensive
conception of AI compared to those in other AI adoption models and are also
open-source artefacts. Thus, the AI-CAM and AI-CM present an accessible
resource to help inform organisational decision-makers on the capability
requirements for (1) AI-based data analytics use cases based on machine
learning technologies; (2) Knowledge representation to engineer and represent
data, information and knowledge using semantic technologies; and (3) AI-based
solutions that seek to emulate human reasoning and decision-making. The AI-CAM
covers the core capability dimensions (business, data, technology,
organisation, AI skills, risks, and ethical considerations) required at the
five capability maturity levels to achieve optimal use of AI in organisations.


---

**[172. [2411.02263] AI Should Challenge, Not Obey](https://arxiv.org/pdf/2411.02263.pdf)** (2024-11-05)

*Advait Sarkar*

  Let's transform our robot secretaries into Socratic gadflies.


---

**[173. [2408.02811] Development of REGAI: Rubric Enabled Generative Artificial Intelligence](https://arxiv.org/pdf/2408.02811.pdf)** (2024-08-07)

*Zach Johnson, Jeremy Straub*

  This paper presents and evaluates a new retrieval augmented generation (RAG)
and large language model (LLM)-based artificial intelligence (AI) technique:
rubric enabled generative artificial intelligence (REGAI). REGAI uses rubrics,
which can be created manually or automatically by the system, to enhance the
performance of LLMs for evaluation purposes. REGAI improves on the performance
of both classical LLMs and RAG-based LLM techniques. This paper describes
REGAI, presents data regarding its performance and discusses several possible
application areas for the technology.


---

**[174. [2503.06353] The AI Pentad, the CHARME$^{2}$D Model, and an Assessment of
  Current-State AI Regulation](https://arxiv.org/pdf/2503.06353.pdf)** (2025-03-11)

*Di Kevin Gao, Sudip Mittal, Jiming Wu, Hongwei Du, Jingdao Chen, Shahram Rahimi*

  Artificial Intelligence (AI) has made remarkable progress in the past few
years with AI-enabled applications beginning to permeate every aspect of our
society. Despite the widespread consensus on the need to regulate AI, there
remains a lack of a unified approach to framing, developing, and assessing AI
regulations. Many of the existing methods take a value-based approach, for
example, accountability, fairness, free from bias, transparency, and trust.
However, these methods often face challenges at the outset due to disagreements
in academia over the subjective nature of these definitions. This paper aims to
establish a unifying model for AI regulation from the perspective of core AI
components. We first introduce the AI Pentad, which comprises the five
essential components of AI: humans and organizations, algorithms, data,
computing, and energy. We then review AI regulatory enablers, including AI
registration and disclosure, AI monitoring, and AI enforcement mechanisms.
Subsequently, we present the CHARME$^{2}$D Model to explore further the
relationship between the AI Pentad and AI regulatory enablers. Finally, we
apply the CHARME$^{2}$D model to assess AI regulatory efforts in the European
Union (EU), China, the United Arab Emirates (UAE), the United Kingdom (UK), and
the United States (US), highlighting their strengths, weaknesses, and gaps.
This comparative evaluation offers insights for future legislative work in the
AI domain.


---

**[175. [2206.03262] Using sensitive data to prevent discrimination by artificial
  intelligence: Does the GDPR need a new exception?](https://arxiv.org/pdf/2206.03262.pdf)** (2022-11-29)

*Marvin van Bekkum, Frederik Zuiderveen Borgesius*

  Organisations can use artificial intelligence to make decisions about people
for a variety of reasons, for instance, to select the best candidates from many
job applications. However, AI systems can have discriminatory effects when used
for decision-making. To illustrate, an AI system could reject applications of
people with a certain ethnicity, while the organisation did not plan such
ethnicity discrimination. But in Europe, an organisation runs into a problem
when it wants to assess whether its AI system accidentally discriminates based
on ethnicity: the organisation may not know the applicants' ethnicity. In
principle, the GDPR bans the use of certain 'special categories of data'
(sometimes called 'sensitive data'), which include data on ethnicity, religion,
and sexual preference. The proposal for an AI Act of the European Commission
includes a provision that would enable organisations to use special categories
of data for auditing their AI systems. This paper asks whether the GDPR's rules
on special categories of personal data hinder the prevention of AI-driven
discrimination. We argue that the GDPR does prohibit such use of special
category data in many circumstances. We also map out the arguments for and
against creating an exception to the GDPR's ban on using special categories of
personal data, to enable preventing discrimination by AI systems. The paper
discusses European law, but the paper can be relevant outside Europe too, as
many policymakers in the world grapple with the tension between privacy and
non-discrimination policy.


---

**[1. [2306.01084] Exploration on HuBERT with Multiple Resolutions](https://arxiv.org/pdf/2306.01084.pdf)** (2023-06-26)

*Jiatong Shi, Yun Tang, Hirofumi Inaguma, Hongyu GOng, Juan Pino, Shinji Watanabe*

  Hidden-unit BERT (HuBERT) is a widely-used self-supervised learning (SSL)
model in speech processing. However, we argue that its fixed 20ms resolution
for hidden representations would not be optimal for various speech-processing
tasks since their attributes (e.g., speaker characteristics and semantics) are
based on different time scales. To address this limitation, we propose
utilizing HuBERT representations at multiple resolutions for downstream tasks.
We explore two approaches, namely the parallel and hierarchical approaches, for
integrating HuBERT features with different resolutions. Through experiments, we
demonstrate that HuBERT with multiple resolutions outperforms the original
model. This highlights the potential of utilizing multiple resolutions in SSL
models like HuBERT to capture diverse information from speech signals.


---

**[2. [2212.14648] Distant Reading of the German Coalition Deal: Recognizing Policy
  Positions with BERT-based Text Classification](https://arxiv.org/pdf/2212.14648.pdf)** (2023-01-02)

*Michael Zylla, Thomas Haider*

  Automated text analysis has become a widely used tool in political science.
In this research, we use a BERT model trained on German party manifestos to
identify the individual parties' contribution to the coalition agreement of
2021.


---

**[3. [2005.02178] IsoBN: Fine-Tuning BERT with Isotropic Batch Normalization](https://arxiv.org/pdf/2005.02178.pdf)** (2021-02-05)

*Wenxuan Zhou, Bill Yuchen Lin, Xiang Ren*

  Fine-tuning pre-trained language models (PTLMs), such as BERT and its better
variant RoBERTa, has been a common practice for advancing performance in
natural language understanding (NLU) tasks. Recent advance in representation
learning shows that isotropic (i.e., unit-variance and uncorrelated) embeddings
can significantly improve performance on downstream tasks with faster
convergence and better generalization. The isotropy of the pre-trained
embeddings in PTLMs, however, is relatively under-explored. In this paper, we
analyze the isotropy of the pre-trained [CLS] embeddings of PTLMs with
straightforward visualization, and point out two major issues: high variance in
their standard deviation, and high correlation between different dimensions. We
also propose a new network regularization method, isotropic batch normalization
(IsoBN) to address the issues, towards learning more isotropic representations
in fine-tuning by dynamically penalizing dominating principal components. This
simple yet effective fine-tuning method yields about 1.0 absolute increment on
the average of seven NLU tasks.


---

**[4. [2203.12907] Mono vs Multilingual BERT: A Case Study in Hindi and Marathi Named
  Entity Recognition](https://arxiv.org/pdf/2203.12907.pdf)** (2023-02-28)

*Onkar Litake, Maithili Sabane, Parth Patil, Aparna Ranade, Raviraj Joshi*

  Named entity recognition (NER) is the process of recognising and classifying
important information (entities) in text. Proper nouns, such as a person's
name, an organization's name, or a location's name, are examples of entities.
The NER is one of the important modules in applications like human resources,
customer support, search engines, content classification, and academia. In this
work, we consider NER for low-resource Indian languages like Hindi and Marathi.
The transformer-based models have been widely used for NER tasks. We consider
different variations of BERT like base-BERT, RoBERTa, and AlBERT and benchmark
them on publicly available Hindi and Marathi NER datasets. We provide an
exhaustive comparison of different monolingual and multilingual
transformer-based models and establish simple baselines currently missing in
the literature. We show that the monolingual MahaRoBERTa model performs the
best for Marathi NER whereas the multilingual XLM-RoBERTa performs the best for
Hindi NER. We also perform cross-language evaluation and present mixed
observations.


---

**[5. [2402.08236] BERT4FCA: A Method for Bipartite Link Prediction using Formal Concept
  Analysis and BERT](https://arxiv.org/pdf/2402.08236.pdf)** (2024-02-14)

*Siqi Peng, Hongyuan Yang, Akihiro Yamamoto*

  We propose BERT4FCA, a novel method for link prediction in bipartite
networks, using formal concept analysis (FCA) and BERT. Link prediction in
bipartite networks is an important task that can solve various practical
problems like friend recommendation in social networks and co-authorship
prediction in author-paper networks. Recent research has found that in
bipartite networks, maximal bi-cliques provide important information for link
prediction, and they can be extracted by FCA. Some FCA-based bipartite link
prediction methods have achieved good performance. However, we figured out that
their performance could be further improved because these methods did not fully
capture the rich information of the extracted maximal bi-cliques. To address
this limitation, we propose an approach using BERT, which can learn more
information from the maximal bi-cliques extracted by FCA and use them to make
link prediction. We conduct experiments on three real-world bipartite networks
and demonstrate that our method outperforms previous FCA-based methods, and
some classic methods such as matrix-factorization and node2vec.


---

**[6. [2201.07449] TourBERT: A pretrained language model for the tourism industry](https://arxiv.org/pdf/2201.07449.pdf)** (2022-05-20)

*Veronika Arefieva, Roman Egger*

  The Bidirectional Encoder Representations from Transformers (BERT) is
currently one of the most important and state-of-the-art models for natural
language. However, it has also been shown that for domain-specific tasks it is
helpful to pretrain BERT on a domain-specific corpus. In this paper, we present
TourBERT, a pretrained language model for tourism. We describe how TourBERT was
developed and evaluated. The evaluations show that TourBERT is outperforming
BERT in all tourism-specific tasks.


---

**[7. [2209.10482] SMTCE: A Social Media Text Classification Evaluation Benchmark and
  BERTology Models for Vietnamese](https://arxiv.org/pdf/2209.10482.pdf)** (2022-09-22)

*Luan Thanh Nguyen, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen*

  Text classification is a typical natural language processing or computational
linguistics task with various interesting applications. As the number of users
on social media platforms increases, data acceleration promotes emerging
studies on Social Media Text Classification (SMTC) or social media text mining
on these valuable resources. In contrast to English, Vietnamese, one of the
low-resource languages, is still not concentrated on and exploited thoroughly.
Inspired by the success of the GLUE, we introduce the Social Media Text
Classification Evaluation (SMTCE) benchmark, as a collection of datasets and
models across a diverse set of SMTC tasks. With the proposed benchmark, we
implement and analyze the effectiveness of a variety of multilingual BERT-based
models (mBERT, XLM-R, and DistilmBERT) and monolingual BERT-based models
(PhoBERT, viBERT, vELECTRA, and viBERT4news) for tasks in the SMTCE benchmark.
Monolingual models outperform multilingual models and achieve state-of-the-art
results on all text classification tasks. It provides an objective assessment
of multilingual and monolingual BERT-based models on the benchmark, which will
benefit future studies about BERTology in the Vietnamese language.


---

**[8. [2205.05391] Query-Based Keyphrase Extraction from Long Documents](https://arxiv.org/pdf/2205.05391.pdf)** (2022-05-12)

*Martin Docekal, Pavel Smrz*

  Transformer-based architectures in natural language processing force input
size limits that can be problematic when long documents need to be processed.
This paper overcomes this issue for keyphrase extraction by chunking the long
documents while keeping a global context as a query defining the topic for
which relevant keyphrases should be extracted. The developed system employs a
pre-trained BERT model and adapts it to estimate the probability that a given
text span forms a keyphrase. We experimented using various context sizes on two
popular datasets, Inspec and SemEval, and a large novel dataset. The presented
results show that a shorter context with a query overcomes a longer one without
the query on long documents.


---

**[9. [2204.03951] RuBioRoBERTa: a pre-trained biomedical language model for Russian
  language biomedical text mining](https://arxiv.org/pdf/2204.03951.pdf)** (2022-04-11)

*Alexander Yalunin, Alexander Nesterov, Dmitriy Umerenkov*

  This paper presents several BERT-based models for Russian language biomedical
text mining (RuBioBERT, RuBioRoBERTa). The models are pre-trained on a corpus
of freely available texts in the Russian biomedical domain. With this
pre-training, our models demonstrate state-of-the-art results on RuMedBench -
Russian medical language understanding benchmark that covers a diverse set of
tasks, including text classification, question answering, natural language
inference, and named entity recognition.


---

**[10. [2104.09243] BERTi\'c -- The Transformer Language Model for Bosnian, Croatian,
  Montenegrin and Serbian](https://arxiv.org/pdf/2104.09243.pdf)** (2021-04-20)

*Nikola Ljubešić, Davor Lauc*

  In this paper we describe a transformer model pre-trained on 8 billion tokens
of crawled text from the Croatian, Bosnian, Serbian and Montenegrin web
domains. We evaluate the transformer model on the tasks of part-of-speech
tagging, named-entity-recognition, geo-location prediction and commonsense
causal reasoning, showing improvements on all tasks over state-of-the-art
models. For commonsense reasoning evaluation, we introduce COPA-HR -- a
translation of the Choice of Plausible Alternatives (COPA) dataset into
Croatian. The BERTi\'c model is made available for free usage and further
task-specific fine-tuning through HuggingFace.


---

**[11. [2208.01875] Introducing BEREL: BERT Embeddings for Rabbinic-Encoded Language](https://arxiv.org/pdf/2208.01875.pdf)** (2022-08-04)

*Avi Shmidman, Joshua Guedalia, Shaltiel Shmidman, Cheyn Shmuel Shmidman, Eli Handel, Moshe Koppel*

  We present a new pre-trained language model (PLM) for Rabbinic Hebrew, termed
Berel (BERT Embeddings for Rabbinic-Encoded Language). Whilst other PLMs exist
for processing Hebrew texts (e.g., HeBERT, AlephBert), they are all trained on
modern Hebrew texts, which diverges substantially from Rabbinic Hebrew in terms
of its lexicographical, morphological, syntactic and orthographic norms. We
demonstrate the superiority of Berel on Rabbinic texts via a challenge set of
Hebrew homographs. We release the new model and homograph challenge set for
unrestricted use.


---

**[12. [2309.04213] UQ at #SMM4H 2023: ALEX for Public Health Analysis with Social Media](https://arxiv.org/pdf/2309.04213.pdf)** (2023-09-13)

*Yan Jiang, Ruihong Qiu, Yi Zhang, Zi Huang*

  As social media becomes increasingly popular, more and more activities
related to public health emerge. Current techniques for public health analysis
involve popular models such as BERT and large language models (LLMs). However,
the costs of training in-domain LLMs for public health are especially
expensive. Furthermore, such kinds of in-domain datasets from social media are
generally imbalanced. To tackle these challenges, the data imbalance issue can
be overcome by data augmentation and balanced training. Moreover, the ability
of the LLMs can be effectively utilized by prompting the model properly. In
this paper, a novel ALEX framework is proposed to improve the performance of
public health analysis on social media by adopting an LLMs explanation
mechanism. Results show that our ALEX model got the best performance among all
submissions in both Task 2 and Task 4 with a high score in Task 1 in Social
Media Mining for Health 2023 (SMM4H)[1]. Our code has been released at https://
github.com/YanJiangJerry/ALEX.


---

**[13. [2112.10925] DB-BERT: a Database Tuning Tool that "Reads the Manual"](https://arxiv.org/pdf/2112.10925.pdf)** (2021-12-22)

*Immanuel Trummer*

  DB-BERT is a database tuning tool that exploits information gained via
natural language analysis of manuals and other relevant text documents. It uses
text to identify database system parameters to tune as well as recommended
parameter values. DB-BERT applies large, pre-trained language models
(specifically, the BERT model) for text analysis. During an initial training
phase, it fine-tunes model weights in order to translate natural language hints
into recommended settings. At run time, DB-BERT learns to aggregate, adapt, and
prioritize hints to achieve optimal performance for a specific database system
and benchmark. Both phases are iterative and use reinforcement learning to
guide the selection of tuning settings to evaluate (penalizing settings that
the database system rejects while rewarding settings that improve performance).
In our experiments, we leverage hundreds of text documents about database
tuning as input for DB-BERT. We compare DB-BERT against various baselines,
considering different benchmarks (TPC-C and TPC-H), metrics (throughput and run
time), as well as database systems (Postgres and MySQL). In all cases, DB-BERT
finds the best parameter settings among all compared methods. The code of
DB-BERT is available online at https://itrummer.github.io/dbbert/.


---

**[14. [2010.12283] ST-BERT: Cross-modal Language Model Pre-training For End-to-end Spoken
  Language Understanding](https://arxiv.org/pdf/2010.12283.pdf)** (2021-04-13)

*Minjeong Kim, Gyuwan Kim, Sang-Woo Lee, Jung-Woo Ha*

  Language model pre-training has shown promising results in various downstream
tasks. In this context, we introduce a cross-modal pre-trained language model,
called Speech-Text BERT (ST-BERT), to tackle end-to-end spoken language
understanding (E2E SLU) tasks. Taking phoneme posterior and subword-level text
as an input, ST-BERT learns a contextualized cross-modal alignment via our two
proposed pre-training tasks: Cross-modal Masked Language Modeling (CM-MLM) and
Cross-modal Conditioned Language Modeling (CM-CLM). Experimental results on
three benchmarks present that our approach is effective for various SLU
datasets and shows a surprisingly marginal performance degradation even when 1%
of the training data are available. Also, our method shows further SLU
performance gain via domain-adaptive pre-training with domain-specific
speech-text pair data.


---

**[15. [2107.13290] Arabic aspect sentiment polarity classification using BERT](https://arxiv.org/pdf/2107.13290.pdf)** (2023-03-13)

*Mohammed M. Abdelgwad, Taysir Hassan A Soliman, Ahmed I. Taloba*

  Aspect-based sentiment analysis(ABSA) is a textual analysis methodology that
defines the polarity of opinions on certain aspects related to specific
targets. The majority of research on ABSA is in English, with a small amount of
work available in Arabic. Most previous Arabic research has relied on deep
learning models that depend primarily on context-independent word embeddings
(e.g.word2vec), where each word has a fixed representation independent of its
context. This article explores the modeling capabilities of contextual
embeddings from pre-trained language models, such as BERT, and making use of
sentence pair input on Arabic aspect sentiment polarity classification task. In
particular, we develop a simple but effective BERT-based neural baseline to
handle this task. Our BERT architecture with a simple linear classification
layer surpassed the state-of-the-art works, according to the experimental
results on three different Arabic datasets. Achieving an accuracy of 89.51% on
the Arabic hotel reviews dataset, 73% on the Human annotated book reviews
dataset, and 85.73% on the Arabic news dataset.


---

**[16. [1910.12647] HUBERT Untangles BERT to Improve Transfer across NLP Tasks](https://arxiv.org/pdf/1910.12647.pdf)** (2021-04-27)

*Mehrad Moradshahi, Hamid Palangi, Monica S. Lam, Paul Smolensky, Jianfeng Gao*

  We introduce HUBERT which combines the structured-representational power of
Tensor-Product Representations (TPRs) and BERT, a pre-trained bidirectional
Transformer language model. We show that there is shared structure between
different NLP datasets that HUBERT, but not BERT, is able to learn and
leverage. We validate the effectiveness of our model on the GLUE benchmark and
HANS dataset. Our experiment results show that untangling data-specific
semantics from general language structure is key for better transfer among NLP
tasks.


---

**[17. [2101.07343] Automatic punctuation restoration with BERT models](https://arxiv.org/pdf/2101.07343.pdf)** (2021-01-20)

*Attila Nagy, Bence Bial, Judit Ács*

  We present an approach for automatic punctuation restoration with BERT models
for English and Hungarian. For English, we conduct our experiments on Ted
Talks, a commonly used benchmark for punctuation restoration, while for
Hungarian we evaluate our models on the Szeged Treebank dataset. Our best
models achieve a macro-averaged $F_1$-score of 79.8 in English and 82.2 in
Hungarian. Our code is publicly available.


---

**[18. [2205.07180] Learning Lip-Based Audio-Visual Speaker Embeddings with AV-HuBERT](https://arxiv.org/pdf/2205.07180.pdf)** (2022-07-18)

*Bowen Shi, Abdelrahman Mohamed, Wei-Ning Hsu*

  This paper investigates self-supervised pre-training for audio-visual speaker
representation learning where a visual stream showing the speaker's mouth area
is used alongside speech as inputs. Our study focuses on the Audio-Visual
Hidden Unit BERT (AV-HuBERT) approach, a recently developed general-purpose
audio-visual speech pre-training framework. We conducted extensive experiments
probing the effectiveness of pre-training and visual modality. Experimental
results suggest that AV-HuBERT generalizes decently to speaker related
downstream tasks, improving label efficiency by roughly ten fold for both
audio-only and audio-visual speaker verification. We also show that
incorporating visual information, even just the lip area, greatly improves the
performance and noise robustness, reducing EER by 38% in the clean condition
and 75% in noisy conditions.


---

**[19. [2110.09665] Ensemble ALBERT on SQuAD 2.0](https://arxiv.org/pdf/2110.09665.pdf)** (2021-10-20)

*Shilun Li, Renee Li, Veronica Peng*

  Machine question answering is an essential yet challenging task in natural
language processing. Recently, Pre-trained Contextual Embeddings (PCE) models
like Bidirectional Encoder Representations from Transformers (BERT) and A Lite
BERT (ALBERT) have attracted lots of attention due to their great performance
in a wide range of NLP tasks. In our Paper, we utilized the fine-tuned ALBERT
models and implemented combinations of additional layers (e.g. attention layer,
RNN layer) on top of them to improve model performance on Stanford Question
Answering Dataset (SQuAD 2.0). We implemented four different models with
different layers on top of ALBERT-base model, and two other models based on
ALBERT-xlarge and ALBERT-xxlarge. We compared their performance to our baseline
model ALBERT-base-v2 + ALBERT-SQuAD-out with details. Our best-performing
individual model is ALBERT-xxlarge + ALBERT-SQuAD-out, which achieved an F1
score of 88.435 on the dev set. Furthermore, we have implemented three
different ensemble algorithms to boost overall performance. By passing in
several best-performing models' results into our weighted voting ensemble
algorithm, our final result ranks first on the Stanford CS224N Test PCE SQuAD
Leaderboard with F1 = 90.123.


---

**[20. [2106.12978] Unsupervised Topic Segmentation of Meetings with BERT Embeddings](https://arxiv.org/pdf/2106.12978.pdf)** (2021-06-25)

*Alessandro Solbiati, Kevin Heffernan, Georgios Damaskinos, Shivani Poddar, Shubham Modi, Jacques Cali*

  Topic segmentation of meetings is the task of dividing multi-person meeting
transcripts into topic blocks. Supervised approaches to the problem have proven
intractable due to the difficulties in collecting and accurately annotating
large datasets. In this paper we show how previous unsupervised topic
segmentation methods can be improved using pre-trained neural architectures. We
introduce an unsupervised approach based on BERT embeddings that achieves a
15.5% reduction in error rate over existing unsupervised approaches applied to
two popular datasets for meeting transcripts.


---

**[21. [2006.03685] BERT-XML: Large Scale Automated ICD Coding Using BERT Pretraining](https://arxiv.org/pdf/2006.03685.pdf)** (2020-06-09)

*Zachariah Zhang, Jingshu Liu, Narges Razavian*

  Clinical interactions are initially recorded and documented in free text
medical notes. ICD coding is the task of classifying and coding all diagnoses,
symptoms and procedures associated with a patient's visit. The process is often
manual and extremely time-consuming and expensive for hospitals. In this paper,
we propose a machine learning model, BERT-XML, for large scale automated ICD
coding from EHR notes, utilizing recently developed unsupervised pretraining
that have achieved state of the art performance on a variety of NLP tasks. We
train a BERT model from scratch on EHR notes, learning with vocabulary better
suited for EHR tasks and thus outperform off-the-shelf models. We adapt the
BERT architecture for ICD coding with multi-label attention. While other works
focus on small public medical datasets, we have produced the first large scale
ICD-10 classification model using millions of EHR notes to predict thousands of
unique ICD codes.


---

**[22. [2204.06029] L3Cube-MahaNER: A Marathi Named Entity Recognition Dataset and BERT
  models](https://arxiv.org/pdf/2204.06029.pdf)** (2022-04-14)

*Parth Patil, Aparna Ranade, Maithili Sabane, Onkar Litake, Raviraj Joshi*

  Named Entity Recognition (NER) is a basic NLP task and finds major
applications in conversational and search systems. It helps us identify key
entities in a sentence used for the downstream application. NER or similar slot
filling systems for popular languages have been heavily used in commercial
applications. In this work, we focus on Marathi, an Indian language, spoken
prominently by the people of Maharashtra state. Marathi is a low resource
language and still lacks useful NER resources. We present L3Cube-MahaNER, the
first major gold standard named entity recognition dataset in Marathi. We also
describe the manual annotation guidelines followed during the process. In the
end, we benchmark the dataset on different CNN, LSTM, and Transformer based
models like mBERT, XLM-RoBERTa, IndicBERT, MahaBERT, etc. The MahaBERT provides
the best performance among all the models. The data and models are available at
https://github.com/l3cube-pune/MarathiNLP .


---

**[23. [2303.05737] Clinical BERTScore: An Improved Measure of Automatic Speech Recognition
  Performance in Clinical Settings](https://arxiv.org/pdf/2303.05737.pdf)** (2024-03-14)

*Joel Shor, Ruyue Agnes Bi, Subhashini Venugopalan, Steven Ibara, Roman Goldenberg, Ehud Rivlin*

  Automatic Speech Recognition (ASR) in medical contexts has the potential to
save time, cut costs, increase report accuracy, and reduce physician burnout.
However, the healthcare industry has been slower to adopt this technology, in
part due to the importance of avoiding medically-relevant transcription
mistakes. In this work, we present the Clinical BERTScore (CBERTScore), an ASR
metric that penalizes clinically-relevant mistakes more than others. We
demonstrate that this metric more closely aligns with clinician preferences on
medical sentences as compared to other metrics (WER, BLUE, METEOR, etc),
sometimes by wide margins. We collect a benchmark of 18 clinician preferences
on 149 realistic medical sentences called the Clinician Transcript Preference
benchmark (CTP) and make it publicly available for the community to further
develop clinically-aware ASR metrics. To our knowledge, this is the first
public dataset of its kind. We demonstrate that CBERTScore more closely matches
what clinicians prefer.


---

**[24. [2004.11493] UHH-LT at SemEval-2020 Task 12: Fine-Tuning of Pre-Trained Transformer
  Networks for Offensive Language Detection](https://arxiv.org/pdf/2004.11493.pdf)** (2020-06-12)

*Gregor Wiedemann, Seid Muhie Yimam, Chris Biemann*

  Fine-tuning of pre-trained transformer networks such as BERT yield
state-of-the-art results for text classification tasks. Typically, fine-tuning
is performed on task-specific training datasets in a supervised manner. One can
also fine-tune in unsupervised manner beforehand by further pre-training the
masked language modeling (MLM) task. Hereby, in-domain data for unsupervised
MLM resembling the actual classification target dataset allows for domain
adaptation of the model. In this paper, we compare current pre-trained
transformer networks with and without MLM fine-tuning on their performance for
offensive language detection. Our MLM fine-tuned RoBERTa-based classifier
officially ranks 1st in the SemEval 2020 Shared Task~12 for the English
language. Further experiments with the ALBERT model even surpass this result.


---

**[25. [2105.12192] NukeLM: Pre-Trained and Fine-Tuned Language Models for the Nuclear and
  Energy Domains](https://arxiv.org/pdf/2105.12192.pdf)** (2021-05-27)

*Lee Burke, Karl Pazdernik, Daniel Fortin, Benjamin Wilson, Rustam Goychayev, John Mattingly*

  Natural language processing (NLP) tasks (text classification, named entity
recognition, etc.) have seen revolutionary improvements over the last few
years. This is due to language models such as BERT that achieve deep knowledge
transfer by using a large pre-trained model, then fine-tuning the model on
specific tasks. The BERT architecture has shown even better performance on
domain-specific tasks when the model is pre-trained using domain-relevant
texts. Inspired by these recent advancements, we have developed NukeLM, a
nuclear-domain language model pre-trained on 1.5 million abstracts from the
U.S. Department of Energy Office of Scientific and Technical Information (OSTI)
database. This NukeLM model is then fine-tuned for the classification of
research articles into either binary classes (related to the nuclear fuel cycle
[NFC] or not) or multiple categories related to the subject of the article. We
show that continued pre-training of a BERT-style architecture prior to
fine-tuning yields greater performance on both article classification tasks.
This information is critical for properly triaging manuscripts, a necessary
task for better understanding citation networks that publish in the nuclear
space, and for uncovering new areas of research in the nuclear (or
nuclear-relevant) domains.


---

**[26. [2006.11512] Sarcasm Detection in Tweets with BERT and GloVe Embeddings](https://arxiv.org/pdf/2006.11512.pdf)** (2024-09-05)

*Akshay Khatri, Pranav P, Anand Kumar M*

  Sarcasm is a form of communication in whichthe person states opposite of what
he actually means. It is ambiguous in nature. In this paper, we propose using
machine learning techniques with BERT and GloVe embeddings to detect sarcasm in
tweets. The dataset is preprocessed before extracting the embeddings. The
proposed model also uses the context in which the user is reacting to along
with his actual response.


---

**[27. [2108.04927] Embodied BERT: A Transformer Model for Embodied, Language-guided Visual
  Task Completion](https://arxiv.org/pdf/2108.04927.pdf)** (2021-11-05)

*Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, Gaurav Sukhatme*

  Language-guided robots performing home and office tasks must navigate in and
interact with the world. Grounding language instructions against visual
observations and actions to take in an environment is an open challenge. We
present Embodied BERT (EmBERT), a transformer-based model which can attend to
high-dimensional, multi-modal inputs across long temporal horizons for
language-conditioned task completion. Additionally, we bridge the gap between
successful object-centric navigation models used for non-interactive agents and
the language-guided visual task completion benchmark, ALFRED, by introducing
object navigation targets for EmBERT training. We achieve competitive
performance on the ALFRED benchmark, and EmBERT marks the first
transformer-based model to successfully handle the long-horizon, dense,
multi-modal histories of ALFRED, and the first ALFRED model to utilize
object-centric navigation targets.


---

**[28. [2012.04539] Dartmouth CS at WNUT-2020 Task 2: Informative COVID-19 Tweet
  Classification Using BERT](https://arxiv.org/pdf/2012.04539.pdf)** (2020-12-09)

*Dylan Whang, Soroush Vosoughi*

  We describe the systems developed for the WNUT-2020 shared task 2,
identification of informative COVID-19 English Tweets. BERT is a highly
performant model for Natural Language Processing tasks. We increased BERT's
performance in this classification task by fine-tuning BERT and concatenating
its embeddings with Tweet-specific features and training a Support Vector
Machine (SVM) for classification (henceforth called BERT+). We compared its
performance to a suite of machine learning models. We used a Twitter specific
data cleaning pipeline and word-level TF-IDF to extract features for the
non-BERT models. BERT+ was the top performing model with an F1-score of 0.8713.


---

**[29. [2208.02070] Efficient Fine-Tuning of Compressed Language Models with Learners](https://arxiv.org/pdf/2208.02070.pdf)** (2022-08-04)

*Danilo Vucetic, Mohammadreza Tayaranian, Maryam Ziaeefard, James J. Clark, Brett H. Meyer, Warren J. Gross*

  Fine-tuning BERT-based models is resource-intensive in memory, computation,
and time. While many prior works aim to improve inference efficiency via
compression techniques, e.g., pruning, these works do not explicitly address
the computational challenges of training to downstream tasks. We introduce
Learner modules and priming, novel methods for fine-tuning that exploit the
overparameterization of pre-trained language models to gain benefits in
convergence speed and resource utilization. Learner modules navigate the double
bind of 1) training efficiently by fine-tuning a subset of parameters, and 2)
training effectively by ensuring quick convergence and high metric scores. Our
results on DistilBERT demonstrate that learners perform on par with or surpass
the baselines. Learners train 7x fewer parameters than state-of-the-art methods
on GLUE. On CoLA, learners fine-tune 20% faster, and have significantly lower
resource utilization.


---

**[30. [2205.03695] AKI-BERT: a Pre-trained Clinical Language Model for Early Prediction of
  Acute Kidney Injury](https://arxiv.org/pdf/2205.03695.pdf)** (2022-05-10)

*Chengsheng Mao, Liang Yao, Yuan Luo*

  Acute kidney injury (AKI) is a common clinical syndrome characterized by a
sudden episode of kidney failure or kidney damage within a few hours or a few
days. Accurate early prediction of AKI for patients in ICU who are more likely
than others to have AKI can enable timely interventions, and reduce the
complications of AKI. Much of the clinical information relevant to AKI is
captured in clinical notes that are largely unstructured text and requires
advanced natural language processing (NLP) for useful information extraction.
On the other hand, pre-trained contextual language models such as Bidirectional
Encoder Representations from Transformers (BERT) have improved performances for
many NLP tasks in general domain recently. However, few have explored BERT on
disease-specific medical domain tasks such as AKI early prediction. In this
paper, we try to apply BERT to specific diseases and present an AKI
domain-specific pre-trained language model based on BERT (AKI-BERT) that could
be used to mine the clinical notes for early prediction of AKI. AKI-BERT is a
BERT model pre-trained on the clinical notes of patients having risks for AKI.
Our experiments on Medical Information Mart for Intensive Care III (MIMIC-III)
dataset demonstrate that AKI-BERT can yield performance improvements for early
AKI prediction, thus expanding the utility of the BERT model from general
clinical domain to disease-specific domain.


---

**[31. [2211.06874] Xu at SemEval-2022 Task 4: Pre-BERT Neural Network Methods vs Post-BERT
  RoBERTa Approach for Patronizing and Condescending Language Detection](https://arxiv.org/pdf/2211.06874.pdf)** (2022-11-15)

*Jinghua Xu*

  This paper describes my participation in the SemEval-2022 Task 4: Patronizing
and Condescending Language Detection. I participate in both subtasks:
Patronizing and Condescending Language (PCL) Identification and Patronizing and
Condescending Language Categorization, with the main focus put on subtask 1.
The experiments compare pre-BERT neural network (NN) based systems against
post-BERT pretrained language model RoBERTa. This research finds NN-based
systems in the experiments perform worse on the task compared to the pretrained
language models. The top-performing RoBERTa system is ranked 26 out of 78 teams
(F1-score: 54.64) in subtask 1, and 23 out of 49 teams (F1-score: 30.03) in
subtask 2.


---

**[32. [2303.17367] A BERT-based Unsupervised Grammatical Error Correction Framework](https://arxiv.org/pdf/2303.17367.pdf)** (2023-03-31)

*Nankai Lin, Hongbin Zhang, Menglan Shen, Yu Wang, Shengyi Jiang, Aimin Yang*

  Grammatical error correction (GEC) is a challenging task of natural language
processing techniques. While more attempts are being made in this approach for
universal languages like English or Chinese, relatively little work has been
done for low-resource languages for the lack of large annotated corpora. In
low-resource languages, the current unsupervised GEC based on language model
scoring performs well. However, the pre-trained language model is still to be
explored in this context. This study proposes a BERT-based unsupervised GEC
framework, where GEC is viewed as multi-class classification task. The
framework contains three modules: data flow construction module, sentence
perplexity scoring module, and error detecting and correcting module. We
propose a novel scoring method for pseudo-perplexity to evaluate a sentence's
probable correctness and construct a Tagalog corpus for Tagalog GEC research.
It obtains competitive performance on the Tagalog corpus we construct and
open-source Indonesian corpus and it demonstrates that our framework is
complementary to baseline method for low-resource GEC task.


---

**[33. [2211.17201] ExtremeBERT: A Toolkit for Accelerating Pretraining of Customized BERT](https://arxiv.org/pdf/2211.17201.pdf)** (2022-12-01)

*Rui Pan, Shizhe Diao, Jianlin Chen, Tong Zhang*

  In this paper, we present ExtremeBERT, a toolkit for accelerating and
customizing BERT pretraining. Our goal is to provide an easy-to-use BERT
pretraining toolkit for the research community and industry. Thus, the
pretraining of popular language models on customized datasets is affordable
with limited resources. Experiments show that, to achieve the same or better
GLUE scores, the time cost of our toolkit is over $6\times$ times less for BERT
Base and $9\times$ times less for BERT Large when compared with the original
BERT paper. The documentation and code are released at
https://github.com/extreme-bert/extreme-bert under the Apache-2.0 license.


---

**[34. [2208.02140] KPI-BERT: A Joint Named Entity Recognition and Relation Extraction Model
  for Financial Reports](https://arxiv.org/pdf/2208.02140.pdf)** (2022-08-04)

*Lars Hillebrand, Tobias Deußer, Tim Dilmaghani, Bernd Kliem, Rüdiger Loitz, Christian Bauckhage, Rafet Sifa*

  We present KPI-BERT, a system which employs novel methods of named entity
recognition (NER) and relation extraction (RE) to extract and link key
performance indicators (KPIs), e.g. "revenue" or "interest expenses", of
companies from real-world German financial documents. Specifically, we
introduce an end-to-end trainable architecture that is based on Bidirectional
Encoder Representations from Transformers (BERT) combining a recurrent neural
network (RNN) with conditional label masking to sequentially tag entities
before it classifies their relations. Our model also introduces a learnable
RNN-based pooling mechanism and incorporates domain expert knowledge by
explicitly filtering impossible relations. We achieve a substantially higher
prediction performance on a new practical dataset of German financial reports,
outperforming several strong baselines including a competing state-of-the-art
span-based entity tagging approach.


---

**[35. [2108.01589] ExBERT: An External Knowledge Enhanced BERT for Natural Language
  Inference](https://arxiv.org/pdf/2108.01589.pdf)** (2021-08-04)

*Amit Gajbhiye, Noura Al Moubayed, Steven Bradley*

  Neural language representation models such as BERT, pre-trained on
large-scale unstructured corpora lack explicit grounding to real-world
commonsense knowledge and are often unable to remember facts required for
reasoning and inference. Natural Language Inference (NLI) is a challenging
reasoning task that relies on common human understanding of language and
real-world commonsense knowledge. We introduce a new model for NLI called
External Knowledge Enhanced BERT (ExBERT), to enrich the contextual
representation with real-world commonsense knowledge from external knowledge
sources and enhance BERT's language understanding and reasoning capabilities.
ExBERT takes full advantage of contextual word representations obtained from
BERT and employs them to retrieve relevant external knowledge from knowledge
graphs and to encode the retrieved external knowledge. Our model adaptively
incorporates the external knowledge context required for reasoning over the
inputs. Extensive experiments on the challenging SciTail and SNLI benchmarks
demonstrate the effectiveness of ExBERT: in comparison to the previous
state-of-the-art, we obtain an accuracy of 95.9% on SciTail and 91.5% on SNLI.


---

**[36. [2311.10770] Exponentially Faster Language Modelling](https://arxiv.org/pdf/2311.10770.pdf)** (2023-11-22)

*Peter Belcak, Roger Wattenhofer*

  Language models only really need to use an exponential fraction of their
neurons for individual inferences. As proof, we present UltraFastBERT, a BERT
variant that uses 0.3% of its neurons during inference while performing on par
with similar BERT models. UltraFastBERT selectively engages just 12 out of 4095
neurons for each layer inference. This is achieved by replacing feedforward
networks with fast feedforward networks (FFFs). While no truly efficient
implementation currently exists to unlock the full acceleration potential of
conditional neural execution, we provide high-level CPU code achieving 78x
speedup over the optimized baseline feedforward implementation, and a PyTorch
implementation delivering 40x speedup over the equivalent batched feedforward
inference. We publish our training code, benchmarking setup, and model weights.


---

**[37. [2207.14043] Trace Refinement in B and Event-B](https://arxiv.org/pdf/2207.14043.pdf)** (2022-07-29)

*Sebastian Stock, Atif Mashkoor, Michael Leuschel, Alexander Egyed*

  Traces are used to show whether a model complies with the intended behavior.
A modeler can use trace checking to ensure the preservation of the model
behavior during the refinement process. In this paper, we present a trace
refinement technique and tool called BERT that allows designers to ensure the
behavioral integrity of high-level traces at the concrete level. The proposed
technique is evaluated within the context of the B and Event-B methods on
industrial-strength case studies from the automotive domain.


---

**[38. [2401.07944] SemEval-2017 Task 4: Sentiment Analysis in Twitter using BERT](https://arxiv.org/pdf/2401.07944.pdf)** (2024-08-31)

*Rupak Kumar Das, Ted Pedersen*

  This paper uses the BERT model, which is a transformer-based architecture, to
solve task 4A, English Language, Sentiment Analysis in Twitter of SemEval2017.
BERT is a very powerful large language model for classification tasks when the
amount of training data is small. For this experiment, we have used the
BERT(BASE) model, which has 12 hidden layers. This model provides better
accuracy, precision, recall, and f1 score than the Naive Bayes baseline model.
It performs better in binary classification subtasks than the multi-class
classification subtasks. We also considered all kinds of ethical issues during
this experiment, as Twitter data contains personal and sensible information.
The dataset and code used in our experiment can be found in this GitHub
repository.


---

**[39. [2204.06328] HuBERT-EE: Early Exiting HuBERT for Efficient Speech Recognition](https://arxiv.org/pdf/2204.06328.pdf)** (2024-06-21)

*Ji Won Yoon, Beom Jun Woo, Nam Soo Kim*

  Pre-training with self-supervised models, such as Hidden-unit BERT (HuBERT)
and wav2vec 2.0, has brought significant improvements in automatic speech
recognition (ASR). However, these models usually require an expensive
computational cost to achieve outstanding performance, slowing down the
inference speed. To improve the model efficiency, we introduce an early exit
scheme for ASR, namely HuBERT-EE, that allows the model to stop the inference
dynamically. In HuBERT-EE, multiple early exit branches are added at the
intermediate layers. When the intermediate prediction of the early exit branch
is confident, the model stops the inference, and the corresponding result can
be returned early. We investigate the proper early exiting criterion and
fine-tuning strategy to effectively perform early exiting. Experimental results
on the LibriSpeech show that HuBERT-EE can accelerate the inference of the
HuBERT while simultaneously balancing the trade-off between the performance and
the latency.


---

**[40. [2404.10097] LegalPro-BERT: Classification of Legal Provisions by fine-tuning BERT
  Large Language Model](https://arxiv.org/pdf/2404.10097.pdf)** (2024-04-18)

*Amit Tewari*

  A contract is a type of legal document commonly used in organizations.
Contract review is an integral and repetitive process to avoid business risk
and liability. Contract analysis requires the identification and classification
of key provisions and paragraphs within an agreement. Identification and
validation of contract clauses can be a time-consuming and challenging task
demanding the services of trained and expensive lawyers, paralegals or other
legal assistants. Classification of legal provisions in contracts using
artificial intelligence and natural language processing is complex due to the
requirement of domain-specialized legal language for model training and the
scarcity of sufficient labeled data in the legal domain. Using general-purpose
models is not effective in this context due to the use of specialized legal
vocabulary in contracts which may not be recognized by a general model. To
address this problem, we propose the use of a pre-trained large language model
which is subsequently calibrated on legal taxonomy. We propose LegalPro-BERT, a
BERT transformer architecture model that we fine-tune to efficiently handle
classification task for legal provisions. We conducted experiments to measure
and compare metrics with current benchmark results. We found that LegalPro-BERT
outperforms the previous benchmark used for comparison in this research.


---

**[41. [2305.09098] Weight-Inherited Distillation for Task-Agnostic BERT Compression](https://arxiv.org/pdf/2305.09098.pdf)** (2024-03-21)

*Taiqiang Wu, Cheng Hou, Shanshan Lao, Jiayi Li, Ngai Wong, Zhe Zhao, Yujiu Yang*

  Knowledge Distillation (KD) is a predominant approach for BERT compression.
Previous KD-based methods focus on designing extra alignment losses for the
student model to mimic the behavior of the teacher model. These methods
transfer the knowledge in an indirect way. In this paper, we propose a novel
Weight-Inherited Distillation (WID), which directly transfers knowledge from
the teacher. WID does not require any additional alignment loss and trains a
compact student by inheriting the weights, showing a new perspective of
knowledge distillation. Specifically, we design the row compactors and column
compactors as mappings and then compress the weights via structural
re-parameterization. Experimental results on the GLUE and SQuAD benchmarks show
that WID outperforms previous state-of-the-art KD-based baselines. Further
analysis indicates that WID can also learn the attention patterns from the
teacher model without any alignment loss on attention distributions. The code
is available at https://github.com/wutaiqiang/WID-NAACL2024.


---

**[42. [2205.15485] FinBERT-MRC: financial named entity recognition using BERT under the
  machine reading comprehension paradigm](https://arxiv.org/pdf/2205.15485.pdf)** (2022-06-01)

*Yuzhe Zhang, Hong Zhang*

  Financial named entity recognition (FinNER) from literature is a challenging
task in the field of financial text information extraction, which aims to
extract a large amount of financial knowledge from unstructured texts. It is
widely accepted to use sequence tagging frameworks to implement FinNER tasks.
However, such sequence tagging models cannot fully take advantage of the
semantic information in the texts. Instead, we formulate the FinNER task as a
machine reading comprehension (MRC) problem and propose a new model termed
FinBERT-MRC. This formulation introduces significant prior information by
utilizing well-designed queries, and extracts start index and end index of
target entities without decoding modules such as conditional random fields
(CRF). We conduct experiments on a publicly available Chinese financial dataset
ChFinAnn and a real-word bussiness dataset AdminPunish. FinBERT-MRC model
achieves average F1 scores of 92.78% and 96.80% on the two datasets,
respectively, with average F1 gains +3.94% and +0.89% over some sequence
tagging models including BiLSTM-CRF, BERT-Tagger, and BERT-CRF. The source code
is available at https://github.com/zyz0000/FinBERT-MRC.


---

**[43. [2305.14907] Coverage-based Example Selection for In-Context Learning](https://arxiv.org/pdf/2305.14907.pdf)** (2023-11-08)

*Shivanshu Gupta, Matt Gardner, Sameer Singh*

  In-context learning (ICL), the ability of large language models to perform
novel tasks by conditioning on a prompt with a few task examples, requires
these examples to be informative about the test instance. The standard approach
of independently ranking and selecting the most similar examples selects
redundant examples while omitting important information. In this work, we show
that BERTScore-Recall (BSR) selects better examples that demonstrate more of
the salient aspects, e.g. reasoning patterns, of the test input. We further
extend BSR and many standard metrics to easily optimizable set-level metrics,
giving still better coverage of those salient aspects. On 15 datasets spanning
6 tasks and with 7 diverse LLMs, we show that (1) BSR is the superior metric
for in-context example selection across the board, and (2) for compositional
tasks, set selection using Set-BSR outperforms independent ranking by up to 17
points on average and, despite being training-free, surpasses methods that
leverage task or LLM-specific training.


---

**[44. [2209.09815] Towards Fine-tuning Pre-trained Language Models with Integer Forward and
  Backward Propagation](https://arxiv.org/pdf/2209.09815.pdf)** (2023-02-14)

*Mohammadreza Tayaranian, Alireza Ghaffari, Marzieh S. Tahaei, Mehdi Rezagholizadeh, Masoud Asgharian, Vahid Partovi Nia*

  The large number of parameters of some prominent language models, such as
BERT, makes their fine-tuning on downstream tasks computationally intensive and
energy hungry. Previously researchers were focused on lower bit-width integer
data types for the forward propagation of language models to save memory and
computation. As for the backward propagation, however, only 16-bit
floating-point data type has been used for the fine-tuning of BERT. In this
work, we use integer arithmetic for both forward and back propagation in the
fine-tuning of BERT. We study the effects of varying the integer bit-width on
the model's metric performance. Our integer fine-tuning uses integer arithmetic
to perform forward propagation and gradient computation of linear, layer-norm,
and embedding layers of BERT. We fine-tune BERT using our integer training
method on SQuAD v1.1 and SQuAD v2., and GLUE benchmark. We demonstrate that
metric performance of fine-tuning 16-bit integer BERT matches both 16-bit and
32-bit floating-point baselines. Furthermore, using the faster and more memory
efficient 8-bit integer data type, integer fine-tuning of BERT loses an average
of 3.1 points compared to the FP32 baseline.


---

**[45. [2409.09143] DomURLs_BERT: Pre-trained BERT-based Model for Malicious Domains and
  URLs Detection and Classification](https://arxiv.org/pdf/2409.09143.pdf)** (2024-09-17)

*Abdelkader El Mahdaouy, Salima Lamsiyah, Meryem Janati Idrissi, Hamza Alami, Zakaria Yartaoui, Ismail Berrada*

  Detecting and classifying suspicious or malicious domain names and URLs is
fundamental task in cybersecurity. To leverage such indicators of compromise,
cybersecurity vendors and practitioners often maintain and update blacklists of
known malicious domains and URLs. However, blacklists frequently fail to
identify emerging and obfuscated threats. Over the past few decades, there has
been significant interest in developing machine learning models that
automatically detect malicious domains and URLs, addressing the limitations of
blacklists maintenance and updates. In this paper, we introduce DomURLs_BERT, a
pre-trained BERT-based encoder adapted for detecting and classifying
suspicious/malicious domains and URLs. DomURLs_BERT is pre-trained using the
Masked Language Modeling (MLM) objective on a large multilingual corpus of
URLs, domain names, and Domain Generation Algorithms (DGA) dataset. In order to
assess the performance of DomURLs_BERT, we have conducted experiments on
several binary and multi-class classification tasks involving domain names and
URLs, covering phishing, malware, DGA, and DNS tunneling. The evaluations
results show that the proposed encoder outperforms state-of-the-art
character-based deep learning models and cybersecurity-focused BERT models
across multiple tasks and datasets. The pre-training dataset, the pre-trained
DomURLs_BERT encoder, and the experiments source code are publicly available.


---

**[46. [2405.02573] A Combination of BERT and Transformer for Vietnamese Spelling Correction](https://arxiv.org/pdf/2405.02573.pdf)** (2024-05-07)

*Hieu Ngo Trung, Duong Tran Ham, Tin Huynh, Kiem Hoang*

  Recently, many studies have shown the efficiency of using Bidirectional
Encoder Representations from Transformers (BERT) in various Natural Language
Processing (NLP) tasks. Specifically, English spelling correction task that
uses Encoder-Decoder architecture and takes advantage of BERT has achieved
state-of-the-art result. However, to our knowledge, there is no implementation
in Vietnamese yet. Therefore, in this study, a combination of Transformer
architecture (state-of-the-art for Encoder-Decoder model) and BERT was proposed
to deal with Vietnamese spelling correction. The experiment results have shown
that our model outperforms other approaches as well as the Google Docs Spell
Checking tool, achieves an 86.24 BLEU score on this task.


---

**[47. [2402.05130] LB-KBQA: Large-language-model and BERT based Knowledge-Based Question
  and Answering System](https://arxiv.org/pdf/2402.05130.pdf)** (2024-02-12)

*Yan Zhao, Zhongyun Li, Yushan Pan, Jiaxing Wang, Yihong Wang*

  Generative Artificial Intelligence (AI), because of its emergent abilities,
has empowered various fields, one typical of which is large language models
(LLMs). One of the typical application fields of Generative AI is large
language models (LLMs), and the natural language understanding capability of
LLM is dramatically improved when compared with conventional AI-based methods.
The natural language understanding capability has always been a barrier to the
intent recognition performance of the Knowledge-Based-Question-and-Answer
(KBQA) system, which arises from linguistic diversity and the newly appeared
intent. Conventional AI-based methods for intent recognition can be divided
into semantic parsing-based and model-based approaches. However, both of the
methods suffer from limited resources in intent recognition. To address this
issue, we propose a novel KBQA system based on a Large Language Model(LLM) and
BERT (LB-KBQA). With the help of generative AI, our proposed method could
detect newly appeared intent and acquire new knowledge. In experiments on
financial domain question answering, our model has demonstrated superior
effectiveness.


---

**[48. [2006.05744] MC-BERT: Efficient Language Pre-Training via a Meta Controller](https://arxiv.org/pdf/2006.05744.pdf)** (2020-06-17)

*Zhenhui Xu, Linyuan Gong, Guolin Ke, Di He, Shuxin Zheng, Liwei Wang, Jiang Bian, Tie-Yan Liu*

  Pre-trained contextual representations (e.g., BERT) have become the
foundation to achieve state-of-the-art results on many NLP tasks. However,
large-scale pre-training is computationally expensive. ELECTRA, an early
attempt to accelerate pre-training, trains a discriminative model that predicts
whether each input token was replaced by a generator. Our studies reveal that
ELECTRA's success is mainly due to its reduced complexity of the pre-training
task: the binary classification (replaced token detection) is more efficient to
learn than the generation task (masked language modeling). However, such a
simplified task is less semantically informative. To achieve better efficiency
and effectiveness, we propose a novel meta-learning framework, MC-BERT. The
pre-training task is a multi-choice cloze test with a reject option, where a
meta controller network provides training input and candidates. Results over
GLUE natural language understanding benchmark demonstrate that our proposed
method is both efficient and effective: it outperforms baselines on GLUE
semantic tasks given the same computational budget.


---

**[49. [2311.04799] DACBERT: Leveraging Dependency Agreement for Cost-Efficient Bert
  Pretraining](https://arxiv.org/pdf/2311.04799.pdf)** (2023-11-09)

*Martin Kuo, Jianyi Zhang, Yiran Chen*

  Building on the cost-efficient pretraining advancements brought about by
Crammed BERT, we enhance its performance and interpretability further by
introducing a novel pretrained model Dependency Agreement Crammed BERT
(DACBERT) and its two-stage pretraining framework - Dependency Agreement
Pretraining. This framework, grounded by linguistic theories, seamlessly weaves
syntax and semantic information into the pretraining process. The first stage
employs four dedicated submodels to capture representative dependency
agreements at the chunk level, effectively converting these agreements into
embeddings. The second stage uses these refined embeddings, in tandem with
conventional BERT embeddings, to guide the pretraining of the rest of the
model. Evaluated on the GLUE benchmark, our DACBERT demonstrates notable
improvement across various tasks, surpassing Crammed BERT by 3.13% in the RTE
task and by 2.26% in the MRPC task. Furthermore, our method boosts the average
GLUE score by 0.83%, underscoring its significant potential. The pretraining
process can be efficiently executed on a single GPU within a 24-hour cycle,
necessitating no supplementary computational resources or extending the
pretraining duration compared with the Crammed BERT. Extensive studies further
illuminate our approach's instrumental role in bolstering the interpretability
of pretrained language models for natural language understanding tasks.


---

**[50. [2411.05930] BERTrend: Neural Topic Modeling for Emerging Trends Detection](https://arxiv.org/pdf/2411.05930.pdf)** (2024-11-22)

*Allaa Boutaleb, Jerome Picault, Guillaume Grosjean*

  Detecting and tracking emerging trends and weak signals in large, evolving
text corpora is vital for applications such as monitoring scientific
literature, managing brand reputation, surveilling critical infrastructure and
more generally to any kind of text-based event detection. Existing solutions
often fail to capture the nuanced context or dynamically track evolving
patterns over time. BERTrend, a novel method, addresses these limitations using
neural topic modeling in an online setting. It introduces a new metric to
quantify topic popularity over time by considering both the number of documents
and update frequency. This metric classifies topics as noise, weak, or strong
signals, flagging emerging, rapidly growing topics for further investigation.
Experimentation on two large real-world datasets demonstrates BERTrend's
ability to accurately detect and track meaningful weak signals while filtering
out noise, offering a comprehensive solution for monitoring emerging trends in
large-scale, evolving text corpora. The method can also be used for
retrospective analysis of past events. In addition, the use of Large Language
Models together with BERTrend offers efficient means for the interpretability
of trends of events.


---

**[51. [2011.12073] Picking BERT's Brain: Probing for Linguistic Dependencies in
  Contextualized Embeddings Using Representational Similarity Analysis](https://arxiv.org/pdf/2011.12073.pdf)** (2020-11-25)

*Michael A. Lepori, R. Thomas McCoy*

  As the name implies, contextualized representations of language are typically
motivated by their ability to encode context. Which aspects of context are
captured by such representations? We introduce an approach to address this
question using Representational Similarity Analysis (RSA). As case studies, we
investigate the degree to which a verb embedding encodes the verb's subject, a
pronoun embedding encodes the pronoun's antecedent, and a full-sentence
representation encodes the sentence's head word (as determined by a dependency
parse). In all cases, we show that BERT's contextualized embeddings reflect the
linguistic dependency being studied, and that BERT encodes these dependencies
to a greater degree than it encodes less linguistically-salient controls. These
results demonstrate the ability of our approach to adjudicate between
hypotheses about which aspects of context are encoded in representations of
language.


---

**[52. [2502.19115] Improving Customer Service with Automatic Topic Detection in User Emails](https://arxiv.org/pdf/2502.19115.pdf)** (2025-04-08)

*Bojana Bašaragin, Darija Medvecki, Gorana Gojić, Milena Oparnica, Dragiša Mišković*

  This study introduces a novel natural language processing pipeline that
enhances customer service efficiency at Telekom Srbija, a leading Serbian
telecommunications company, through automated email topic detection and
labeling. Central to the pipeline is BERTopic, a modular framework that allows
unsupervised topic modeling. After a series of preprocessing and postprocessing
steps, we assign one of 12 topics and several additional labels to incoming
emails, allowing the customer service to filter and access them through a
custom-made application. The model's performance was evaluated by assessing the
speed and correctness of the automatically assigned topics, with a weighted
average processing time of 0.041 seconds per email and a weighted average F1
score of 0.96. The pipeline shows broad applicability across languages,
particularly to those that are low-resourced and morphologically rich. The
system now operates in the company's production environment, streamlining
customer service operations through automated email classification.


---

**[53. [2302.07189] Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity
  Linking](https://arxiv.org/pdf/2302.07189.pdf)** (2023-09-06)

*Hang Dong, Jiaoyan Chen, Yuan He, Yinan Liu, Ian Horrocks*

  Discovering entity mentions that are out of a Knowledge Base (KB) from texts
plays a critical role in KB maintenance, but has not yet been fully explored.
The current methods are mostly limited to the simple threshold-based approach
and feature-based classification, and the datasets for evaluation are
relatively rare. We propose BLINKout, a new BERT-based Entity Linking (EL)
method which can identify mentions that do not have corresponding KB entities
by matching them to a special NIL entity. To better utilize BERT, we propose
new techniques including NIL entity representation and classification, with
synonym enhancement. We also apply KB Pruning and Versioning strategies to
automatically construct out-of-KB datasets from common in-KB EL datasets.
Results on five datasets of clinical notes, biomedical publications, and
Wikipedia articles in various domains show the advantages of BLINKout over
existing methods to identify out-of-KB mentions for the medical ontologies,
UMLS, SNOMED CT, and the general KB, WikiData.


---

**[54. [2101.11363] KoreALBERT: Pretraining a Lite BERT Model for Korean Language
  Understanding](https://arxiv.org/pdf/2101.11363.pdf)** (2021-01-28)

*Hyunjae Lee, Jaewoong Yoon, Bonggyu Hwang, Seongho Joe, Seungjai Min, Youngjune Gwon*

  A Lite BERT (ALBERT) has been introduced to scale up deep bidirectional
representation learning for natural languages. Due to the lack of pretrained
ALBERT models for Korean language, the best available practice is the
multilingual model or resorting back to the any other BERT-based model. In this
paper, we develop and pretrain KoreALBERT, a monolingual ALBERT model
specifically for Korean language understanding. We introduce a new training
objective, namely Word Order Prediction (WOP), and use alongside the existing
MLM and SOP criteria to the same architecture and model parameters. Despite
having significantly fewer model parameters (thus, quicker to train), our
pretrained KoreALBERT outperforms its BERT counterpart on 6 different NLU
tasks. Consistent with the empirical results in English by Lan et al.,
KoreALBERT seems to improve downstream task performance involving
multi-sentence encoding for Korean language. The pretrained KoreALBERT is
publicly available to encourage research and application development for Korean
NLP.


---

**[55. [2501.08758] Expanding Vietnamese SentiWordNet to Improve Performance of Vietnamese
  Sentiment Analysis Models](https://arxiv.org/pdf/2501.08758.pdf)** (2025-01-16)

*Hong-Viet Tran, Van-Tan Bui, Lam-Quan Tran*

  Sentiment analysis is one of the most crucial tasks in Natural Language
Processing (NLP), involving the training of machine learning models to classify
text based on the polarity of opinions. Pre-trained Language Models (PLMs) can
be applied to downstream tasks through fine-tuning, eliminating the need to
train the model from scratch. Specifically, PLMs have been employed for
Sentiment Analysis, a process that involves detecting, analyzing, and
extracting the polarity of text sentiments. Numerous models have been proposed
to address this task, with pre-trained PhoBERT-V2 models standing out as the
state-of-the-art language models for Vietnamese. The PhoBERT-V2 pre-training
approach is based on RoBERTa, optimizing the BERT pre-training method for more
robust performance. In this paper, we introduce a novel approach that combines
PhoBERT-V2 and SentiWordnet for Sentiment Analysis of Vietnamese reviews. Our
proposed model utilizes PhoBERT-V2 for Vietnamese, offering a robust
optimization for the prominent BERT model in the context of Vietnamese
language, and leverages SentiWordNet, a lexical resource explicitly designed to
support sentiment classification applications. Experimental results on the VLSP
2016 and AIVIVN 2019 datasets demonstrate that our sentiment analysis system
has achieved excellent performance in comparison to other models.


---

**[56. [2203.16369] Incorporating Dynamic Semantics into Pre-Trained Language Model for
  Aspect-based Sentiment Analysis](https://arxiv.org/pdf/2203.16369.pdf)** (2022-11-24)

*Kai Zhang, Kun Zhang, Mengdi Zhang, Hongke Zhao, Qi Liu, Wei Wu, Enhong Chen*

  Aspect-based sentiment analysis (ABSA) predicts sentiment polarity towards a
specific aspect in the given sentence. While pre-trained language models such
as BERT have achieved great success, incorporating dynamic semantic changes
into ABSA remains challenging. To this end, in this paper, we propose to
address this problem by Dynamic Re-weighting BERT (DR-BERT), a novel method
designed to learn dynamic aspect-oriented semantics for ABSA. Specifically, we
first take the Stack-BERT layers as a primary encoder to grasp the overall
semantic of the sentence and then fine-tune it by incorporating a lightweight
Dynamic Re-weighting Adapter (DRA). Note that the DRA can pay close attention
to a small region of the sentences at each step and re-weigh the vitally
important words for better aspect-aware sentiment understanding. Finally,
experimental results on three benchmark datasets demonstrate the effectiveness
and the rationality of our proposed model and provide good interpretable
insights for future semantic modeling.


---

**[57. [2110.02042] ur-iw-hnt at GermEval 2021: An Ensembling Strategy with Multiple BERT
  Models](https://arxiv.org/pdf/2110.02042.pdf)** (2021-10-06)

*Hoai Nam Tran, Udo Kruschwitz*

  This paper describes our approach (ur-iw-hnt) for the Shared Task of
GermEval2021 to identify toxic, engaging, and fact-claiming comments. We
submitted three runs using an ensembling strategy by majority (hard) voting
with multiple different BERT models of three different types: German-based,
Twitter-based, and multilingual models. All ensemble models outperform single
models, while BERTweet is the winner of all individual models in every subtask.
Twitter-based models perform better than GermanBERT models, and multilingual
models perform worse but by a small margin.


---

**[58. [1908.04812] An Effective Domain Adaptive Post-Training Method for BERT in Response
  Selection](https://arxiv.org/pdf/1908.04812.pdf)** (2020-07-28)

*Taesun Whang, Dongyub Lee, Chanhee Lee, Kisu Yang, Dongsuk Oh, HeuiSeok Lim*

  We focus on multi-turn response selection in a retrieval-based dialog system.
In this paper, we utilize the powerful pre-trained language model
Bi-directional Encoder Representations from Transformer (BERT) for a multi-turn
dialog system and propose a highly effective post-training method on
domain-specific corpus. Although BERT is easily adopted to various NLP tasks
and outperforms previous baselines of each task, it still has limitations if a
task corpus is too focused on a certain domain. Post-training on
domain-specific corpus (e.g., Ubuntu Corpus) helps the model to train
contextualized representations and words that do not appear in general corpus
(e.g., English Wikipedia). Experimental results show that our approach achieves
new state-of-the-art on two response selection benchmarks (i.e., Ubuntu Corpus
V1, Advising Corpus) performance improvement by 5.9% and 6% on R@1.


---

**[59. [2312.10652] Explorers at #SMM4H 2023: Enhancing BERT for Health Applications through
  Knowledge and Model Fusion](https://arxiv.org/pdf/2312.10652.pdf)** (2023-12-19)

*Xutong Yue, Xilai Wang, Yuxin He, Zhenkun Zhou*

  An increasing number of individuals are willing to post states and opinions
in social media, which has become a valuable data resource for studying human
health. Furthermore, social media has been a crucial research point for
healthcare now. This paper outlines the methods in our participation in the
#SMM4H 2023 Shared Tasks, including data preprocessing, continual pre-training
and fine-tuned optimization strategies. Especially for the Named Entity
Recognition (NER) task, we utilize the model architecture named W2NER that
effectively enhances the model generalization ability. Our method achieved
first place in the Task 3. This paper has been peer-reviewed and accepted for
presentation at the #SMM4H 2023 Workshop.


---

**[60. [2412.05816] Real-Time Prediction for Athletes' Psychological States Using
  BERT-XGBoost: Enhancing Human-Computer Interaction](https://arxiv.org/pdf/2412.05816.pdf)** (2024-12-10)

*Chenming Duan, Zhitao Shu, Jingsi Zhang, Feng Xue*

  Understanding and predicting athletes' mental states is crucial for
optimizing sports performance. This study introduces a hybrid BERT-XGBoost
model to analyze psychological factors such as emotions, anxiety, and stress,
and predict their impact on performance. By combining BERT's bidirectional
contextual learning with XGBoost's classification efficiency, the model
achieves high accuracy (94%) in identifying psychological patterns from both
structured and unstructured data, including self-reports and observational data
tagged with categories like emotional balance and stress. The model also
incorporates real-time monitoring and feedback mechanisms to provide
personalized interventions based on athletes' psychological states. Designed to
engage athletes intuitively, the system adapts its feedback dynamically to
promote emotional well-being and performance enhancement. By analyzing
emotional trajectories in real-time offers empathetic, proactive interactions.
This approach optimizes performance outcomes and ensures continuous monitoring
of mental health, improving human-computer interaction and providing an
adaptive, user-centered model for psychological support in sports.


---

**[61. [2105.09816] Intra-Document Cascading: Learning to Select Passages for Neural
  Document Ranking](https://arxiv.org/pdf/2105.09816.pdf)** (2021-05-21)

*Sebastian Hofstätter, Bhaskar Mitra, Hamed Zamani, Nick Craswell, Allan Hanbury*

  An emerging recipe for achieving state-of-the-art effectiveness in neural
document re-ranking involves utilizing large pre-trained language models -
e.g., BERT - to evaluate all individual passages in the document and then
aggregating the outputs by pooling or additional Transformer layers. A major
drawback of this approach is high query latency due to the cost of evaluating
every passage in the document with BERT. To make matters worse, this high
inference cost and latency varies based on the length of the document, with
longer documents requiring more time and computation. To address this
challenge, we adopt an intra-document cascading strategy, which prunes passages
of a candidate document using a less expensive model, called ESM, before
running a scoring model that is more expensive and effective, called ETM. We
found it best to train ESM (short for Efficient Student Model) via knowledge
distillation from the ETM (short for Effective Teacher Model) e.g., BERT. This
pruning allows us to only run the ETM model on a smaller set of passages whose
size does not vary by document length. Our experiments on the MS MARCO and TREC
Deep Learning Track benchmarks suggest that the proposed Intra-Document
Cascaded Ranking Model (IDCM) leads to over 400% lower query latency by
providing essentially the same effectiveness as the state-of-the-art BERT-based
document ranking models.


---

**[62. [2010.07615] Asynchronous \epsilon-Greedy Bayesian Optimisation](https://arxiv.org/pdf/2010.07615.pdf)** (2021-06-14)

*George De Ath, Richard M. Everson, Jonathan E. Fieldsend*

  Batch Bayesian optimisation (BO) is a successful technique for the
optimisation of expensive black-box functions. Asynchronous BO can reduce
wallclock time by starting a new evaluation as soon as another finishes, thus
maximising resource utilisation. To maximise resource allocation, we develop a
novel asynchronous BO method, AEGiS (Asynchronous $\epsilon$-Greedy Global
Search) that combines greedy search, exploiting the surrogate's mean
prediction, with Thompson sampling and random selection from the approximate
Pareto set describing the trade-off between exploitation (surrogate mean
prediction) and exploration (surrogate posterior variance). We demonstrate
empirically the efficacy of AEGiS on synthetic benchmark problems,
meta-surrogate hyperparameter tuning problems and real-world problems, showing
that AEGiS generally outperforms existing methods for asynchronous BO. When a
single worker is available performance is no worse than BO using expected
improvement.


---

**[63. [2102.01051] SJ_AJ@DravidianLangTech-EACL2021: Task-Adaptive Pre-Training of
  Multilingual BERT models for Offensive Language Identification](https://arxiv.org/pdf/2102.01051.pdf)** (2021-03-15)

*Sai Muralidhar Jayanthi, Akshat Gupta*

  In this paper we present our submission for the EACL 2021-Shared Task on
Offensive Language Identification in Dravidian languages. Our final system is
an ensemble of mBERT and XLM-RoBERTa models which leverage task-adaptive
pre-training of multilingual BERT models with a masked language modeling
objective. Our system was ranked 1st for Kannada, 2nd for Malayalam and 3rd for
Tamil.


---

**[64. [2004.04037] DynaBERT: Dynamic BERT with Adaptive Width and Depth](https://arxiv.org/pdf/2004.04037.pdf)** (2020-10-12)

*Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, Qun Liu*

  The pre-trained language models like BERT, though powerful in many natural
language processing tasks, are both computation and memory expensive. To
alleviate this problem, one approach is to compress them for specific tasks
before deployment. However, recent works on BERT compression usually compress
the large BERT model to a fixed smaller size. They can not fully satisfy the
requirements of different edge devices with various hardware performances. In
this paper, we propose a novel dynamic BERT model (abbreviated as DynaBERT),
which can flexibly adjust the size and latency by selecting adaptive width and
depth. The training process of DynaBERT includes first training a
width-adaptive BERT and then allowing both adaptive width and depth, by
distilling knowledge from the full-sized model to small sub-networks. Network
rewiring is also used to keep the more important attention heads and neurons
shared by more sub-networks. Comprehensive experiments under various efficiency
constraints demonstrate that our proposed dynamic BERT (or RoBERTa) at its
largest size has comparable performance as BERT-base (or RoBERTa-base), while
at smaller widths and depths consistently outperforms existing BERT compression
methods. Code is available at
https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/DynaBERT.


---

**[65. [2206.13703] Kwame for Science: An AI Teaching Assistant Based on Sentence-BERT for
  Science Education in West Africa](https://arxiv.org/pdf/2206.13703.pdf)** (2022-07-12)

*George Boateng, Samuel John, Andrew Glago, Samuel Boateng, Victor Kumbol*

  Africa has a high student-to-teacher ratio which limits students' access to
teachers. Consequently, students struggle to get answers to their questions. In
this work, we extended Kwame, our previous AI teaching assistant, adapted it
for science education, and deployed it as a web app. Kwame for Science answers
questions of students based on the Integrated Science subject of the West
African Senior Secondary Certificate Examination (WASSCE). Kwame for Science is
a Sentence-BERT-based question-answering web app that displays 3 paragraphs as
answers along with a confidence score in response to science questions.
Additionally, it displays the top 5 related past exam questions and their
answers in addition to the 3 paragraphs. Our preliminary evaluation of the
Kwame for Science with a 2.5-week real-world deployment showed a top 3 accuracy
of 87.5% (n=56) with 190 users across 11 countries. Kwame for Science will
enable the delivery of scalable, cost-effective, and quality remote education
to millions of people across Africa.


---

**[66. [2312.10679] Bengali Intent Classification with Generative Adversarial BERT](https://arxiv.org/pdf/2312.10679.pdf)** (2023-12-19)

*Mehedi Hasan, Mohammad Jahid Ibna Basher, Md. Tanvir Rouf Shawon*

  Intent classification is a fundamental task in natural language
understanding, aiming to categorize user queries or sentences into predefined
classes to understand user intent. The most challenging aspect of this
particular task lies in effectively incorporating all possible classes of
intent into a dataset while ensuring adequate linguistic variation. Plenty of
research has been conducted in the related domains in rich-resource languages
like English. In this study, we introduce BNIntent30, a comprehensive Bengali
intent classification dataset containing 30 intent classes. The dataset is
excerpted and translated from the CLINIC150 dataset containing a diverse range
of user intents categorized over 150 classes. Furthermore, we propose a novel
approach for Bengali intent classification using Generative Adversarial BERT to
evaluate the proposed dataset, which we call GAN-BnBERT. Our approach leverages
the power of BERT-based contextual embeddings to capture salient linguistic
features and contextual information from the text data, while the generative
adversarial network (GAN) component complements the model's ability to learn
diverse representations of existing intent classes through generative modeling.
Our experimental results demonstrate that the GAN-BnBERT model achieves
superior performance on the newly introduced BNIntent30 dataset, surpassing the
existing Bi-LSTM and the stand-alone BERT-based classification model.


---

**[67. [2301.12699] KG-BERTScore: Incorporating Knowledge Graph into BERTScore for
  Reference-Free Machine Translation Evaluation](https://arxiv.org/pdf/2301.12699.pdf)** (2023-01-31)

*Zhanglin Wu, Min Zhang, Ming Zhu, Yinglu Li, Ting Zhu, Hao Yang, Song Peng, Ying Qin*

  BERTScore is an effective and robust automatic metric for referencebased
machine translation evaluation. In this paper, we incorporate multilingual
knowledge graph into BERTScore and propose a metric named KG-BERTScore, which
linearly combines the results of BERTScore and bilingual named entity matching
for reference-free machine translation evaluation. From the experimental
results on WMT19 QE as a metric without references shared tasks, our metric
KG-BERTScore gets higher overall correlation with human judgements than the
current state-of-the-art metrics for reference-free machine translation
evaluation.1 Moreover, the pre-trained multilingual model used by KG-BERTScore
and the parameter for linear combination are also studied in this paper.


---

**[68. [2208.10246] SDBERT: SparseDistilBERT, a faster and smaller BERT model](https://arxiv.org/pdf/2208.10246.pdf)** (2022-08-23)

*Devaraju Vinoda, Pawan Kumar Yadav*

  In this work we introduce a new transformer architecture called
SparseDistilBERT (SDBERT), which is a combination of sparse attention and
knowledge distillantion (KD). We implemented sparse attention mechanism to
reduce quadratic dependency on input length to linear. In addition to reducing
computational complexity of the model, we used knowledge distillation (KD). We
were able to reduce the size of BERT model by 60% while retaining 97%
performance and it only took 40% of time to train.


---

**[69. [2005.00683] Birds have four legs?! NumerSense: Probing Numerical Commonsense
  Knowledge of Pre-trained Language Models](https://arxiv.org/pdf/2005.00683.pdf)** (2020-09-21)

*Bill Yuchen Lin, Seyeon Lee, Rahul Khanna, Xiang Ren*

  Recent works show that pre-trained language models (PTLMs), such as BERT,
possess certain commonsense and factual knowledge. They suggest that it is
promising to use PTLMs as "neural knowledge bases" via predicting masked words.
Surprisingly, we find that this may not work for numerical commonsense
knowledge (e.g., a bird usually has two legs). In this paper, we investigate
whether and to what extent we can induce numerical commonsense knowledge from
PTLMs as well as the robustness of this process. To study this, we introduce a
novel probing task with a diagnostic dataset, NumerSense, containing 13.6k
masked-word-prediction probes (10.5k for fine-tuning and 3.1k for testing). Our
analysis reveals that: (1) BERT and its stronger variant RoBERTa perform poorly
on the diagnostic dataset prior to any fine-tuning; (2) fine-tuning with
distant supervision brings some improvement; (3) the best supervised model
still performs poorly as compared to human performance (54.06% vs 96.3% in
accuracy).


---

**[70. [2110.06620] Maximizing Efficiency of Language Model Pre-training for Learning
  Representation](https://arxiv.org/pdf/2110.06620.pdf)** (2021-10-14)

*Junmo Kang, Suwon Shin, Jeonghwan Kim, Jaeyoung Jo, Sung-Hyon Myaeng*

  Pre-trained language models in the past years have shown exponential growth
in model parameters and compute time. ELECTRA is a novel approach for improving
the compute efficiency of pre-trained language models (e.g. BERT) based on
masked language modeling (MLM) by addressing the sample inefficiency problem
with the replaced token detection (RTD) task. Our work proposes adaptive early
exit strategy to maximize the efficiency of the pre-training process by
relieving the model's subsequent layers of the need to process latent features
by leveraging earlier layer representations. Moreover, we evaluate an initial
approach to the problem that has not succeeded in maintaining the accuracy of
the model while showing a promising compute efficiency by thoroughly
investigating the necessity of the generator module of ELECTRA.


---

**[71. [2001.11316] Adversarial Training for Aspect-Based Sentiment Analysis with BERT](https://arxiv.org/pdf/2001.11316.pdf)** (2020-10-26)

*Akbar Karimi, Leonardo Rossi, Andrea Prati*

  Aspect-Based Sentiment Analysis (ABSA) deals with the extraction of
sentiments and their targets. Collecting labeled data for this task in order to
help neural networks generalize better can be laborious and time-consuming. As
an alternative, similar data to the real-world examples can be produced
artificially through an adversarial process which is carried out in the
embedding space. Although these examples are not real sentences, they have been
shown to act as a regularization method which can make neural networks more
robust. In this work, we apply adversarial training, which was put forward by
Goodfellow et al. (2014), to the post-trained BERT (BERT-PT) language model
proposed by Xu et al. (2019) on the two major tasks of Aspect Extraction and
Aspect Sentiment Classification in sentiment analysis. After improving the
results of post-trained BERT by an ablation study, we propose a novel
architecture called BERT Adversarial Training (BAT) to utilize adversarial
training in ABSA. The proposed model outperforms post-trained BERT in both
tasks. To the best of our knowledge, this is the first study on the application
of adversarial training in ABSA.


---

**[72. [2312.05483] Teamwork Dimensions Classification Using BERT](https://arxiv.org/pdf/2312.05483.pdf)** (2023-12-12)

*Junyoung Lee, Elizabeth Koh*

  Teamwork is a necessary competency for students that is often inadequately
assessed. Towards providing a formative assessment of student teamwork, an
automated natural language processing approach was developed to identify
teamwork dimensions of students' online team chat. Developments in the field of
natural language processing and artificial intelligence have resulted in
advanced deep transfer learning approaches namely the Bidirectional Encoder
Representations from Transformers (BERT) model that allow for more in-depth
understanding of the context of the text. While traditional machine learning
algorithms were used in the previous work for the automatic classification of
chat messages into the different teamwork dimensions, our findings have shown
that classifiers based on the pre-trained language model BERT provides improved
classification performance, as well as much potential for generalizability in
the language use of varying team chat contexts and team member demographics.
This model will contribute towards an enhanced learning analytics tool for
teamwork assessment and feedback.


---

**[73. [2411.14393] POS-tagging to highlight the skeletal structure of sentences](https://arxiv.org/pdf/2411.14393.pdf)** (2024-11-22)

*Grigorii Churakov*

  This study presents the development of a part-of-speech (POS) tagging model
to extract the skeletal structure of sentences using transfer learning with the
BERT architecture for token classification. The model, fine-tuned on Russian
text, demonstrating its effectiveness. The approach offers potential
applications in enhancing natural language processing tasks, such as improving
machine translation.
  Keywords: part of speech tagging, morphological analysis, natural language
processing, BERT.


---

**[74. [2210.16663] BERT Meets CTC: New Formulation of End-to-End Speech Recognition with
  Pre-trained Masked Language Model](https://arxiv.org/pdf/2210.16663.pdf)** (2023-04-21)

*Yosuke Higuchi, Brian Yan, Siddhant Arora, Tetsuji Ogawa, Tetsunori Kobayashi, Shinji Watanabe*

  This paper presents BERT-CTC, a novel formulation of end-to-end speech
recognition that adapts BERT for connectionist temporal classification (CTC).
Our formulation relaxes the conditional independence assumptions used in
conventional CTC and incorporates linguistic knowledge through the explicit
output dependency obtained by BERT contextual embedding. BERT-CTC attends to
the full contexts of the input and hypothesized output sequences via the
self-attention mechanism. This mechanism encourages a model to learn
inner/inter-dependencies between the audio and token representations while
maintaining CTC's training efficiency. During inference, BERT-CTC combines a
mask-predict algorithm with CTC decoding, which iteratively refines an output
sequence. The experimental results reveal that BERT-CTC improves over
conventional approaches across variations in speaking styles and languages.
Finally, we show that the semantic representations in BERT-CTC are beneficial
towards downstream spoken language understanding tasks.


---

**[75. [2112.00699] BERT_SE: A Pre-trained Language Representation Model for Software
  Engineering](https://arxiv.org/pdf/2112.00699.pdf)** (2021-12-02)

*Eliane Maria De Bortoli Fávero, Dalcimar Casanova*

  The application of Natural Language Processing (NLP) has achieved a high
level of relevance in several areas. In the field of software engineering (SE),
NLP applications are based on the classification of similar texts (e.g.
software requirements), applied in tasks of estimating software effort,
selection of human resources, etc. Classifying software requirements has been a
complex task, considering the informality and complexity inherent in the texts
produced during the software development process. The pre-trained embedding
models are shown as a viable alternative when considering the low volume of
textual data labeled in the area of software engineering, as well as the lack
of quality of these data. Although there is much research around the
application of word embedding in several areas, to date, there is no knowledge
of studies that have explored its application in the creation of a specific
model for the domain of the SE area. Thus, this article presents the proposal
for a contextualized embedding model, called BERT_SE, which allows the
recognition of specific and relevant terms in the context of SE. The assessment
of BERT_SE was performed using the software requirements classification task,
demonstrating that this model has an average improvement rate of 13% concerning
the BERT_base model, made available by the authors of BERT. The code and
pre-trained models are available at https://github.com/elianedb.


---

**[76. [2208.01375] BERT4Loc: BERT for Location -- POI Recommender System](https://arxiv.org/pdf/2208.01375.pdf)** (2023-05-17)

*Syed Raza Bashir, Shaina Raza, Vojislav Misic*

  Recommending points of interest (POIs) is a challenging task that requires
extracting comprehensive location data from location-based social media
platforms. To provide effective location-based recommendations, it's important
to analyze users' historical behavior and preferences. In this study, we
present a sophisticated location-aware recommendation system that uses
Bidirectional Encoder Representations from Transformers (BERT) to offer
personalized location-based suggestions. Our model combines location
information and user preferences to provide more relevant recommendations
compared to models that predict the next POI in a sequence. Our experiments on
two benchmark dataset show that our BERT-based model outperforms various
state-of-the-art sequential models. Moreover, we see the effectiveness of the
proposed model for quality through additional experiments.


---

**[77. [2105.11618] TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference](https://arxiv.org/pdf/2105.11618.pdf)** (2021-05-26)

*Deming Ye, Yankai Lin, Yufei Huang, Maosong Sun*

  Existing pre-trained language models (PLMs) are often computationally
expensive in inference, making them impractical in various resource-limited
real-world applications. To address this issue, we propose a dynamic token
reduction approach to accelerate PLMs' inference, named TR-BERT, which could
flexibly adapt the layer number of each token in inference to avoid redundant
calculation. Specially, TR-BERT formulates the token reduction process as a
multi-step token selection problem and automatically learns the selection
strategy via reinforcement learning. The experimental results on several
downstream NLP tasks show that TR-BERT is able to speed up BERT by 2-5 times to
satisfy various performance demands. Moreover, TR-BERT can also achieve better
performance with less computation in a suite of long-text tasks since its
token-level layer number adaption greatly accelerates the self-attention
operation in PLMs. The source code and experiment details of this paper can be
obtained from https://github.com/thunlp/TR-BERT.


---

**[78. [2307.07160] Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking
  In-domain Keywords](https://arxiv.org/pdf/2307.07160.pdf)** (2023-07-17)

*Shahriar Golchin, Mihai Surdeanu, Nazgol Tavabi, Ata Kiapour*

  We propose a novel task-agnostic in-domain pre-training method that sits
between generic pre-training and fine-tuning. Our approach selectively masks
in-domain keywords, i.e., words that provide a compact representation of the
target domain. We identify such keywords using KeyBERT (Grootendorst, 2020). We
evaluate our approach using six different settings: three datasets combined
with two distinct pre-trained language models (PLMs). Our results reveal that
the fine-tuned PLMs adapted using our in-domain pre-training strategy
outperform PLMs that used in-domain pre-training with random masking as well as
those that followed the common pre-train-then-fine-tune paradigm. Further, the
overhead of identifying in-domain keywords is reasonable, e.g., 7-15% of the
pre-training time (for two epochs) for BERT Large (Devlin et al., 2019).


---

**[79. [2202.02268] StonkBERT: Can Language Models Predict Medium-Run Stock Price Movements?](https://arxiv.org/pdf/2202.02268.pdf)** (2022-02-07)

*Stefan Pasch, Daniel Ehnes*

  To answer this question, we fine-tune transformer-based language models,
including BERT, on different sources of company-related text data for a
classification task to predict the one-year stock price performance. We use
three different types of text data: News articles, blogs, and annual reports.
This allows us to analyze to what extent the performance of language models is
dependent on the type of the underlying document. StonkBERT, our
transformer-based stock performance classifier, shows substantial improvement
in predictive accuracy compared to traditional language models. The highest
performance was achieved with news articles as text source. Performance
simulations indicate that these improvements in classification accuracy also
translate into above-average stock market returns.


---

**[80. [2401.02158] Shayona@SMM4H23: COVID-19 Self diagnosis classification using BERT and
  LightGBM models](https://arxiv.org/pdf/2401.02158.pdf)** (2024-01-05)

*Rushi Chavda, Darshan Makwana, Vraj Patel, Anupam Shukla*

  This paper describes approaches and results for shared Task 1 and 4 of
SMMH4-23 by Team Shayona. Shared Task-1 was binary classification of english
tweets self-reporting a COVID-19 diagnosis, and Shared Task-4 was Binary
classification of English Reddit posts self-reporting a social anxiety disorder
diagnosis. Our team has achieved the highest f1-score 0.94 in Task-1 among all
participants. We have leveraged the Transformer model (BERT) in combination
with the LightGBM model for both tasks.


---

**[81. [2103.02800] Hardware Acceleration of Fully Quantized BERT for Efficient Natural
  Language Processing](https://arxiv.org/pdf/2103.02800.pdf)** (2021-03-05)

*Zejian Liu, Gang Li, Jian Cheng*

  BERT is the most recent Transformer-based model that achieves
state-of-the-art performance in various NLP tasks. In this paper, we
investigate the hardware acceleration of BERT on FPGA for edge computing. To
tackle the issue of huge computational complexity and memory footprint, we
propose to fully quantize the BERT (FQ-BERT), including weights, activations,
softmax, layer normalization, and all the intermediate results. Experiments
demonstrate that the FQ-BERT can achieve 7.94x compression for weights with
negligible performance loss. We then propose an accelerator tailored for the
FQ-BERT and evaluate on Xilinx ZCU102 and ZCU111 FPGA. It can achieve a
performance-per-watt of 3.18 fps/W, which is 28.91x and 12.72x over Intel(R)
Core(TM) i7-8700 CPU and NVIDIA K80 GPU, respectively.


---

**[82. [2009.11473] AnchiBERT: A Pre-Trained Model for Ancient ChineseLanguage Understanding
  and Generation](https://arxiv.org/pdf/2009.11473.pdf)** (2021-04-22)

*Huishuang Tian, Kexin Yang, Dayiheng Liu, Jiancheng Lv*

  Ancient Chinese is the essence of Chinese culture. There are several natural
language processing tasks of ancient Chinese domain, such as ancient-modern
Chinese translation, poem generation, and couplet generation. Previous studies
usually use the supervised models which deeply rely on parallel data. However,
it is difficult to obtain large-scale parallel data of ancient Chinese. In
order to make full use of the more easily available monolingual ancient Chinese
corpora, we release AnchiBERT, a pre-trained language model based on the
architecture of BERT, which is trained on large-scale ancient Chinese corpora.
We evaluate AnchiBERT on both language understanding and generation tasks,
including poem classification, ancient-modern Chinese translation, poem
generation, and couplet generation. The experimental results show that
AnchiBERT outperforms BERT as well as the non-pretrained models and achieves
state-of-the-art results in all cases.


---

**[83. [2305.18319] Automated Feedback Generation for a Chemistry Database and Abstracting
  Exercise](https://arxiv.org/pdf/2305.18319.pdf)** (2023-05-31)

*Oscar Morris, Russell Morris*

  Timely feedback is an important part of teaching and learning. Here we
describe how a readily available neural network transformer (machine-learning)
model (BERT) can be used to give feedback on the structure of the response to
an abstracting exercise where students are asked to summarise the contents of a
published article after finding it from a publication database. The dataset
contained 207 submissions from two consecutive years of the course, summarising
a total of 21 different papers from the primary literature. The model was
pre-trained using an available dataset (approx. 15,000 samples) and then
fine-tuned on 80% of the submitted dataset. This fine tuning was seen to be
important. The sentences in the student submissions are characterised into
three classes - background, technique and observation - which allows a
comparison of how each submission is structured. Comparing the structure of the
students' abstract a large collection of those from the PubMed database shows
that students in this exercise concentrate more on the background to the paper
and less on the techniques and results than the abstracts to papers themselves.
The results allowed feedback for each submitted assignment to be automatically
generated.


---

**[84. [2002.06652] SBERT-WK: A Sentence Embedding Method by Dissecting BERT-based Word
  Models](https://arxiv.org/pdf/2002.06652.pdf)** (2020-06-02)

*Bin Wang, C. -C. Jay Kuo*

  Sentence embedding is an important research topic in natural language
processing (NLP) since it can transfer knowledge to downstream tasks.
Meanwhile, a contextualized word representation, called BERT, achieves the
state-of-the-art performance in quite a few NLP tasks. Yet, it is an open
problem to generate a high quality sentence representation from BERT-based word
models. It was shown in previous study that different layers of BERT capture
different linguistic properties. This allows us to fusion information across
layers to find better sentence representation. In this work, we study the
layer-wise pattern of the word representation of deep contextualized models.
Then, we propose a new sentence embedding method by dissecting BERT-based word
models through geometric analysis of the space spanned by the word
representation. It is called the SBERT-WK method. No further training is
required in SBERT-WK. We evaluate SBERT-WK on semantic textual similarity and
downstream supervised tasks. Furthermore, ten sentence-level probing tasks are
presented for detailed linguistic analysis. Experiments show that SBERT-WK
achieves the state-of-the-art performance. Our codes are publicly available.


---

**[85. [2403.01994] Vanilla Transformers are Transfer Capability Teachers](https://arxiv.org/pdf/2403.01994.pdf)** (2024-03-05)

*Xin Lu, Yanyan Zhao, Bing Qin*

  Recently, Mixture of Experts (MoE) Transformers have garnered increasing
attention due to their advantages in model capacity and computational
efficiency. However, studies have indicated that MoE Transformers underperform
vanilla Transformers in many downstream tasks, significantly diminishing the
practical value of MoE models. To explain this issue, we propose that the
pre-training performance and transfer capability of a model are joint
determinants of its downstream task performance. MoE models, in comparison to
vanilla models, have poorer transfer capability, leading to their subpar
performance in downstream tasks. To address this issue, we introduce the
concept of transfer capability distillation, positing that although vanilla
models have weaker performance, they are effective teachers of transfer
capability. The MoE models guided by vanilla models can achieve both strong
pre-training performance and transfer capability, ultimately enhancing their
performance in downstream tasks. We design a specific distillation method and
conduct experiments on the BERT architecture. Experimental results show a
significant improvement in downstream performance of MoE models, and many
further evidences also strongly support the concept of transfer capability
distillation. Finally, we attempt to interpret transfer capability distillation
and provide some insights from the perspective of model feature.


---

**[86. [2105.11314] RobeCzech: Czech RoBERTa, a monolingual contextualized language
  representation model](https://arxiv.org/pdf/2105.11314.pdf)** (2021-10-15)

*Milan Straka, Jakub Náplava, Jana Straková, David Samuel*

  We present RobeCzech, a monolingual RoBERTa language representation model
trained on Czech data. RoBERTa is a robustly optimized Transformer-based
pretraining approach. We show that RobeCzech considerably outperforms
equally-sized multilingual and Czech-trained contextualized language
representation models, surpasses current state of the art in all five evaluated
NLP tasks and reaches state-of-the-art results in four of them. The RobeCzech
model is released publicly at https://hdl.handle.net/11234/1-3691 and
https://huggingface.co/ufal/robeczech-base.


---

**[87. [2109.10234] BERTweetFR : Domain Adaptation of Pre-Trained Language Models for French
  Tweets](https://arxiv.org/pdf/2109.10234.pdf)** (2021-09-22)

*Yanzhu Guo, Virgile Rennard, Christos Xypolopoulos, Michalis Vazirgiannis*

  We introduce BERTweetFR, the first large-scale pre-trained language model for
French tweets. Our model is initialized using the general-domain French
language model CamemBERT which follows the base architecture of RoBERTa.
Experiments show that BERTweetFR outperforms all previous general-domain French
language models on two downstream Twitter NLP tasks of offensiveness
identification and named entity recognition. The dataset used in the
offensiveness detection task is first created and annotated by our team,
filling in the gap of such analytic datasets in French. We make our model
publicly available in the transformers library with the aim of promoting future
research in analytic tasks for French tweets.


---

**[88. [2406.08519] Question-Answering (QA) Model for a Personalized Learning Assistant for
  Arabic Language](https://arxiv.org/pdf/2406.08519.pdf)** (2024-06-14)

*Mohammad Sammoudi, Ahmad Habaybeh, Huthaifa I. Ashqar, Mohammed Elhenawy*

  This paper describes the creation, optimization, and assessment of a
question-answering (QA) model for a personalized learning assistant that uses
BERT transformers customized for the Arabic language. The model was
particularly finetuned on science textbooks in Palestinian curriculum. Our
approach uses BERT's brilliant capabilities to automatically produce correct
answers to questions in the field of science education. The model's ability to
understand and extract pertinent information is improved by finetuning it using
11th and 12th grade biology book in Palestinian curriculum. This increases the
model's efficacy in producing enlightening responses. Exact match (EM) and F1
score metrics are used to assess the model's performance; the results show an
EM score of 20% and an F1 score of 51%. These findings show that the model can
comprehend and react to questions in the context of Palestinian science book.
The results demonstrate the potential of BERT-based QA models to support
learning and understanding Arabic students questions.


---

**[89. [2209.07562] TwHIN-BERT: A Socially-Enriched Pre-trained Language Model for
  Multilingual Tweet Representations at Twitter](https://arxiv.org/pdf/2209.07562.pdf)** (2023-08-29)

*Xinyang Zhang, Yury Malkov, Omar Florez, Serim Park, Brian McWilliams, Jiawei Han, Ahmed El-Kishky*

  Pre-trained language models (PLMs) are fundamental for natural language
processing applications. Most existing PLMs are not tailored to the noisy
user-generated text on social media, and the pre-training does not factor in
the valuable social engagement logs available in a social network. We present
TwHIN-BERT, a multilingual language model productionized at Twitter, trained on
in-domain data from the popular social network. TwHIN-BERT differs from prior
pre-trained language models as it is trained with not only text-based
self-supervision, but also with a social objective based on the rich social
engagements within a Twitter heterogeneous information network (TwHIN). Our
model is trained on 7 billion tweets covering over 100 distinct languages,
providing a valuable representation to model short, noisy, user-generated text.
We evaluate our model on various multilingual social recommendation and
semantic understanding tasks and demonstrate significant metric improvement
over established pre-trained language models. We open-source TwHIN-BERT and our
curated hashtag prediction and social engagement benchmark datasets to the
research community.


---

**[90. [2301.00321] Floods Relevancy and Identification of Location from Twitter Posts using
  NLP Techniques](https://arxiv.org/pdf/2301.00321.pdf)** (2023-01-03)

*Muhammad Suleman, Muhammad Asif, Tayyab Zamir, Ayaz Mehmood, Jebran Khan, Nasir Ahmad, Kashif Ahmad*

  This paper presents our solutions for the MediaEval 2022 task on DisasterMM.
The task is composed of two subtasks, namely (i) Relevance Classification of
Twitter Posts (RCTP), and (ii) Location Extraction from Twitter Texts (LETT).
The RCTP subtask aims at differentiating flood-related and non-relevant social
posts while LETT is a Named Entity Recognition (NER) task and aims at the
extraction of location information from the text. For RCTP, we proposed four
different solutions based on BERT, RoBERTa, Distil BERT, and ALBERT obtaining
an F1-score of 0.7934, 0.7970, 0.7613, and 0.7924, respectively. For LETT, we
used three models namely BERT, RoBERTa, and Distil BERTA obtaining an F1-score
of 0.6256, 0.6744, and 0.6723, respectively.


---

**[91. [2004.13816] DomBERT: Domain-oriented Language Model for Aspect-based Sentiment
  Analysis](https://arxiv.org/pdf/2004.13816.pdf)** (2020-04-30)

*Hu Xu, Bing Liu, Lei Shu, Philip S. Yu*

  This paper focuses on learning domain-oriented language models driven by end
tasks, which aims to combine the worlds of both general-purpose language models
(such as ELMo and BERT) and domain-specific language understanding. We propose
DomBERT, an extension of BERT to learn from both in-domain corpus and relevant
domain corpora. This helps in learning domain language models with
low-resources. Experiments are conducted on an assortment of tasks in
aspect-based sentiment analysis, demonstrating promising results.


---

**[92. [2003.13821] NukeBERT: A Pre-trained language model for Low Resource Nuclear Domain](https://arxiv.org/pdf/2003.13821.pdf)** (2024-09-05)

*Ayush Jain, N. M. Meenachi, B. Venkatraman*

  Significant advances have been made in recent years on Natural Language
Processing with machines surpassing human performance in many tasks, including
but not limited to Question Answering. The majority of deep learning methods
for Question Answering targets domains with large datasets and highly matured
literature. The area of Nuclear and Atomic energy has largely remained
unexplored in exploiting non-annotated data for driving industry viable
applications. Due to lack of dataset, a new dataset was created from the 7000
research papers on nuclear domain. This paper contributes to research in
understanding nuclear domain knowledge which is then evaluated on Nuclear
Question Answering Dataset (NQuAD) created by nuclear domain experts as part of
this research. NQuAD contains 612 questions developed on 181 paragraphs
randomly selected from the IGCAR research paper corpus. In this paper, the
Nuclear Bidirectional Encoder Representational Transformers (NukeBERT) is
proposed, which incorporates a novel technique for building BERT vocabulary to
make it suitable for tasks with less training data. The experiments evaluated
on NQuAD revealed that NukeBERT was able to outperform BERT significantly, thus
validating the adopted methodology. Training NukeBERT is computationally
expensive and hence we will be open-sourcing the NukeBERT pretrained weights
and NQuAD for fostering further research work in the nuclear domain.


---

**[93. [2304.04539] UATTA-EB: Uncertainty-Aware Test-Time Augmented Ensemble of BERTs for
  Classifying Common Mental Illnesses on Social Media Posts](https://arxiv.org/pdf/2304.04539.pdf)** (2023-04-11)

*Pratinav Seth, Mihir Agarwal*

  Given the current state of the world, because of existing situations around
the world, millions of people suffering from mental illnesses feel isolated and
unable to receive help in person. Psychological studies have shown that our
state of mind can manifest itself in the linguistic features we use to
communicate. People have increasingly turned to online platforms to express
themselves and seek help with their conditions. Deep learning methods have been
commonly used to identify and analyze mental health conditions from various
sources of information, including social media. Still, they face challenges,
including a lack of reliability and overconfidence in predictions resulting in
the poor calibration of the models. To solve these issues, We propose UATTA-EB:
Uncertainty-Aware Test-Time Augmented Ensembling of BERTs for producing
reliable and well-calibrated predictions to classify six possible types of
mental illnesses- None, Depression, Anxiety, Bipolar Disorder, ADHD, and PTSD
by analyzing unstructured user data on Reddit.


---

**[94. [2312.17349] Language Model as an Annotator: Unsupervised Context-aware Quality
  Phrase Generation](https://arxiv.org/pdf/2312.17349.pdf)** (2024-01-01)

*Zhihao Zhang, Yuan Zuo, Chenghua Lin, Junjie Wu*

  Phrase mining is a fundamental text mining task that aims to identify quality
phrases from context. Nevertheless, the scarcity of extensive gold labels
datasets, demanding substantial annotation efforts from experts, renders this
task exceptionally challenging. Furthermore, the emerging, infrequent, and
domain-specific nature of quality phrases presents further challenges in
dealing with this task. In this paper, we propose LMPhrase, a novel
unsupervised context-aware quality phrase mining framework built upon large
pre-trained language models (LMs). Specifically, we first mine quality phrases
as silver labels by employing a parameter-free probing technique called
Perturbed Masking on the pre-trained language model BERT (coined as Annotator).
In contrast to typical statistic-based or distantly-supervised methods, our
silver labels, derived from large pre-trained language models, take into
account rich contextual information contained in the LMs. As a result, they
bring distinct advantages in preserving informativeness, concordance, and
completeness of quality phrases. Secondly, training a discriminative span
prediction model heavily relies on massive annotated data and is likely to face
the risk of overfitting silver labels. Alternatively, we formalize phrase
tagging task as the sequence generation problem by directly fine-tuning on the
Sequence-to-Sequence pre-trained language model BART with silver labels (coined
as Generator). Finally, we merge the quality phrases from both the Annotator
and Generator as the final predictions, considering their complementary nature
and distinct characteristics. Extensive experiments show that our LMPhrase
consistently outperforms all the existing competitors across two different
granularity phrase mining tasks, where each task is tested on two different
domain datasets.


---

**[95. [2105.07452] How is BERT surprised? Layerwise detection of linguistic anomalies](https://arxiv.org/pdf/2105.07452.pdf)** (2021-05-18)

*Bai Li, Zining Zhu, Guillaume Thomas, Yang Xu, Frank Rudzicz*

  Transformer language models have shown remarkable ability in detecting when a
word is anomalous in context, but likelihood scores offer no information about
the cause of the anomaly. In this work, we use Gaussian models for density
estimation at intermediate layers of three language models (BERT, RoBERTa, and
XLNet), and evaluate our method on BLiMP, a grammaticality judgement benchmark.
In lower layers, surprisal is highly correlated to low token frequency, but
this correlation diminishes in upper layers. Next, we gather datasets of
morphosyntactic, semantic, and commonsense anomalies from psycholinguistic
studies; we find that the best performing model RoBERTa exhibits surprisal in
earlier layers when the anomaly is morphosyntactic than when it is semantic,
while commonsense anomalies do not exhibit surprisal at any intermediate layer.
These results suggest that language models employ separate mechanisms to detect
different types of linguistic anomalies.


---

**[96. [2406.00314] CASE: Efficient Curricular Data Pre-training for Building Assistive
  Psychology Expert Models](https://arxiv.org/pdf/2406.00314.pdf)** (2024-10-03)

*Sarthak Harne, Monjoy Narayan Choudhury, Madhav Rao, TK Srikanth, Seema Mehrotra, Apoorva Vashisht, Aarushi Basu, Manjit Sodhi*

  The limited availability of psychologists necessitates efficient
identification of individuals requiring urgent mental healthcare. This study
explores the use of Natural Language Processing (NLP) pipelines to analyze text
data from online mental health forums used for consultations. By analyzing
forum posts, these pipelines can flag users who may require immediate
professional attention. A crucial challenge in this domain is data privacy and
scarcity. To address this, we propose utilizing readily available curricular
texts used in institutes specializing in mental health for pre-training the NLP
pipelines. This helps us mimic the training process of a psychologist. Our work
presents CASE-BERT that flags potential mental health disorders based on forum
text. CASE-BERT demonstrates superior performance compared to existing methods,
achieving an f1 score of 0.91 for Depression and 0.88 for Anxiety, two of the
most commonly reported mental health disorders. Our code and data are publicly
available.


---

**[97. [2104.05745] Fighting the COVID-19 Infodemic with a Holistic BERT Ensemble](https://arxiv.org/pdf/2104.05745.pdf)** (2021-04-14)

*Giorgos Tziafas, Konstantinos Kogkalidis, Tommaso Caselli*

  This paper describes the TOKOFOU system, an ensemble model for misinformation
detection tasks based on six different transformer-based pre-trained encoders,
implemented in the context of the COVID-19 Infodemic Shared Task for English.
We fine tune each model on each of the task's questions and aggregate their
prediction scores using a majority voting approach. TOKOFOU obtains an overall
F1 score of 89.7%, ranking first.


---

**[98. [2207.04008] ABB-BERT: A BERT model for disambiguating abbreviations and contractions](https://arxiv.org/pdf/2207.04008.pdf)** (2022-07-11)

*Prateek Kacker, Andi Cupallari, Aswin Gridhar Subramanian, Nimit Jain*

  Abbreviations and contractions are commonly found in text across different
domains. For example, doctors' notes contain many contractions that can be
personalized based on their choices. Existing spelling correction models are
not suitable to handle expansions because of many reductions of characters in
words. In this work, we propose ABB-BERT, a BERT-based model, which deals with
an ambiguous language containing abbreviations and contractions. ABB-BERT can
rank them from thousands of options and is designed for scale. It is trained on
Wikipedia text, and the algorithm allows it to be fine-tuned with little
compute to get better performance for a domain or person. We are publicly
releasing the training dataset for abbreviations and contractions derived from
Wikipedia.


---

**[99. [2110.07552] BI-RADS BERT & Using Section Segmentation to Understand Radiology
  Reports](https://arxiv.org/pdf/2110.07552.pdf)** (2024-09-04)

*Grey Kuling, Belinda Curpen, Anne L. Martel*

  Radiology reports are one of the main forms of communication between
radiologists and other clinicians and contain important information for patient
care. In order to use this information for research and automated patient care
programs, it is necessary to convert the raw text into structured data suitable
for analysis. State-of-the-art natural language processing (NLP)
domain-specific contextual word embeddings have been shown to achieve
impressive accuracy for these tasks in medicine, but have yet to be utilized
for section structure segmentation. In this work, we pre-trained a contextual
embedding BERT model using breast radiology reports and developed a classifier
that incorporated the embedding with auxiliary global textual features in order
to perform section segmentation. This model achieved a 98% accuracy at
segregating free text reports sentence by sentence into sections of information
outlined in the Breast Imaging Reporting and Data System (BI-RADS) lexicon, a
significant improvement over the Classic BERT model without auxiliary
information. We then evaluated whether using section segmentation improved the
downstream extraction of clinically relevant information such as
modality/procedure, previous cancer, menopausal status, the purpose of the
exam, breast density, and breast MRI background parenchymal enhancement. Using
the BERT model pre-trained on breast radiology reports combined with section
segmentation resulted in an overall accuracy of 95.9% in the field extraction
tasks. This is a 17% improvement compared to an overall accuracy of 78.9% for
field extraction with models using Classic BERT embeddings and not using
section segmentation. Our work shows the strength of using BERT in radiology
report analysis and the advantages of section segmentation in identifying key
features of patient factors recorded in breast radiology reports.


---

**[100. [2104.04739] MIPT-NSU-UTMN at SemEval-2021 Task 5: Ensembling Learning with
  Pre-trained Language Models for Toxic Spans Detection](https://arxiv.org/pdf/2104.04739.pdf)** (2021-08-30)

*Mikhail Kotyushev, Anna Glazkova, Dmitry Morozov*

  This paper describes our system for SemEval-2021 Task 5 on Toxic Spans
Detection. We developed ensemble models using BERT-based neural architectures
and post-processing to combine tokens into spans. We evaluated several
pre-trained language models using various ensemble techniques for toxic span
identification and achieved sizable improvements over our baseline fine-tuned
BERT models. Finally, our system obtained a F1-score of 67.55% on test data.


---

**[101. [2305.04673] PreCog: Exploring the Relation between Memorization and Performance in
  Pre-trained Language Models](https://arxiv.org/pdf/2305.04673.pdf)** (2024-11-12)

*Leonardo Ranaldi, Elena Sofia Ruzzetti, Fabio Massimo Zanzotto*

  Pre-trained Language Models such as BERT are impressive machines with the
ability to memorize, possibly generalized learning examples. We present here a
small, focused contribution to the analysis of the interplay between
memorization and performance of BERT in downstream tasks. We propose PreCog, a
measure for evaluating memorization from pre-training, and we analyze its
correlation with the BERT's performance. Our experiments show that highly
memorized examples are better classified, suggesting memorization is an
essential key to success for BERT.


---

**[102. [2008.06884] DeVLBert: Learning Deconfounded Visio-Linguistic Representations](https://arxiv.org/pdf/2008.06884.pdf)** (2020-10-05)

*Shengyu Zhang, Tan Jiang, Tan Wang, Kun Kuang, Zhou Zhao, Jianke Zhu, Jin Yu, Hongxia Yang, Fei Wu*

  In this paper, we propose to investigate the problem of out-of-domain
visio-linguistic pretraining, where the pretraining data distribution differs
from that of downstream data on which the pretrained model will be fine-tuned.
Existing methods for this problem are purely likelihood-based, leading to the
spurious correlations and hurt the generalization ability when transferred to
out-of-domain downstream tasks. By spurious correlation, we mean that the
conditional probability of one token (object or word) given another one can be
high (due to the dataset biases) without robust (causal) relationships between
them. To mitigate such dataset biases, we propose a Deconfounded
Visio-Linguistic Bert framework, abbreviated as DeVLBert, to perform
intervention-based learning. We borrow the idea of the backdoor adjustment from
the research field of causality and propose several neural-network based
architectures for Bert-style out-of-domain pretraining. The quantitative
results on three downstream tasks, Image Retrieval (IR), Zero-shot IR, and
Visual Question Answering, show the effectiveness of DeVLBert by boosting
generalization ability.


---

**[103. [2001.04246] AdaBERT: Task-Adaptive BERT Compression with Differentiable Neural
  Architecture Search](https://arxiv.org/pdf/2001.04246.pdf)** (2021-01-25)

*Daoyuan Chen, Yaliang Li, Minghui Qiu, Zhen Wang, Bofang Li, Bolin Ding, Hongbo Deng, Jun Huang, Wei Lin, Jingren Zhou*

  Large pre-trained language models such as BERT have shown their effectiveness
in various natural language processing tasks. However, the huge parameter size
makes them difficult to be deployed in real-time applications that require
quick inference with limited resources. Existing methods compress BERT into
small models while such compression is task-independent, i.e., the same
compressed BERT for all different downstream tasks. Motivated by the necessity
and benefits of task-oriented BERT compression, we propose a novel compression
method, AdaBERT, that leverages differentiable Neural Architecture Search to
automatically compress BERT into task-adaptive small models for specific tasks.
We incorporate a task-oriented knowledge distillation loss to provide search
hints and an efficiency-aware loss as search constraints, which enables a good
trade-off between efficiency and effectiveness for task-adaptive BERT
compression. We evaluate AdaBERT on several NLP tasks, and the results
demonstrate that those task-adaptive compressed models are 12.7x to 29.3x
faster than BERT in inference time and 11.5x to 17.0x smaller in terms of
parameter size, while comparable performance is maintained.


---

**[104. [2106.03181] Transient Chaos in BERT](https://arxiv.org/pdf/2106.03181.pdf)** (2022-12-06)

*Katsuma Inoue, Soh Ohara, Yasuo Kuniyoshi, Kohei Nakajima*

  Language is an outcome of our complex and dynamic human-interactions and the
technique of natural language processing (NLP) is hence built on human
linguistic activities. Bidirectional Encoder Representations from Transformers
(BERT) has recently gained its popularity by establishing the state-of-the-art
scores in several NLP benchmarks. A Lite BERT (ALBERT) is literally
characterized as a lightweight version of BERT, in which the number of BERT
parameters is reduced by repeatedly applying the same neural network called
Transformer's encoder layer. By pre-training the parameters with a massive
amount of natural language data, ALBERT can convert input sentences into
versatile high-dimensional vectors potentially capable of solving multiple NLP
tasks. In that sense, ALBERT can be regarded as a well-designed
high-dimensional dynamical system whose operator is the Transformer's encoder,
and essential structures of human language are thus expected to be encapsulated
in its dynamics. In this study, we investigated the embedded properties of
ALBERT to reveal how NLP tasks are effectively solved by exploiting its
dynamics. We thereby aimed to explore the nature of human language from the
dynamical expressions of the NLP model. Our short-term analysis clarified that
the pre-trained model stably yields trajectories with higher dimensionality,
which would enhance the expressive capacity required for NLP tasks. Also, our
long-term analysis revealed that ALBERT intrinsically shows transient chaos, a
typical nonlinear phenomenon showing chaotic dynamics only in its transient,
and the pre-trained ALBERT model tends to produce the chaotic trajectory for a
significantly longer time period compared to a randomly-initialized one. Our
results imply that local chaoticity would contribute to improving NLP
performance, uncovering a novel aspect in the role of chaotic dynamics in human
language behaviors.


---

**[105. [2309.02003] Bayesian Phase Search for Probabilistic Amplitude Shaping](https://arxiv.org/pdf/2309.02003.pdf)** (2023-09-11)

*Mohammad Taha Askari, Lutz Lampe*

  We introduce a Bayesian carrier phase recovery (CPR) algorithm which is
robust against low signal-to-noise ratio scenarios. It is therefore effective
for phase recovery for probabilistic amplitude shaping (PAS). Results validate
that the new algorithm overcomes the degradation experienced by blind
phase-search CPR for PAS.


---

**[106. [2405.06721] Kolmogorov-Arnold Networks are Radial Basis Function Networks](https://arxiv.org/pdf/2405.06721.pdf)** (2024-05-14)

*Ziyao Li*

  This short paper is a fast proof-of-concept that the 3-order B-splines used
in Kolmogorov-Arnold Networks (KANs) can be well approximated by Gaussian
radial basis functions. Doing so leads to FastKAN, a much faster implementation
of KAN which is also a radial basis function (RBF) network.


---

**[107. [2101.06326] Grid Search Hyperparameter Benchmarking of BERT, ALBERT, and LongFormer
  on DuoRC](https://arxiv.org/pdf/2101.06326.pdf)** (2021-03-30)

*Alex John Quijano, Sam Nguyen, Juanita Ordonez*

  The purpose of this project is to evaluate three language models named BERT,
ALBERT, and LongFormer on the Question Answering dataset called DuoRC. The
language model task has two inputs, a question, and a context. The context is a
paragraph or an entire document while the output is the answer based on the
context. The goal is to perform grid search hyperparameter fine-tuning using
DuoRC. Pretrained weights of the models are taken from the Huggingface library.
Different sets of hyperparameters are used to fine-tune the models using two
versions of DuoRC which are the SelfRC and the ParaphraseRC. The results show
that the ALBERT (pretrained using the SQuAD1 dataset) has an F1 score of 76.4
and an accuracy score of 68.52 after fine-tuning on the SelfRC dataset. The
Longformer model (pretrained using the SQuAD and SelfRC datasets) has an F1
score of 52.58 and an accuracy score of 46.60 after fine-tuning on the
ParaphraseRC dataset. The current results outperformed the results from the
previous model by DuoRC.


---

**[108. [2204.03839] Infusing Knowledge from Wikipedia to Enhance Stance Detection](https://arxiv.org/pdf/2204.03839.pdf)** (2022-04-11)

*Zihao He, Negar Mokhberian, Kristina Lerman*

  Stance detection infers a text author's attitude towards a target. This is
challenging when the model lacks background knowledge about the target. Here,
we show how background knowledge from Wikipedia can help enhance the
performance on stance detection. We introduce Wikipedia Stance Detection BERT
(WS-BERT) that infuses the knowledge into stance encoding. Extensive results on
three benchmark datasets covering social media discussions and online debates
indicate that our model significantly outperforms the state-of-the-art methods
on target-specific stance detection, cross-target stance detection, and
zero/few-shot stance detection.


---

**[109. [2407.12376] Deep Learning-based Sentiment Analysis of Olympics Tweets](https://arxiv.org/pdf/2407.12376.pdf)** (2024-07-18)

*Indranil Bandyopadhyay, Rahul Karmakar*

  Sentiment analysis (SA), is an approach of natural language processing (NLP)
for determining a text's emotional tone by analyzing subjective information
such as views, feelings, and attitudes toward specific topics, products,
services, events, or experiences. This study attempts to develop an advanced
deep learning (DL) model for SA to understand global audience emotions through
tweets in the context of the Olympic Games. The findings represent global
attitudes around the Olympics and contribute to advancing the SA models. We
have used NLP for tweet pre-processing and sophisticated DL models for arguing
with SA, this research enhances the reliability and accuracy of sentiment
classification. The study focuses on data selection, preprocessing,
visualization, feature extraction, and model building, featuring a baseline
Na\"ive Bayes (NB) model and three advanced DL models: Convolutional Neural
Network (CNN), Bidirectional Long Short-Term Memory (BiLSTM), and Bidirectional
Encoder Representations from Transformers (BERT). The results of the
experiments show that the BERT model can efficiently classify sentiments
related to the Olympics, achieving the highest accuracy of 99.23%.


---

**[110. [2106.03484] BERTGEN: Multi-task Generation through BERT](https://arxiv.org/pdf/2106.03484.pdf)** (2021-06-08)

*Faidon Mitzalis, Ozan Caglayan, Pranava Madhyastha, Lucia Specia*

  We present BERTGEN, a novel generative, decoder-only model which extends BERT
by fusing multimodal and multilingual pretrained models VL-BERT and M-BERT,
respectively. BERTGEN is auto-regressively trained for language generation
tasks, namely image captioning, machine translation and multimodal machine
translation, under a multitask setting. With a comprehensive set of
evaluations, we show that BERTGEN outperforms many strong baselines across the
tasks explored. We also show BERTGEN's ability for zero-shot language
generation, where it exhibits competitive performance to supervised
counterparts. Finally, we conduct ablation studies which demonstrate that
BERTGEN substantially benefits from multi-tasking and effectively transfers
relevant inductive biases from the pre-trained models.


---

**[111. [2110.03142] A Comparative Study of Transformer-Based Language Models on Extractive
  Question Answering](https://arxiv.org/pdf/2110.03142.pdf)** (2021-10-08)

*Kate Pearce, Tiffany Zhan, Aneesh Komanduri, Justin Zhan*

  Question Answering (QA) is a task in natural language processing that has
seen considerable growth after the advent of transformers. There has been a
surge in QA datasets that have been proposed to challenge natural language
processing models to improve human and existing model performance. Many
pre-trained language models have proven to be incredibly effective at the task
of extractive question answering. However, generalizability remains as a
challenge for the majority of these models. That is, some datasets require
models to reason more than others. In this paper, we train various pre-trained
language models and fine-tune them on multiple question answering datasets of
varying levels of difficulty to determine which of the models are capable of
generalizing the most comprehensively across different datasets. Further, we
propose a new architecture, BERT-BiLSTM, and compare it with other language
models to determine if adding more bidirectionality can improve model
performance. Using the F1-score as our metric, we find that the RoBERTa and
BART pre-trained models perform the best across all datasets and that our
BERT-BiLSTM model outperforms the baseline BERT model.


---

**[112. [2403.15458] Fine-Tuning Pre-trained Language Models to Detect In-Game Trash Talks](https://arxiv.org/pdf/2403.15458.pdf)** (2024-03-28)

*Daniel Fesalbon, Arvin De La Cruz, Marvin Mallari, Nelson Rodelas*

  Common problems in playing online mobile and computer games were related to
toxic behavior and abusive communication among players. Based on different
reports and studies, the study also discusses the impact of online hate speech
and toxicity on players' in-game performance and overall well-being. This study
investigates the capability of pre-trained language models to classify or
detect trash talk or toxic in-game messages The study employs and evaluates the
performance of pre-trained BERT and GPT language models in detecting toxicity
within in-game chats. Using publicly available APIs, in-game chat data from
DOTA 2 game matches were collected, processed, reviewed, and labeled as
non-toxic, mild (toxicity), and toxic. The study was able to collect around two
thousand in-game chats to train and test BERT (Base-uncased), BERT
(Large-uncased), and GPT-3 models. Based on the three models' state-of-the-art
performance, this study concludes pre-trained language models' promising
potential for addressing online hate speech and in-game insulting trash talk.


---

**[113. [2201.02080] BERN2: an advanced neural biomedical named entity recognition and
  normalization tool](https://arxiv.org/pdf/2201.02080.pdf)** (2022-10-07)

*Mujeen Sung, Minbyul Jeong, Yonghwa Choi, Donghyeon Kim, Jinhyuk Lee, Jaewoo Kang*

  In biomedical natural language processing, named entity recognition (NER) and
named entity normalization (NEN) are key tasks that enable the automatic
extraction of biomedical entities (e.g. diseases and drugs) from the
ever-growing biomedical literature. In this article, we present BERN2 (Advanced
Biomedical Entity Recognition and Normalization), a tool that improves the
previous neural network-based NER tool by employing a multi-task NER model and
neural network-based NEN models to achieve much faster and more accurate
inference. We hope that our tool can help annotate large-scale biomedical texts
for various tasks such as biomedical knowledge graph construction.


---

**[114. [2105.01044] Goldilocks: Just-Right Tuning of BERT for Technology-Assisted Review](https://arxiv.org/pdf/2105.01044.pdf)** (2022-01-21)

*Eugene Yang, Sean MacAvaney, David D. Lewis, Ophir Frieder*

  Technology-assisted review (TAR) refers to iterative active learning
workflows for document review in high recall retrieval (HRR) tasks. TAR
research and most commercial TAR software have applied linear models such as
logistic regression to lexical features. Transformer-based models with
supervised tuning are known to improve effectiveness on many text
classification tasks, suggesting their use in TAR. We indeed find that the
pre-trained BERT model reduces review cost by 10% to 15% in TAR workflows
simulated on the RCV1-v2 newswire collection. In contrast, we likewise
determined that linear models outperform BERT for simulated legal discovery
topics on the Jeb Bush e-mail collection. This suggests the match between
transformer pre-training corpora and the task domain is of greater significance
than generally appreciated. Additionally, we show that just-right language
model fine-tuning on the task collection before starting active learning is
critical. Too little or too much fine-tuning hinders performance, worse than
that of linear models, even for a favorable corpus such as RCV1-v2.


---

**[115. [2211.11304] TCBERT: A Technical Report for Chinese Topic Classification BERT](https://arxiv.org/pdf/2211.11304.pdf)** (2022-11-22)

*Ting Han, Kunhao Pan, Xinyu Chen, Dingjie Song, Yuchen Fan, Xinyu Gao, Ruyi Gan, Jiaxing Zhang*

  Bidirectional Encoder Representations from Transformers or
BERT~\cite{devlin-etal-2019-bert} has been one of the base models for various
NLP tasks due to its remarkable performance. Variants customized for different
languages and tasks are proposed to further improve the performance. In this
work, we investigate supervised continued
pre-training~\cite{gururangan-etal-2020-dont} on BERT for Chinese topic
classification task. Specifically, we incorporate prompt-based learning and
contrastive learning into the pre-training. To adapt to the task of Chinese
topic classification, we collect around 2.1M Chinese data spanning various
topics. The pre-trained Chinese Topic Classification BERTs (TCBERTs) with
different parameter sizes are open-sourced at
\url{https://huggingface.co/IDEA-CCNL}.


---

**[116. [2312.04463] Leveraging Transformer-based Language Models to Automate Requirements
  Satisfaction Assessment](https://arxiv.org/pdf/2312.04463.pdf)** (2023-12-08)

*Amrit Poudel, Jinfeng Lin, Jane Cleland-Huang*

  Requirements Satisfaction Assessment (RSA) evaluates whether the set of
design elements linked to a single requirement provide sufficient coverage of
that requirement -- typically meaning that all concepts in the requirement are
addressed by at least one of the design elements. RSA is an important software
engineering activity for systems with any form of hierarchical decomposition --
especially safety or mission critical ones. In previous studies, researchers
used basic Information Retrieval (IR) models to decompose requirements and
design elements into chunks, and then evaluated the extent to which chunks of
design elements covered all chunks in the requirement. However, results had low
accuracy because many critical concepts that extend across the entirety of the
sentence were not well represented when the sentence was parsed into
independent chunks. In this paper we leverage recent advances in natural
language processing to deliver significantly more accurate results. We propose
two major architectures: Satisfaction BERT (Sat-BERT), and Dual-Satisfaction
BERT (DSat-BERT), along with their multitask learning variants to improve
satisfaction assessments. We perform RSA on five different datasets and compare
results from our variants against the chunk-based legacy approach. All
BERT-based models significantly outperformed the legacy baseline, and Sat-BERT
delivered the best results returning an average improvement of 124.75% in Mean
Average Precision.


---

**[117. [2106.07932] Medical Code Prediction from Discharge Summary: Document to Sequence
  BERT using Sequence Attention](https://arxiv.org/pdf/2106.07932.pdf)** (2021-11-12)

*Tak-Sung Heo, Yongmin Yoo, Yeongjoon Park, Byeong-Cheol Jo, Kyungsun Kim*

  Clinical notes are unstructured text generated by clinicians during patient
encounters. Clinical notes are usually accompanied by a set of metadata codes
from the International Classification of Diseases(ICD). ICD code is an
important code used in various operations, including insurance, reimbursement,
medical diagnosis, etc. Therefore, it is important to classify ICD codes
quickly and accurately. However, annotating these codes is costly and
time-consuming. So we propose a model based on bidirectional encoder
representations from transformers (BERT) using the sequence attention method
for automatic ICD code assignment. We evaluate our approach on the medical
information mart for intensive care III (MIMIC-III) benchmark dataset. Our
model achieved performance of macro-averaged F1: 0.62898 and micro-averaged F1:
0.68555 and is performing better than a performance of the state-of-the-art
model using the MIMIC-III dataset. The contribution of this study proposes a
method of using BERT that can be applied to documents and a sequence attention
method that can capture important sequence in-formation appearing in documents.


---

**[118. [2006.08097] FinBERT: A Pretrained Language Model for Financial Communications](https://arxiv.org/pdf/2006.08097.pdf)** (2020-07-10)

*Yi Yang, Mark Christopher Siy UY, Allen Huang*

  Contextual pretrained language models, such as BERT (Devlin et al., 2019),
have made significant breakthrough in various NLP tasks by training on large
scale of unlabeled text re-sources.Financial sector also accumulates large
amount of financial communication text.However, there is no pretrained finance
specific language models available. In this work,we address the need by
pretraining a financial domain specific BERT models, FinBERT, using a large
scale of financial communication corpora. Experiments on three financial
sentiment classification tasks confirm the advantage of FinBERT over generic
domain BERT model. The code and pretrained models are available at
https://github.com/yya518/FinBERT. We hope this will be useful for
practitioners and researchers working on financial NLP tasks.


---

**[119. [2007.14477] GUIR at SemEval-2020 Task 12: Domain-Tuned Contextualized Models for
  Offensive Language Detection](https://arxiv.org/pdf/2007.14477.pdf)** (2020-07-30)

*Sajad Sotudeh, Tong Xiang, Hao-Ren Yao, Sean MacAvaney, Eugene Yang, Nazli Goharian, Ophir Frieder*

  Offensive language detection is an important and challenging task in natural
language processing. We present our submissions to the OffensEval 2020 shared
task, which includes three English sub-tasks: identifying the presence of
offensive language (Sub-task A), identifying the presence of target in
offensive language (Sub-task B), and identifying the categories of the target
(Sub-task C). Our experiments explore using a domain-tuned contextualized
language model (namely, BERT) for this task. We also experiment with different
components and configurations (e.g., a multi-view SVM) stacked upon BERT models
for specific sub-tasks. Our submissions achieve F1 scores of 91.7% in Sub-task
A, 66.5% in Sub-task B, and 63.2% in Sub-task C. We perform an ablation study
which reveals that domain tuning considerably improves the classification
performance. Furthermore, error analysis shows common misclassification errors
made by our model and outlines research directions for future.


---

**[120. [2010.12077] Summarizing Utterances from Japanese Assembly Minutes using Political
  Sentence-BERT-based Method for QA Lab-PoliInfo-2 Task of NTCIR-15](https://arxiv.org/pdf/2010.12077.pdf)** (2020-10-26)

*Daiki Shirafuji, Hiromichi Kameya, Rafal Rzepka, Kenji Araki*

  There are many discussions held during political meetings, and a large number
of utterances for various topics is included in their transcripts. We need to
read all of them if we want to follow speakers\' intentions or opinions about a
given topic. To avoid such a costly and time-consuming process to grasp often
longish discussions, NLP researchers work on generating concise summaries of
utterances. Summarization subtask in QA Lab-PoliInfo-2 task of the NTCIR-15
addresses this problem for Japanese utterances in assembly minutes, and our
team (SKRA) participated in this subtask. As a first step for summarizing
utterances, we created a new pre-trained sentence embedding model, i.e. the
Japanese Political Sentence-BERT. With this model, we summarize utterances
without labelled data. This paper describes our approach to solving the task
and discusses its results.


---

**[121. [2106.10899] Ad Text Classification with Transformer-Based Natural Language
  Processing Methods](https://arxiv.org/pdf/2106.10899.pdf)** (2021-06-24)

*Umut Özdil, Büşra Arslan, D. Emre Taşar, Gökçe Polat, Şükrü Ozan*

  In this study, a natural language processing-based (NLP-based) method is
proposed for the sector-wise automatic classification of ad texts created on
online advertising platforms. Our data set consists of approximately 21,000
labeled advertising texts from 12 different sectors. In the study, the
Bidirectional Encoder Representations from Transformers (BERT) model, which is
a transformer-based language model that is recently used in fields such as text
classification in the natural language processing literature, was used. The
classification efficiencies obtained using a pre-trained BERT model for the
Turkish language are shown in detail.


---

**[122. [2204.08922] Feature Structure Distillation with Centered Kernel Alignment in BERT
  Transferring](https://arxiv.org/pdf/2204.08922.pdf)** (2024-10-28)

*Hee-Jun Jung, Doyeon Kim, Seung-Hoon Na, Kangil Kim*

  Knowledge distillation is an approach to transfer information on
representations from a teacher to a student by reducing their difference. A
challenge of this approach is to reduce the flexibility of the student's
representations inducing inaccurate learning of the teacher's knowledge. To
resolve it in transferring, we investigate distillation of structures of
representations specified to three types: intra-feature, local inter-feature,
global inter-feature structures. To transfer them, we introduce feature
structure distillation methods based on the Centered Kernel Alignment, which
assigns a consistent value to similar features structures and reveals more
informative relations. In particular, a memory-augmented transfer method with
clustering is implemented for the global structures. The methods are
empirically analyzed on the nine tasks for language understanding of the GLUE
dataset with Bidirectional Encoder Representations from Transformers (BERT),
which is a representative neural language model. In the results, the proposed
methods effectively transfer the three types of structures and improve
performance compared to state-of-the-art distillation methods. Indeed, the code
for the methods is available in https://github.com/maroo-sky/FSD.


---

**[123. [2107.00175] Elbert: Fast Albert with Confidence-Window Based Early Exit](https://arxiv.org/pdf/2107.00175.pdf)** (2021-07-02)

*Keli Xie, Siyuan Lu, Meiqi Wang, Zhongfeng Wang*

  Despite the great success in Natural Language Processing (NLP) area, large
pre-trained language models like BERT are not well-suited for
resource-constrained or real-time applications owing to the large number of
parameters and slow inference speed. Recently, compressing and accelerating
BERT have become important topics. By incorporating a parameter-sharing
strategy, ALBERT greatly reduces the number of parameters while achieving
competitive performance. Nevertheless, ALBERT still suffers from a long
inference time. In this work, we propose the ELBERT, which significantly
improves the average inference speed compared to ALBERT due to the proposed
confidence-window based early exit mechanism, without introducing additional
parameters or extra training overhead. Experimental results show that ELBERT
achieves an adaptive inference speedup varying from 2$\times$ to 10$\times$
with negligible accuracy degradation compared to ALBERT on various datasets.
Besides, ELBERT achieves higher accuracy than existing early exit methods used
for accelerating BERT under the same computation cost. Furthermore, to
understand the principle of the early exit mechanism, we also visualize the
decision-making process of it in ELBERT.


---

**[124. [2303.18138] BERT4ETH: A Pre-trained Transformer for Ethereum Fraud Detection](https://arxiv.org/pdf/2303.18138.pdf)** (2023-11-01)

*Sihao Hu, Zhen Zhang, Bingqiao Luo, Shengliang Lu, Bingsheng He, Ling Liu*

  As various forms of fraud proliferate on Ethereum, it is imperative to
safeguard against these malicious activities to protect susceptible users from
being victimized. While current studies solely rely on graph-based fraud
detection approaches, it is argued that they may not be well-suited for dealing
with highly repetitive, skew-distributed and heterogeneous Ethereum
transactions. To address these challenges, we propose BERT4ETH, a universal
pre-trained Transformer encoder that serves as an account representation
extractor for detecting various fraud behaviors on Ethereum. BERT4ETH features
the superior modeling capability of Transformer to capture the dynamic
sequential patterns inherent in Ethereum transactions, and addresses the
challenges of pre-training a BERT model for Ethereum with three practical and
effective strategies, namely repetitiveness reduction, skew alleviation and
heterogeneity modeling. Our empirical evaluation demonstrates that BERT4ETH
outperforms state-of-the-art methods with significant enhancements in terms of
the phishing account detection and de-anonymization tasks. The code for
BERT4ETH is available at: https://github.com/git-disl/BERT4ETH.


---

**[125. [2004.04124] LadaBERT: Lightweight Adaptation of BERT through Hybrid Model
  Compression](https://arxiv.org/pdf/2004.04124.pdf)** (2020-10-22)

*Yihuan Mao, Yujing Wang, Chufan Wu, Chen Zhang, Yang Wang, Yaming Yang, Quanlu Zhang, Yunhai Tong, Jing Bai*

  BERT is a cutting-edge language representation model pre-trained by a large
corpus, which achieves superior performances on various natural language
understanding tasks. However, a major blocking issue of applying BERT to online
services is that it is memory-intensive and leads to unsatisfactory latency of
user requests, raising the necessity of model compression. Existing solutions
leverage the knowledge distillation framework to learn a smaller model that
imitates the behaviors of BERT. However, the training procedure of knowledge
distillation is expensive itself as it requires sufficient training data to
imitate the teacher model. In this paper, we address this issue by proposing a
hybrid solution named LadaBERT (Lightweight adaptation of BERT through hybrid
model compression), which combines the advantages of different model
compression methods, including weight pruning, matrix factorization and
knowledge distillation. LadaBERT achieves state-of-the-art accuracy on various
public datasets while the training overheads can be reduced by an order of
magnitude.


---

**[126. [2104.02138] Semantic Distance: A New Metric for ASR Performance Analysis Towards
  Spoken Language Understanding](https://arxiv.org/pdf/2104.02138.pdf)** (2021-04-07)

*Suyoun Kim, Abhinav Arora, Duc Le, Ching-Feng Yeh, Christian Fuegen, Ozlem Kalinli, Michael L. Seltzer*

  Word Error Rate (WER) has been the predominant metric used to evaluate the
performance of automatic speech recognition (ASR) systems. However, WER is
sometimes not a good indicator for downstream Natural Language Understanding
(NLU) tasks, such as intent recognition, slot filling, and semantic parsing in
task-oriented dialog systems. This is because WER takes into consideration only
literal correctness instead of semantic correctness, the latter of which is
typically more important for these downstream tasks. In this study, we propose
a novel Semantic Distance (SemDist) measure as an alternative evaluation metric
for ASR systems to address this issue. We define SemDist as the distance
between a reference and hypothesis pair in a sentence-level embedding space. To
represent the reference and hypothesis as a sentence embedding, we exploit
RoBERTa, a state-of-the-art pre-trained deep contextualized language model
based on the transformer architecture. We demonstrate the effectiveness of our
proposed metric on various downstream tasks, including intent recognition,
semantic parsing, and named entity recognition.


---

**[127. [2403.12212] Evaluating Named Entity Recognition: A comparative analysis of mono- and
  multilingual transformer models on a novel Brazilian corporate earnings call
  transcripts dataset](https://arxiv.org/pdf/2403.12212.pdf)** (2024-09-02)

*Ramon Abilio, Guilherme Palermo Coelho, Ana Estela Antunes da Silva*

  Since 2018, when the Transformer architecture was introduced, Natural
Language Processing has gained significant momentum with pre-trained
Transformer-based models that can be fine-tuned for various tasks. Most models
are pre-trained on large English corpora, making them less applicable to other
languages, such as Brazilian Portuguese. In our research, we identified two
models pre-trained in Brazilian Portuguese (BERTimbau and PTT5) and two
multilingual models (mBERT and mT5). BERTimbau and mBERT use only the Encoder
module, while PTT5 and mT5 use both the Encoder and Decoder. Our study aimed to
evaluate their performance on a financial Named Entity Recognition (NER) task
and determine the computational requirements for fine-tuning and inference. To
this end, we developed the Brazilian Financial NER (BraFiNER) dataset,
comprising sentences from Brazilian banks' earnings calls transcripts annotated
using a weakly supervised approach. Additionally, we introduced a novel
approach that reframes the token classification task as a text generation
problem. After fine-tuning the models, we evaluated them using performance and
error metrics. Our findings reveal that BERT-based models consistently
outperform T5-based models. While the multilingual models exhibit comparable
macro F1-scores, BERTimbau demonstrates superior performance over PTT5. In
terms of error metrics, BERTimbau outperforms the other models. We also
observed that PTT5 and mT5 generated sentences with changes in monetary and
percentage values, highlighting the importance of accuracy and consistency in
the financial domain. Our findings provide insights into the differing
performance of BERT- and T5-based models for the NER task.


---

**[128. [2301.11069] BERT-Embedding and Citation Network Analysis based Query Expansion
  Technique for Scholarly Search](https://arxiv.org/pdf/2301.11069.pdf)** (2023-01-27)

*Shah Khalid, Shah Khusro, Aftab Alam, Abdul Wahid*

  The enormous growth of research publications has made it challenging for
academic search engines to bring the most relevant papers against the given
search query. Numerous solutions have been proposed over the years to improve
the effectiveness of academic search, including exploiting query expansion and
citation analysis. Query expansion techniques mitigate the mismatch between the
language used in a query and indexed documents. However, these techniques can
suffer from introducing non-relevant information while expanding the original
query. Recently, contextualized model BERT to document retrieval has been quite
successful in query expansion. Motivated by such issues and inspired by the
success of BERT, this paper proposes a novel approach called QeBERT. QeBERT
exploits BERT-based embedding and Citation Network Analysis (CNA) in query
expansion for improving scholarly search. Specifically, we use the
context-aware BERT-embedding and CNA for query expansion in Pseudo-Relevance
Feedback (PRF) fash-ion. Initial experimental results on the ACL dataset show
that BERT-embedding can provide a valuable augmentation to query expansion and
improve search relevance when combined with CNA.


---

**[129. [2306.12245] Bidirectional End-to-End Learning of Retriever-Reader Paradigm for
  Entity Linking](https://arxiv.org/pdf/2306.12245.pdf)** (2024-03-21)

*Yinghui Li, Yong Jiang, Yangning Li, Xingyu Lu, Pengjun Xie, Ying Shen, Hai-Tao Zheng*

  Entity Linking (EL) is a fundamental task for Information Extraction and
Knowledge Graphs. The general form of EL (i.e., end-to-end EL) aims to first
find mentions in the given input document and then link the mentions to
corresponding entities in a specific knowledge base. Recently, the paradigm of
retriever-reader promotes the progress of end-to-end EL, benefiting from the
advantages of dense entity retrieval and machine reading comprehension.
However, the existing study only trains the retriever and the reader separately
in a pipeline manner, which ignores the benefit that the interaction between
the retriever and the reader can bring to the task. To advance the
retriever-reader paradigm to perform more perfectly on end-to-end EL, we
propose BEER$^2$, a Bidirectional End-to-End training framework for Retriever
and Reader. Through our designed bidirectional end-to-end training, BEER$^2$
guides the retriever and the reader to learn from each other, make progress
together, and ultimately improve EL performance. Extensive experiments on
benchmarks of multiple domains demonstrate the effectiveness of our proposed
BEER$^2$.


---

**[130. [2011.02788] NUAA-QMUL at SemEval-2020 Task 8: Utilizing BERT and DenseNet for
  Internet Meme Emotion Analysis](https://arxiv.org/pdf/2011.02788.pdf)** (2020-11-10)

*Xiaoyu Guo, Jing Ma, Arkaitz Zubiaga*

  This paper describes our contribution to SemEval 2020 Task 8: Memotion
Analysis. Our system learns multi-modal embeddings from text and images in
order to classify Internet memes by sentiment. Our model learns text embeddings
using BERT and extracts features from images with DenseNet, subsequently
combining both features through concatenation. We also compare our results with
those produced by DenseNet, ResNet, BERT, and BERT-ResNet. Our results show
that image classification models have the potential to help classifying memes,
with DenseNet outperforming ResNet. Adding text features is however not always
helpful for Memotion Analysis.


---

**[131. [2108.04539] BROS: A Pre-trained Language Model Focusing on Text and Layout for
  Better Key Information Extraction from Documents](https://arxiv.org/pdf/2108.04539.pdf)** (2022-04-06)

*Teakgyu Hong, Donghyun Kim, Mingi Ji, Wonseok Hwang, Daehyun Nam, Sungrae Park*

  Key information extraction (KIE) from document images requires understanding
the contextual and spatial semantics of texts in two-dimensional (2D) space.
Many recent studies try to solve the task by developing pre-trained language
models focusing on combining visual features from document images with texts
and their layout. On the other hand, this paper tackles the problem by going
back to the basic: effective combination of text and layout. Specifically, we
propose a pre-trained language model, named BROS (BERT Relying On Spatiality),
that encodes relative positions of texts in 2D space and learns from unlabeled
documents with area-masking strategy. With this optimized training scheme for
understanding texts in 2D space, BROS shows comparable or better performance
compared to previous methods on four KIE benchmarks (FUNSD, SROIE*, CORD, and
SciTSR) without relying on visual features. This paper also reveals two
real-world challenges in KIE tasks-(1) minimizing the error from incorrect text
ordering and (2) efficient learning from fewer downstream examples-and
demonstrates the superiority of BROS over previous methods. Code is available
at https://github.com/clovaai/bros.


---

**[132. [2006.01432] BERT Based Multilingual Machine Comprehension in English and Hindi](https://arxiv.org/pdf/2006.01432.pdf)** (2020-06-03)

*Somil Gupta, Nilesh Khade*

  Multilingual Machine Comprehension (MMC) is a Question-Answering (QA)
sub-task that involves quoting the answer for a question from a given snippet,
where the question and the snippet can be in different languages. Recently
released multilingual variant of BERT (m-BERT), pre-trained with 104 languages,
has performed well in both zero-shot and fine-tuned settings for multilingual
tasks; however, it has not been used for English-Hindi MMC yet. We, therefore,
present in this article, our experiments with m-BERT for MMC in zero-shot,
mono-lingual (e.g. Hindi Question-Hindi Snippet) and cross-lingual (e.g.
English QuestionHindi Snippet) fine-tune setups. These model variants are
evaluated on all possible multilingual settings and results are compared
against the current state-of-the-art sequential QA system for these languages.
Experiments show that m-BERT, with fine-tuning, improves performance on all
evaluation settings across both the datasets used by the prior model, therefore
establishing m-BERT based MMC as the new state-of-the-art for English and
Hindi. We also publish our results on an extended version of the recently
released XQuAD dataset, which we propose to use as the evaluation benchmark for
future research.


---

**[133. [2405.01575] Software Mention Recognition with a Three-Stage Framework Based on
  BERTology Models at SOMD 2024](https://arxiv.org/pdf/2405.01575.pdf)** (2024-05-06)

*Thuy Nguyen Thi, Anh Nguyen Viet, Thin Dang Van, Ngan Nguyen Luu Thuy*

  This paper describes our systems for the sub-task I in the Software Mention
Detection in Scholarly Publications shared-task. We propose three approaches
leveraging different pre-trained language models (BERT, SciBERT, and XLM-R) to
tackle this challenge. Our bestperforming system addresses the named entity
recognition (NER) problem through a three-stage framework. (1) Entity Sentence
Classification - classifies sentences containing potential software mentions;
(2) Entity Extraction - detects mentions within classified sentences; (3)
Entity Type Classification - categorizes detected mentions into specific
software types. Experiments on the official dataset demonstrate that our
three-stage framework achieves competitive performance, surpassing both other
participating teams and our alternative approaches. As a result, our framework
based on the XLM-R-based model achieves a weighted F1-score of 67.80%,
delivering our team the 3rd rank in Sub-task I for the Software Mention
Recognition task.


---

**[134. [2405.15896] Enhancing Augmentative and Alternative Communication with Card
  Prediction and Colourful Semantics](https://arxiv.org/pdf/2405.15896.pdf)** (2024-05-28)

*Jayr Pereira, Francisco Rodrigues, Jaylton Pereira, Cleber Zanchettin, Robson Fidalgo*

  This paper presents an approach to enhancing Augmentative and Alternative
Communication (AAC) systems by integrating Colourful Semantics (CS) with
transformer-based language models specifically tailored for Brazilian
Portuguese. We introduce an adapted BERT model, BERTptCS, which incorporates
the CS framework for improved prediction of communication cards. The primary
aim is to enhance the accuracy and contextual relevance of communication card
predictions, which are essential in AAC systems for individuals with complex
communication needs (CCN). We compared BERTptCS with a baseline model,
BERTptAAC, which lacks CS integration. Our results demonstrate that BERTptCS
significantly outperforms BERTptAAC in various metrics, including top-k
accuracy, Mean Reciprocal Rank (MRR), and Entropy@K. Integrating CS into the
language model improves prediction accuracy and offers a more intuitive and
contextual understanding of user inputs, facilitating more effective
communication.


---

**[135. [1902.05650] Asynchronous Coagent Networks](https://arxiv.org/pdf/1902.05650.pdf)** (2020-08-11)

*James E. Kostas, Chris Nota, Philip S. Thomas*

  Coagent policy gradient algorithms (CPGAs) are reinforcement learning
algorithms for training a class of stochastic neural networks called coagent
networks. In this work, we prove that CPGAs converge to locally optimal
policies. Additionally, we extend prior theory to encompass asynchronous and
recurrent coagent networks. These extensions facilitate the straightforward
design and analysis of hierarchical reinforcement learning algorithms like the
option-critic, and eliminate the need for complex derivations of customized
learning rules for these algorithms.


---

**[136. [2503.05060] ModernBERT is More Efficient than Conventional BERT for Chest CT
  Findings Classification in Japanese Radiology Reports](https://arxiv.org/pdf/2503.05060.pdf)** (2025-03-10)

*Yosuke Yamagishi, Tomohiro Kikuchi, Shouhei Hanaoka, Takeharu Yoshikawa, Osamu Abe*

  Objective: This study aims to evaluate and compare the performance of two
Japanese language models-conventional Bidirectional Encoder Representations
from Transformers (BERT) and the newer ModernBERT-in classifying findings from
chest CT reports, with a focus on tokenization efficiency, processing time, and
classification performance. Methods: We conducted a retrospective study using
the CT-RATE-JPN dataset containing 22,778 training reports and 150 test
reports. Both models were fine-tuned for multi-label classification of 18
common chest CT conditions. The training data was split in 18,222:4,556 for
training and validation. Performance was evaluated using F1 scores for each
condition and exact match accuracy across all 18 labels. Results: ModernBERT
demonstrated superior tokenization efficiency, requiring 24.0% fewer tokens per
document (258.1 vs. 339.6) compared to BERT Base. This translated to
significant performance improvements, with ModernBERT completing training in
1877.67 seconds versus BERT's 3090.54 seconds (39% reduction). ModernBERT
processed 38.82 samples per second during training (1.65x faster) and 139.90
samples per second during inference (1.66x faster). Despite these efficiency
gains, classification performance remained comparable, with ModernBERT
achieving superior F1 scores in 8 conditions, while BERT performed better in 4
conditions. Overall exact match accuracy was slightly higher for ModernBERT
(74.67% vs. 72.67%), though this difference was not statistically significant
(p=0.6291). Conclusion: ModernBERT offers substantial improvements in
tokenization efficiency and training speed without sacrificing classification
performance. These results suggest that ModernBERT is a promising candidate for
clinical applications in Japanese radiology reports analysis.


---

**[137. [2305.18315] CDJUR-BR -- A Golden Collection of Legal Document from Brazilian Justice
  with Fine-Grained Named Entities](https://arxiv.org/pdf/2305.18315.pdf)** (2023-05-31)

*Antonio Mauricio, Vladia Pinheiro, Vasco Furtado, João Araújo Monteiro Neto, Francisco das Chagas Jucá Bomfim, André Câmara Ferreira da Costa, Raquel Silveira, Nilsiton Aragão*

  A basic task for most Legal Artificial Intelligence (Legal AI) applications
is Named Entity Recognition (NER). However, texts produced in the context of
legal practice make references to entities that are not trivially recognized by
the currently available NERs. There is a lack of categorization of legislation,
jurisprudence, evidence, penalties, the roles of people in a legal process
(judge, lawyer, victim, defendant, witness), types of locations (crime
location, defendant's address), etc. In this sense, there is still a need for a
robust golden collection, annotated with fine-grained entities of the legal
domain, and which covers various documents of a legal process, such as
petitions, inquiries, complaints, decisions and sentences. In this article, we
describe the development of the Golden Collection of the Brazilian Judiciary
(CDJUR-BR) contemplating a set of fine-grained named entities that have been
annotated by experts in legal documents. The creation of CDJUR-BR followed its
own methodology that aimed to attribute a character of comprehensiveness and
robustness. Together with the CDJUR-BR repository we provided a NER based on
the BERT model and trained with the CDJUR-BR, whose results indicated the
prevalence of the CDJUR-BR.


---

**[138. [2008.02496] ConvBERT: Improving BERT with Span-based Dynamic Convolution](https://arxiv.org/pdf/2008.02496.pdf)** (2021-02-03)

*Zihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng Chen, Jiashi Feng, Shuicheng Yan*

  Pre-trained language models like BERT and its variants have recently achieved
impressive performance in various natural language understanding tasks.
However, BERT heavily relies on the global self-attention block and thus
suffers large memory footprint and computation cost. Although all its attention
heads query on the whole input sequence for generating the attention map from a
global perspective, we observe some heads only need to learn local
dependencies, which means the existence of computation redundancy. We therefore
propose a novel span-based dynamic convolution to replace these self-attention
heads to directly model local dependencies. The novel convolution heads,
together with the rest self-attention heads, form a new mixed attention block
that is more efficient at both global and local context learning. We equip BERT
with this mixed attention design and build a ConvBERT model. Experiments have
shown that ConvBERT significantly outperforms BERT and its variants in various
downstream tasks, with lower training cost and fewer model parameters.
Remarkably, ConvBERTbase model achieves 86.4 GLUE score, 0.7 higher than
ELECTRAbase, while using less than 1/4 training cost. Code and pre-trained
models will be released.


---

**[139. [2303.05388] German BERT Model for Legal Named Entity Recognition](https://arxiv.org/pdf/2303.05388.pdf)** (2023-03-10)

*Harshil Darji, Jelena Mitrović, Michael Granitzer*

  The use of BERT, one of the most popular language models, has led to
improvements in many Natural Language Processing (NLP) tasks. One such task is
Named Entity Recognition (NER) i.e. automatic identification of named entities
such as location, person, organization, etc. from a given text. It is also an
important base step for many NLP tasks such as information extraction and
argumentation mining. Even though there is much research done on NER using BERT
and other popular language models, the same is not explored in detail when it
comes to Legal NLP or Legal Tech. Legal NLP applies various NLP techniques such
as sentence similarity or NER specifically on legal data. There are only a
handful of models for NER tasks using BERT language models, however, none of
these are aimed at legal documents in German. In this paper, we fine-tune a
popular BERT language model trained on German data (German BERT) on a Legal
Entity Recognition (LER) dataset. To make sure our model is not overfitting, we
performed a stratified 10-fold cross-validation. The results we achieve by
fine-tuning German BERT on the LER dataset outperform the BiLSTM-CRF+ model
used by the authors of the same LER dataset. Finally, we make the model openly
available via HuggingFace.


---

**[140. [2204.13607] Process-BERT: A Framework for Representation Learning on Educational
  Process Data](https://arxiv.org/pdf/2204.13607.pdf)** (2022-04-29)

*Alexander Scarlatos, Christopher Brinton, Andrew Lan*

  Educational process data, i.e., logs of detailed student activities in
computerized or online learning platforms, has the potential to offer deep
insights into how students learn. One can use process data for many downstream
tasks such as learning outcome prediction and automatically delivering
personalized intervention. However, analyzing process data is challenging since
the specific format of process data varies a lot depending on different
learning/testing scenarios. In this paper, we propose a framework for learning
representations of educational process data that is applicable across many
different learning scenarios. Our framework consists of a pre-training step
that uses BERT-type objectives to learn representations from sequential process
data and a fine-tuning step that further adjusts these representations on
downstream prediction tasks. We apply our framework to the 2019 nation's report
card data mining competition dataset that consists of student problem-solving
process data and detail the specific models we use in this scenario. We conduct
both quantitative and qualitative experiments to show that our framework
results in process data representations that are both predictive and
informative.


---

**[141. [2205.12452] Sparse*BERT: Sparse Models Generalize To New tasks and Domains](https://arxiv.org/pdf/2205.12452.pdf)** (2023-04-07)

*Daniel Campos, Alexandre Marques, Tuan Nguyen, Mark Kurtz, ChengXiang Zhai*

  Large Language Models have become the core architecture upon which most
modern natural language processing (NLP) systems build. These models can
consistently deliver impressive accuracy and robustness across tasks and
domains, but their high computational overhead can make inference difficult and
expensive. To make using these models less costly, recent work has explored
leveraging structured and unstructured pruning, quantization, and distillation
to improve inference speed and decrease size. This paper studies how models
pruned using Gradual Unstructured Magnitude Pruning can transfer between
domains and tasks. Our experimentation shows that models that are pruned during
pretraining using general domain masked language models can transfer to novel
domains and tasks without extensive hyperparameter exploration or specialized
approaches. We demonstrate that our general sparse model Sparse*BERT can become
SparseBioBERT simply by pretraining the compressed architecture on unstructured
biomedical text. Moreover, we show that SparseBioBERT can match the quality of
BioBERT with only 10\% of the parameters.


---

**[142. [2102.01909] HeBERT & HebEMO: a Hebrew BERT Model and a Tool for Polarity Analysis
  and Emotion Recognition](https://arxiv.org/pdf/2102.01909.pdf)** (2022-06-28)

*Avihay Chriqui, Inbal Yahav*

  This paper introduces HeBERT and HebEMO. HeBERT is a Transformer-based model
for modern Hebrew text, which relies on a BERT (Bidirectional Encoder
Representations for Transformers) architecture. BERT has been shown to
outperform alternative architectures in sentiment analysis, and is suggested to
be particularly appropriate for MRLs. Analyzing multiple BERT specifications,
we find that while model complexity correlates with high performance on
language tasks that aim to understand terms in a sentence, a more-parsimonious
model better captures the sentiment of entire sentence. Either way, out
BERT-based language model outperforms all existing Hebrew alternatives on all
common language tasks. HebEMO is a tool that uses HeBERT to detect polarity and
extract emotions from Hebrew UGC. HebEMO is trained on a unique
Covid-19-related UGC dataset that we collected and annotated for this study.
Data collection and annotation followed an active learning procedure that aimed
to maximize predictability. We show that HebEMO yields a high F1-score of 0.96
for polarity classification. Emotion detection reaches F1-scores of 0.78-0.97
for various target emotions, with the exception of surprise, which the model
failed to capture (F1 = 0.41). These results are better than the best-reported
performance, even among English-language models of emotion detection.


---

**[143. [2303.09266] SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for
  Accelerating BERT Inference](https://arxiv.org/pdf/2303.09266.pdf)** (2023-05-09)

*Boren Hu, Yun Zhu, Jiacheng Li, Siliang Tang*

  Dynamic early exiting has been proven to improve the inference speed of the
pre-trained language model like BERT. However, all samples must go through all
consecutive layers before early exiting and more complex samples usually go
through more layers, which still exists redundant computation. In this paper,
we propose a novel dynamic early exiting combined with layer skipping for BERT
inference named SmartBERT, which adds a skipping gate and an exiting operator
into each layer of BERT. SmartBERT can adaptively skip some layers and
adaptively choose whether to exit. Besides, we propose cross-layer contrastive
learning and combine it into our training phases to boost the intermediate
layers and classifiers which would be beneficial for early exiting. To keep the
consistent usage of skipping gates between training and inference phases, we
propose a hard weight mechanism during training phase. We conduct experiments
on eight classification datasets of the GLUE benchmark. Experimental results
show that SmartBERT achieves 2-3x computation reduction with minimal accuracy
drops compared with BERT and our method outperforms previous methods in both
efficiency and accuracy. Moreover, in some complex datasets like RTE and WNLI,
we prove that the early exiting based on entropy hardly works, and the skipping
mechanism is essential for reducing computation.


---

**[144. [2001.06286] RobBERT: a Dutch RoBERTa-based Language Model](https://arxiv.org/pdf/2001.06286.pdf)** (2020-09-17)

*Pieter Delobelle, Thomas Winters, Bettina Berendt*

  Pre-trained language models have been dominating the field of natural
language processing in recent years, and have led to significant performance
gains for various complex natural language tasks. One of the most prominent
pre-trained language models is BERT, which was released as an English as well
as a multilingual version. Although multilingual BERT performs well on many
tasks, recent studies show that BERT models trained on a single language
significantly outperform the multilingual version. Training a Dutch BERT model
thus has a lot of potential for a wide range of Dutch NLP tasks. While previous
approaches have used earlier implementations of BERT to train a Dutch version
of BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch
language model called RobBERT. We measured its performance on various tasks as
well as the importance of the fine-tuning dataset size. We also evaluated the
importance of language-specific tokenizers and the model's fairness. We found
that RobBERT improves state-of-the-art results for various tasks, and
especially significantly outperforms other models when dealing with smaller
datasets. These results indicate that it is a powerful pre-trained model for a
large variety of Dutch language tasks. The pre-trained and fine-tuned models
are publicly available to support further downstream Dutch NLP applications.


---

**[145. [2308.16055] AsyncET: Asynchronous Learning for Knowledge Graph Entity Typing with
  Auxiliary Relations](https://arxiv.org/pdf/2308.16055.pdf)** (2023-08-31)

*Yun-Cheng Wang, Xiou Ge, Bin Wang, C. -C. Jay Kuo*

  Knowledge graph entity typing (KGET) is a task to predict the missing entity
types in knowledge graphs (KG). Previously, KG embedding (KGE) methods tried to
solve the KGET task by introducing an auxiliary relation, 'hasType', to model
the relationship between entities and their types. However, a single auxiliary
relation has limited expressiveness for diverse entity-type patterns. We
improve the expressiveness of KGE methods by introducing multiple auxiliary
relations in this work. Similar entity types are grouped to reduce the number
of auxiliary relations and improve their capability to model entity-type
patterns with different granularities. With the presence of multiple auxiliary
relations, we propose a method adopting an Asynchronous learning scheme for
Entity Typing, named AsyncET, which updates the entity and type embeddings
alternatively to keep the learned entity embedding up-to-date and informative
for entity type prediction. Experiments are conducted on two commonly used KGET
datasets to show that the performance of KGE methods on the KGET task can be
substantially improved by the proposed multiple auxiliary relations and
asynchronous embedding learning. Furthermore, our method has a significant
advantage over state-of-the-art methods in model sizes and time complexity.


---

**[146. [2406.09573] Analyzing Gender Polarity in Short Social Media Texts with BERT: The
  Role of Emojis and Emoticons](https://arxiv.org/pdf/2406.09573.pdf)** (2024-06-17)

*Saba Yousefian Jazi, Amir Mirzaeinia, Sina Yousefian Jazi*

  In this effort we fine tuned different models based on BERT to detect the
gender polarity of twitter accounts. We specially focused on analyzing the
effect of using emojis and emoticons in performance of our model in classifying
task. We were able to demonstrate that the use of these none word inputs
alongside the mention of other accounts in a short text format like tweet has
an impact in detecting the account holder's gender.


---

**[147. [2101.10642] Evaluation of BERT and ALBERT Sentence Embedding Performance on
  Downstream NLP Tasks](https://arxiv.org/pdf/2101.10642.pdf)** (2021-01-27)

*Hyunjin Choi, Judong Kim, Seongho Joe, Youngjune Gwon*

  Contextualized representations from a pre-trained language model are central
to achieve a high performance on downstream NLP task. The pre-trained BERT and
A Lite BERT (ALBERT) models can be fine-tuned to give state-ofthe-art results
in sentence-pair regressions such as semantic textual similarity (STS) and
natural language inference (NLI). Although BERT-based models yield the [CLS]
token vector as a reasonable sentence embedding, the search for an optimal
sentence embedding scheme remains an active research area in computational
linguistics. This paper explores on sentence embedding models for BERT and
ALBERT. In particular, we take a modified BERT network with siamese and triplet
network structures called Sentence-BERT (SBERT) and replace BERT with ALBERT to
create Sentence-ALBERT (SALBERT). We also experiment with an outer CNN
sentence-embedding network for SBERT and SALBERT. We evaluate performances of
all sentence-embedding models considered using the STS and NLI datasets. The
empirical results indicate that our CNN architecture improves ALBERT models
substantially more than BERT models for STS benchmark. Despite significantly
fewer model parameters, ALBERT sentence embedding is highly competitive to BERT
in downstream NLP evaluations.


---

**[148. [2305.12458] Infor-Coef: Information Bottleneck-based Dynamic Token Downsampling for
  Compact and Efficient language model](https://arxiv.org/pdf/2305.12458.pdf)** (2023-05-23)

*Wenxi Tan*

  The prevalence of Transformer-based pre-trained language models (PLMs) has
led to their wide adoption for various natural language processing tasks.
However, their excessive overhead leads to large latency and computational
costs. The statically compression methods allocate fixed computation to
different samples, resulting in redundant computation. The dynamic token
pruning method selectively shortens the sequences but are unable to change the
model size and hardly achieve the speedups as static pruning. In this paper, we
propose a model accelaration approaches for large language models that
incorporates dynamic token downsampling and static pruning, optimized by the
information bottleneck loss. Our model, Infor-Coef, achieves an 18x FLOPs
speedup with an accuracy degradation of less than 8\% compared to BERT. This
work provides a promising approach to compress and accelerate transformer-based
models for NLP tasks.


---

**[149. [2103.16110] Kaleido-BERT: Vision-Language Pre-training on Fashion Domain](https://arxiv.org/pdf/2103.16110.pdf)** (2021-04-16)

*Mingchen Zhuge, Dehong Gao, Deng-Ping Fan, Linbo Jin, Ben Chen, Haoming Zhou, Minghui Qiu, Ling Shao*

  We present a new vision-language (VL) pre-training model dubbed Kaleido-BERT,
which introduces a novel kaleido strategy for fashion cross-modality
representations from transformers. In contrast to random masking strategy of
recent VL models, we design alignment guided masking to jointly focus more on
image-text semantic relations. To this end, we carry out five novel tasks,
i.e., rotation, jigsaw, camouflage, grey-to-color, and blank-to-color for
self-supervised VL pre-training at patches of different scale. Kaleido-BERT is
conceptually simple and easy to extend to the existing BERT framework, it
attains new state-of-the-art results by large margins on four downstream tasks,
including text retrieval (R@1: 4.03% absolute improvement), image retrieval
(R@1: 7.13% abs imv.), category recognition (ACC: 3.28% abs imv.), and fashion
captioning (Bleu4: 1.2 abs imv.). We validate the efficiency of Kaleido-BERT on
a wide range of e-commerical websites, demonstrating its broader potential in
real-world applications.


---

**[150. [2502.07176] MatrixKAN: Parallelized Kolmogorov-Arnold Network](https://arxiv.org/pdf/2502.07176.pdf)** (2025-03-04)

*Cale Coffman, Lizhong Chen*

  Kolmogorov-Arnold Networks (KAN) are a new class of neural network
architecture representing a promising alternative to the Multilayer Perceptron
(MLP), demonstrating improved expressiveness and interpretability. However,
KANs suffer from slow training and inference speeds relative to MLPs due in
part to the recursive nature of the underlying B-spline calculations. This
issue is particularly apparent with respect to KANs utilizing high-degree
B-splines, as the number of required non-parallelizable recursions is
proportional to B-spline degree. We solve this issue by proposing MatrixKAN, a
novel optimization that parallelizes B-spline calculations with matrix
representation and operations, thus significantly improving effective
computation time for models utilizing high-degree B-splines. In this paper, we
demonstrate the superior scaling of MatrixKAN's computation time relative to
B-spline degree. Further, our experiments demonstrate speedups of approximately
40x relative to KAN, with significant additional speedup potential for larger
datasets or higher spline degrees.


---

**[151. [2312.03752] Automatic Scoring of Students' Science Writing Using Hybrid Neural
  Network](https://arxiv.org/pdf/2312.03752.pdf)** (2023-12-27)

*Ehsan Latif, Xiaoming Zhai*

  This study explores the efficacy of a multi-perspective hybrid neural network
(HNN) for scoring student responses in science education with an analytic
rubric. We compared the accuracy of the HNN model with four ML approaches
(BERT, AACR, Naive Bayes, and Logistic Regression). The results have shown that
HHN achieved 8%, 3%, 1%, and 0.12% higher accuracy than Naive Bayes, Logistic
Regression, AACR, and BERT, respectively, for five scoring aspects (p<0.001).
The overall HNN's perceived accuracy (M = 96.23%, SD = 1.45%) is comparable to
the (training and inference) expensive BERT model's accuracy (M = 96.12%, SD =
1.52%). We also have observed that HNN is x2 more efficient in training and
inferencing than BERT and has comparable efficiency to the lightweight but less
accurate Naive Bayes model. Our study confirmed the accuracy and efficiency of
using HNN to score students' science writing automatically.


---

**[152. [2109.03094] FHAC at GermEval 2021: Identifying German toxic, engaging, and
  fact-claiming comments with ensemble learning](https://arxiv.org/pdf/2109.03094.pdf)** (2021-09-08)

*Tobias Bornheim, Niklas Grieger, Stephan Bialonski*

  The availability of language representations learned by large pretrained
neural network models (such as BERT and ELECTRA) has led to improvements in
many downstream Natural Language Processing tasks in recent years. Pretrained
models usually differ in pretraining objectives, architectures, and datasets
they are trained on which can affect downstream performance. In this
contribution, we fine-tuned German BERT and German ELECTRA models to identify
toxic (subtask 1), engaging (subtask 2), and fact-claiming comments (subtask 3)
in Facebook data provided by the GermEval 2021 competition. We created
ensembles of these models and investigated whether and how classification
performance depends on the number of ensemble members and their composition. On
out-of-sample data, our best ensemble achieved a macro-F1 score of 0.73 (for
all subtasks), and F1 scores of 0.72, 0.70, and 0.76 for subtasks 1, 2, and 3,
respectively.


---

**[153. [2010.12730] Char2Subword: Extending the Subword Embedding Space Using Robust
  Character Compositionality](https://arxiv.org/pdf/2010.12730.pdf)** (2021-09-27)

*Gustavo Aguilar, Bryan McCann, Tong Niu, Nazneen Rajani, Nitish Keskar, Thamar Solorio*

  Byte-pair encoding (BPE) is a ubiquitous algorithm in the subword
tokenization process of language models as it provides multiple benefits.
However, this process is solely based on pre-training data statistics, making
it hard for the tokenizer to handle infrequent spellings. On the other hand,
though robust to misspellings, pure character-level models often lead to
unreasonably long sequences and make it harder for the model to learn
meaningful words. To alleviate these challenges, we propose a character-based
subword module (char2subword) that learns the subword embedding table in
pre-trained models like BERT. Our char2subword module builds representations
from characters out of the subword vocabulary, and it can be used as a drop-in
replacement of the subword embedding table. The module is robust to
character-level alterations such as misspellings, word inflection, casing, and
punctuation. We integrate it further with BERT through pre-training while
keeping BERT transformer parameters fixed--and thus, providing a practical
method. Finally, we show that incorporating our module to mBERT significantly
improves the performance on the social media linguistic code-switching
evaluation (LinCE) benchmark.


---

**[154. [2106.02208] BERTTune: Fine-Tuning Neural Machine Translation with BERTScore](https://arxiv.org/pdf/2106.02208.pdf)** (2021-06-07)

*Inigo Jauregi Unanue, Jacob Parnell, Massimo Piccardi*

  Neural machine translation models are often biased toward the limited
translation references seen during training. To amend this form of overfitting,
in this paper we propose fine-tuning the models with a novel training objective
based on the recently-proposed BERTScore evaluation metric. BERTScore is a
scoring function based on contextual embeddings that overcomes the typical
limitations of n-gram-based metrics (e.g. synonyms, paraphrases), allowing
translations that are different from the references, yet close in the
contextual embedding space, to be treated as substantially correct. To be able
to use BERTScore as a training objective, we propose three approaches for
generating soft predictions, allowing the network to remain completely
differentiable end-to-end. Experiments carried out over four, diverse language
pairs have achieved improvements of up to 0.58 pp (3.28%) in BLEU score and up
to 0.76 pp (0.98%) in BERTScore (F_BERT) when fine-tuning a strong baseline.


---

**[155. [2301.01982] Emotion-Cause Pair Extraction as Question Answering](https://arxiv.org/pdf/2301.01982.pdf)** (2023-01-09)

*Huu-Hiep Nguyen, Minh-Tien Nguyen*

  The task of Emotion-Cause Pair Extraction (ECPE) aims to extract all
potential emotion-cause pairs of a document without any annotation of emotion
or cause clauses. Previous approaches on ECPE have tried to improve
conventional two-step processing schemes by using complex architectures for
modeling emotion-cause interaction. In this paper, we cast the ECPE task to the
question answering (QA) problem and propose simple yet effective BERT-based
solutions to tackle it. Given a document, our Guided-QA model first predicts
the best emotion clause using a fixed question. Then the predicted emotion is
used as a question to predict the most potential cause for the emotion. We
evaluate our model on a standard ECPE corpus. The experimental results show
that despite its simplicity, our Guided-QA achieves promising results and is
easy to reproduce. The code of Guided-QA is also provided.


---

**[156. [2104.01563] ReCAM@IITK at SemEval-2021 Task 4: BERT and ALBERT based Ensemble for
  Abstract Word Prediction](https://arxiv.org/pdf/2104.01563.pdf)** (2021-04-06)

*Abhishek Mittal, Ashutosh Modi*

  This paper describes our system for Task 4 of SemEval-2021: Reading
Comprehension of Abstract Meaning (ReCAM). We participated in all subtasks
where the main goal was to predict an abstract word missing from a statement.
We fine-tuned the pre-trained masked language models namely BERT and ALBERT and
used an Ensemble of these as our submitted system on Subtask 1
(ReCAM-Imperceptibility) and Subtask 2 (ReCAM-Nonspecificity). For Subtask 3
(ReCAM-Intersection), we submitted the ALBERT model as it gives the best
results. We tried multiple approaches and found that Masked Language
Modeling(MLM) based approach works the best.


---

**[157. [2112.04329] JABER and SABER: Junior and Senior Arabic BERt](https://arxiv.org/pdf/2112.04329.pdf)** (2022-01-11)

*Abbas Ghaddar, Yimeng Wu, Ahmad Rashid, Khalil Bibi, Mehdi Rezagholizadeh, Chao Xing, Yasheng Wang, Duan Xinyu, Zhefeng Wang, Baoxing Huai, Xin Jiang, Qun Liu, Philippe Langlais*

  Language-specific pre-trained models have proven to be more accurate than
multilingual ones in a monolingual evaluation setting, Arabic is no exception.
However, we found that previously released Arabic BERT models were
significantly under-trained. In this technical report, we present JABER and
SABER, Junior and Senior Arabic BERt respectively, our pre-trained language
model prototypes dedicated for Arabic. We conduct an empirical study to
systematically evaluate the performance of models across a diverse set of
existing Arabic NLU tasks. Experimental results show that JABER and SABER
achieve state-of-the-art performances on ALUE, a new benchmark for Arabic
Language Understanding Evaluation, as well as on a well-established NER
benchmark.


---

**[158. [2310.18349] A Boundary Offset Prediction Network for Named Entity Recognition](https://arxiv.org/pdf/2310.18349.pdf)** (2023-10-31)

*Minghao Tang, Yongquan He, Yongxiu Xu, Hongbo Xu, Wenyuan Zhang, Yang Lin*

  Named entity recognition (NER) is a fundamental task in natural language
processing that aims to identify and classify named entities in text. However,
span-based methods for NER typically assign entity types to text spans,
resulting in an imbalanced sample space and neglecting the connections between
non-entity and entity spans. To address these issues, we propose a novel
approach for NER, named the Boundary Offset Prediction Network (BOPN), which
predicts the boundary offsets between candidate spans and their nearest entity
spans. By leveraging the guiding semantics of boundary offsets, BOPN
establishes connections between non-entity and entity spans, enabling
non-entity spans to function as additional positive samples for entity
detection. Furthermore, our method integrates entity type and span
representations to generate type-aware boundary offsets instead of using entity
types as detection targets. We conduct experiments on eight widely-used NER
datasets, and the results demonstrate that our proposed BOPN outperforms
previous state-of-the-art methods.


---

**[159. [2405.15039] CEEBERT: Cross-Domain Inference in Early Exit BERT](https://arxiv.org/pdf/2405.15039.pdf)** (2024-05-27)

*Divya Jyoti Bajpai, Manjesh Kumar Hanawal*

  Pre-trained Language Models (PLMs), like BERT, with self-supervision
objectives exhibit remarkable performance and generalization across various
tasks. However, they suffer in inference latency due to their large size. To
address this issue, side branches are attached at intermediate layers, enabling
early inference of samples without requiring them to pass through all layers.
However, the challenge is to decide which layer to infer and exit each sample
so that the accuracy and latency are balanced. Moreover, the distribution of
the samples to be inferred may differ from that used for training necessitating
cross-domain adaptation. We propose an online learning algorithm named
Cross-Domain Inference in Early Exit BERT (CeeBERT) that dynamically determines
early exits of samples based on the level of confidence at each exit point.
CeeBERT learns optimal thresholds from domain-specific confidence observed at
intermediate layers on the fly, eliminating the need for labeled data.
Experimental results on five distinct datasets with BERT and ALBERT models
demonstrate CeeBERT's ability to improve latency by reducing unnecessary
computations with minimal drop in performance. By adapting to the threshold
values, CeeBERT can speed up the BERT/ALBERT models by $2\times$ - $3.5\times$
with minimal drop in accuracy.


---
